[
    "Formal Safety and Security of Neural Networks",
    "Neural\nnetworks are at the forefront of AI research that finds more and more\napplications in safety-critical domains. While neural networks show impressive\nperformance in terms of prediction error, it is hard to assure that their use in critical\nsystems does not pose a significant risk. In this module, we will use interactive\ntheorem prover Coq to develop verified tools that enable safety and security of neural networks, analyze their behavior when deployed in a safety-critical context and protect them from\nexternal manipulations.",
    "Project meetings",
    ":",
    "FH 301",
    "Zoom",
    ", Tuesday, 10:00-12:00",
    "MAR 4.062, Friday, 10:00 - 12:00",
    "Bi-weekly Jour Fixe:",
    "MAR 4.062, Friday, 10:00 - 12:00",
    "Zoom link:",
    "https://tu-berlin.zoom.us/j/66678783848?pwd=UDFoOHE0azl4VmZoRGdHeTJWcWV0dz09",
    "Slack link",
    ":",
    "https://rawsose2024.slack.com",
    "GitLab repository:",
    "https://git.tu-berlin.de/andreialeksandrov98/rawsose24",
    "Software Foundations:",
    "https://softwarefoundations.cis.upenn.edu/lf-current/index.html",
    "Aktivität Announcements auswählen",
    "Announcements",
    "Forum",
    "Aktivität Discussions auswählen",
    "Discussions",
    "Forum",
    "Aktivität raw_assesment_criteria.pdf auswählen",
    "raw_assesment_criteria.pdf",
    "Datei",
    "Hochgeladen 16.04.2024 09:47",
    "Aktivität RaW_topic_catalogue.pdf auswählen",
    "RaW_topic_catalogue.pdf",
    "Datei",
    "Hochgeladen 30.04.2024 13:09"
]