[
    "Select activity Announcements",
    "Announcements",
    "Forum",
    "Select activity Course description",
    "Course description",
    "*NEW*:",
    "See exam guidelines",
    "here",
    ".",
    "Please, send any feedback on the course and its lectures on",
    "this feedback form",
    ".",
    "Time",
    ": Wednesdays 10-12",
    "Place",
    ": MA376",
    "Begins",
    ": April 17, 2023 (weekly)",
    "Language of instruction:",
    "English",
    "Credits:",
    "5",
    "This course is an introduction to the theory of first-order optimization methods for high-dimensional\n problems, as well as some theory of online learning, and how it applies\n to optimization. It is an introduction to the theory of many methods \nused in Machine Learning.",
    "Prerequisites:",
    "linear algebra, calculus, basic topology and basic probability.",
    "A\n small number of results from the Convex Analysis course will be used. \nThey will be stated without proof and some intuition will be given. You \ndo not need to course Convex Analysis, but it is a good complement for \nthis course.",
    "Some topics that will be covered:",
    "Introduction to some convex optimization concepts",
    "Online learning and regret minimization:",
    "Experts problem",
    "Follow the Regularized Leader",
    "Mirror Descent",
    "Multi-armed bandits",
    "Applying online learning to optimization",
    "High-dimensional optimization algorithms",
    "Steepest descent",
    "Mirror Descent / FTRL in optimization",
    "Accelerated Gradient Descent",
    "Accelerated Proximal Point method, and inexact implementation",
    "Frank-Wolfe algorithm",
    "Reductions",
    "Lower bounds",
    "Depending on the time, we may cover some algorithms for min-max convex concave optimization and some stochastic methods.",
    "Professor's email",
    ":",
    "martinez-rubio@zib.de",
    "Select activity References",
    "References",
    "The following are some monographs and other sources that contain most of the material in this course. The ones marked with * are specially recommended if the student wants to delve deeper. However, many proofs in this course are shown in a new form prioritizing simplicity and intuition for the steps taken, so most results will be found in these sources but with very different proofs.",
    "A reference for the basics of the optimization tools is the",
    "Convex Analysis notes",
    "from the course taught now in the summer semester. This is not required but it can be helpful for the interested student.",
    "Online learning:",
    "*",
    "A Modern Introduction to Online Learning",
    ", 2023, Orabona.",
    "Introduction to Online Optimization",
    ", 2011, Bubeck.",
    "Introduction to Online Convex Optimization",
    ", Second Edition, 2023, Hazan.",
    "The Multiplicative Weights Update Method: A Meta Algorithm and Applications",
    ", 2009, Arora et al.",
    "Optimization:",
    "*",
    "Convex Optimization: Algorithms and Complexity",
    ", 2014, Bubeck",
    "*",
    "Lectures on convex optimization",
    ", Vol. 137. Springer, 2018, Yurii Nesterov et al.",
    "Continuous Algorithms",
    ", Parts I, II, and III, 2024, Tian.",
    "Proximal algorithms",
    ", 2013, Parikh et al.",
    "The approximate duality gap technique: a unified theory of first-order methods",
    ", 2017, Diakonikolas et al.",
    "Conditional Gradient Methods",
    ", 2022, Braun et al.",
    "Select activity Convex Analysis notes",
    "Convex Analysis notes",
    "File",
    "Uploaded 26/04/24, 21:18"
]