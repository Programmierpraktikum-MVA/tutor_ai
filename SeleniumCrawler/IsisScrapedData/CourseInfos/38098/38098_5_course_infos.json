[
    "Data Augmentation",
    "Key Words",
    "Publication",
    "Link",
    "CutOut (method)",
    "T. DeVries and G. W. Taylor, \"Improved Regularization of Convolutional Neural Networks with Cutout\", in",
    "preprint arxiv:1708.04552",
    ", 2017.",
    "devries_2017",
    "MixUp (method)",
    "H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, \"mixup: Beyond Empirical Risk Minimization\", in",
    "International Conference on Learning Representations",
    ", 2018.",
    "zhang_2018",
    "CutMix (method)",
    "S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y. Yoo, \"CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features\", in",
    "Proceedings of the IEEE/CVF International Conference on Computer Vision",
    ", 2019, pp. 6023–6032.",
    "yun_2019",
    "Attentive CutMix (method)",
    "D. Walawalkar, Z. Shen, Z. Liu, and M. Savvides, \"Attentive Cutmix: An \nEnhanced Data Augmentation Approach for Deep Learning Based Image \nClassification\", in",
    "IEEE International Conference on Acoustics, Speech \nand Signal Processing",
    ", 2020, pp. 3642–3646.",
    "walawalkar_2020",
    "AutoAug (strategy)",
    "E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le, \"AutoAugment: Learning Augmentation Strategies From Data\", in",
    "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
    ", 2019.",
    "cubuk_2019",
    "RandAug (strategy)",
    "E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le, \"Randaugment: Practical automated data augmentation with a reduced search space\", in",
    "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops",
    ", 2020, pp. 702–703.",
    "cubuk_2020",
    "deeper understanding",
    "R. Gontijo-Lopes, S. Smullin, E. D. Cubuk, and E. Dyer, \"Tradeoffs in Data Augmentation: An Empirical Study\", in",
    "International Conference on Learning Representations",
    ", 2021.",
    "gontijo-lopes_2021",
    "deeper understanding",
    "R. Balestriero, L. Bottou, and Y. LeCun, \"The Effects of Regularization and Data Augmentation are Class Dependent\", in",
    "Advances in Neural Information Processing Systems",
    ", 2022, vol. 36.",
    "balestriero_2022",
    "Semi-Supervised Learning",
    "Key Words",
    "Publication",
    "Link",
    "MixMatch",
    "D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver, and C. \nA. Raffel, \"Mixmatch: A holistic approach to semi-supervised learning\",",
    "Advances in Neural Information Processing Systems",
    ", vol. 32, 2019.",
    "berthelot_2019",
    "UDA",
    "Q. Xie, Z. Dai, E. Hovy, T. Luong, and Q. Le, \"Unsupervised data augmentation for consistency training\",",
    "Advances in Neural Information Processing Systems",
    ", vol. 33, pp. 6256–6268, 2020.",
    "xie_2020",
    "FixMatch",
    "K. Sohn",
    "et al.",
    ", \"Fixmatch: Simplifying semi-supervised learning with consistency and confidence\",",
    "Advances in Neural Information Processing Systems",
    ", vol. 33, pp. 596–608, 2020.",
    "sohn_2020",
    "ReMixMatch",
    "D. Berthelot, N. Carlini, E. D. Cubuk, A. Kurakin, H. Zhang, and C. Raffel, \"ReMixMatch: Semi-Supervised Learning with Distribution Matching and Augmentation Anchoring\" in",
    "International Conference on Learning Representations",
    ", 2020.",
    "berthelot_2020",
    "FlexMatch",
    "B. Zhang, Y. Wang, W. Hou, H. Wu, J. Wang, M. Okumura, and T. Shinozaki, \"FlexMatch: Boosting Semi-Supervised Learning with Curriculum Pseudo Labeling\",",
    "Advances in Neural Information Processing Systems",
    ", vol. 34, pp. 18408-18419, 2021.",
    "zhang_2021",
    "Self-Supervised Learning",
    "Key Words",
    "Publication",
    "Link",
    "SimCLR",
    "T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, \"A Simple Framework for Contrastive Learning of Visual Representations\", in",
    "Proceedings of the 37th International Conference on Machine Learning",
    ", 2020, vol. 119, pp. 1597–1607.",
    "chen_2020",
    "MoCo",
    "K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, \"Momentum contrast for unsupervised visual representation learning\", in",
    "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
    ", 2020, pp. 9729–9738.",
    "he_2020",
    "BYOL",
    "J.-B. Grill et al., \"Bootstrap your own latent-a new approach to self-supervised learning\", in",
    "Advances in Neural Information Processing Systems",
    ", vol. 33, pp. 21271–21284, 2020.",
    "grill_2020",
    "DINO",
    "M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin, \"Emerging Properties in Self-Supervised Vision Transformers\", in",
    "Proceedings of the IEEE/CVF International Conference on Computer Vision",
    ", 2021, pp. 9650-9660.",
    "caron_2021",
    "MAE (generative Self-SL)",
    "K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick, \"Masked autoencoders are scalable vision learners\", in",
    "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
    ", 2022, pp. 16000–16009.",
    "he_2022",
    "background on augmentations",
    "T. Xiao, X. Wang, A. A. Efros, and T. Darrell, \"What Should Not Be Contrastive in Contrastive Learning\", in",
    "International Conference on Learning Representations",
    ", 2021.",
    "xiao_2021",
    "deeper understanding",
    "T. Chen, C. Luo, and L. Li, \"Intriguing properties of contrastive losses\", in",
    "Advances in Neural Information Processing Systems",
    ", vol. 34, pp. 11834–11845, 2021.",
    "chen_2021",
    "Self-SL for imbalanced data (optional)",
    "Z. Jiang, T. Chen, B. J. Mortazavi, and Z. Wang, \"Self-damaging contrastive learning\", in",
    "International Conference on Machine Learning",
    ", 2021, pp. 4927–4939.",
    "jiang_2021",
    "General Overview",
    "R. Balestriero, M. Ibrahim, V. Sobal, A. Morcos, S. Shekhar, T. Goldstein, F. Bordes, A. Bardes, G. Mialon, Y. Tian, A. Schwarzschild, A. G. Wilson, J. Geiping, Q. Garrido, P. Fernandez, A. Bar, H. Pirsiavash, Y. LeCun, and M. Goldblum, \"A Cookbook of Self-Supervised Learning\",",
    "arXiv preprint arXiv:2304.12210",
    ", 2023.",
    "balestriero_2023",
    "Modern CNNs vs. Vision Transformer",
    "Key Words",
    "Publication",
    "Link",
    "ConvNeXt, main paper",
    "Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie, \"A convnet for the 2020s\", in",
    "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
    ", 2022, pp. 11976–11986.",
    "liu_2022",
    "ViT, vision transformer",
    "A. Dosovitskiy",
    "et al.",
    ", \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\", in",
    "International Conference on Learning Representations",
    ", 2021.",
    "dosovitskiy_2021",
    "DeiT, vision transformer",
    "H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jegou, \"Training data-efficient image transformers & distillation through \nattention\", in",
    "Proceedings of the 38th International Conference on Machine Learning",
    ", 2021, vol. 139, pp. 10347–10357.",
    "touvron_2021",
    "Swin Transformer, CNN-inspired vision transformer",
    "Z. Liu",
    "et al.",
    ", \"Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows\", in",
    "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)",
    ", 2021, pp. 10012–10022.",
    "liu_2021",
    "EfficientNet, RL-based neural network architecture search",
    "M. Tan and Q. Le, \"Efficientnet: Rethinking model scaling for convolutional neural networks\", in",
    "International conference on machine learning",
    ", 2019, pp. 6105–6114.",
    "tan_2019",
    "EfficientNetV2",
    "M. Tan and Q. Le, \"Efficientnetv2: Smaller models and faster training\", in",
    "International conference on machine learning",
    ", 2021, pp. 10096–10106.",
    "tan_2021",
    "ConvNeXtV2, enable fully convolutional MIM (optional)",
    "S. Woo",
    "et al.",
    ", \"ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders\",",
    "arXiv preprint arXiv:2301. 00808",
    ", 2023.",
    "woo_2023",
    "Explainable AI",
    "Key Words",
    "Publication",
    "Link",
    "LIME (method)",
    "M. T. Ribeiro, S. Singh, and C. Guestrin, \"Why should I trust you?\" Explaining the Predictions of any Classifier\",",
    "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
    ", 2016.",
    "ribeiro_2016",
    "GradCAM (method)",
    "R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. \nBatra, \"Grad-cam: Visual explanations from deep networks via \ngradient-based localization\", in",
    "Proceedings of the IEEE international conference on computer vision",
    ", 2017, pp. 618–626.",
    "selvaraju_2017",
    "Score-CAM (method)",
    "H. Wang et al., \"Score-CAM: Score-weighted visual explanations for convolutional neural networks\", in",
    "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops",
    ", 2020, pp. 24–25.",
    "wang_2020",
    "DeepLIFT (method)",
    "A. Shrikumar, P. Greenside, and A. Kundaje, \"Learning important features through propagating activation differences\",",
    "International Conference on Machine Learning",
    ", 2017.",
    "shrikumar_2017",
    "method extension, explaining global behavior",
    "T. Chakraborty, U. Trehan, K. Mallat, and J.-L. Dugelay, \"Generalizing Adversarial Explanations with Grad-CAM\", in",
    "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
    ", 2022, pp. 187–193.",
    "chakraborty_2022",
    "xAI limitations",
    "A. Ghorbani, A. Abid, and J. Zou, \"Interpretation of neural networks is fragile\", in",
    "Proceedings of the AAAI conference on artificial intelligence",
    ", 2019, vol. 33, pp. 3681–3688.",
    "ghorbani_2019",
    "xAI limitations",
    "L. Sixt, M. Granz, and T. Landgraf, \"When explanations lie: Why many modified bp attributions fail\", in",
    "International Conference on Machine Learning",
    ", 2020, pp. 9046–9057.",
    "sixt_2020",
    "empirical evaluation framework",
    "A. Khakzar, P. Khorsandi, R. Nobahari, and N. Navab, \"Do Explanations Explain? Model Knows Best\", in",
    "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
    ", 2022, pp. 10244–10253.",
    "khakzar_2022",
    "review of xAI evaluation protocols (optional)",
    "M. Nauta",
    "et al.",
    ", \"From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic Review on Evaluating Explainable AI\",",
    "ACM Comput. Surv.",
    ", Feb. 2023.",
    "nauta_2023",
    "Generalization and Memorization",
    "Key Words",
    "Publication",
    "Link",
    "first paper questioning generalization theory",
    "C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals, \"Understanding \ndeep learning (still) requires rethinking generalization\",",
    "Communications of the ACM",
    ", vol. 64, no. 3, pp. 107–115, 2021.",
    "zhang_2017(2021)",
    "memorization, postulating long-tail theory",
    "V. Feldman, \"Does learning require memorization? a short tale about a long tail\", in",
    "Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing",
    ", 2020, pp. 954–959.",
    "feldman_2020a",
    "memorization, proving long-tail theory",
    "V. Feldman and C. Zhang, \"What neural networks memorize and why: Discovering the long tail via influence estimation\",",
    "Advances in Neural Information Processing Systems",
    ", vol. 33, pp. 2881–2891, 2020.",
    "feldman_2020b",
    "sample difficulty, learning order",
    "Z. Jiang, C.\n Zhang, K. Talwar, and M. C. Mozer, \"Characterizing Structural \nRegularities of Labeled Data in Overparameterized Models\", in",
    "International Conference on Machine Learning",
    ", 2021, pp. 5034–5044.",
    "jiang_2021",
    "sample difficulty,",
    "learning order",
    "R. Baldock, H. Maennel, and B. Neyshabur, \"Deep learning through the lens of example difficulty\",",
    "Advances in Neural Information Processing Systems",
    ", vol. 34, pp. 10876–10889, 2021.",
    "baldock_2021",
    "representation in generalization",
    "D. Doimo, A. Glielmo, S. Goldt, and A. Laio, \"Redundant representations help generalization in wide neural networks\",",
    "Advances in Neural Information Processing Systems",
    ", vol. 35, pp. 19659-19672, 2022.",
    "doimo_2022",
    "generalization vs. memorization",
    "A. Morcos, M. Raghu, and S. Bengio, \"Insights on representational similarity in neural networks with canonical correlation\",",
    "Advances in Neural Information Processing Systems",
    ", vol. 31, 2018.",
    "morcos_2018",
    "generalization vs. memorization",
    "C. Stephenson, A. Ganesh, Y. Hui, H. Tang, and S. Chung, \"On the \ngeometry of generalization and memorization in deep neural networks\", in",
    "International Conference on Learning Representations",
    ", 2020.",
    "stephenson_2020",
    "Special Aspects of Learning",
    "Key Words",
    "Publication",
    "Link",
    "deep double descent",
    "P. Nakkiran, G. Kaplun, Y. Bansal, T. Yang, B. Barak, and I. Sutskever, \"Deep Double Descent: Where Bigger Models and More Data Hurt\", in",
    "International Conference on Learning Representations",
    ", 2020.",
    "nakkiran_2020",
    "curriculum learning",
    "X. Wu, E. Dyer, and B. Neyshabur, \"When Do Curricula Work?\", in",
    "International Conference on Learning Representations",
    ", 2021.",
    "wu_2021",
    "intrinsic dimension, understanding representations",
    "A. Ansuini, A. Laio, J. H. Macke, and D. Zoccolan, \"Intrinsic dimension of data representations in deep neural networks\",",
    "Advances in Neural Information Processing Systems",
    ", vol. 32, 2019.",
    "ansuini_2019",
    "spurious correlation",
    "K. Hamidieh, H. Zhang, and M. Ghassemi, \"Evaluating and Improving \nRobustness of Self-Supervised Representations to Spurious Correlations\",\n in",
    "ICML 2022: Workshop on Spurious Correlations, Invariance and Stability",
    ", 2022.",
    "hamidieh_2022",
    "spurious correlation",
    "P. Kirichenko, P. Izmailov, and A. G. Wilson, \"Last layer re-training is sufficient for robustness to spurious correlations\",",
    "International Conference on Learning Representations",
    ", 2023.",
    "kirichenko_2023",
    "spurious correlation",
    "Y.-Y. Yang, C.-N. Chou, and K. Chaudhuri, \"Understanding Rare Spurious Correlations in Neural Networks\", in",
    "ICML 2022: Workshop on Spurious Correlations, Invariance and Stability",
    ", 2022.",
    "yang_2022",
    "The Lottery Ticket Hypothesis",
    "Key Words",
    "Publication",
    "Link",
    "lottery ticket hypothesis (LTH)",
    "J. Frankle and M. Carbin, \"The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\", in",
    "International Conference on Learning Representations",
    ", 2019.",
    "frankle_2019",
    "generalizing LTH",
    "A. Morcos, H. Yu, M. Paganini, and Y. Tian, \"One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers\", in",
    "Advances in Neural Information Processing Systems",
    ", 2019, vol. 32.",
    "morcos_2019",
    "generalizing LTH",
    "X. Chen, Y. Cheng, S. Wang, Z. Gan, J. Liu, and Z. Wang, \"The Elastic Lottery Ticket Hypothesis\", in",
    "Advances in Neural Information Processing Systems",
    ", 2021, vol. 34, pp. 26609–26621.",
    "chen_2021",
    "deeper understanding",
    "H. Zhou, J. Lan, R. Liu, and J. Yosinski, \"Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask\", in",
    "Advances in Neural Information Processing Systems",
    ", 2019, vol. 32.",
    "zhou_2019",
    "deeper understanding",
    "X. Ma et al., \"Sanity Checks for Lottery Tickets: Does Your Winning Ticket Really Win the Jackpot?\", in",
    "Advances in Neural Information Processing Systems",
    ", 2021, vol. 34, pp. 12749–12760.",
    "ma_2021",
    "deeper understanding",
    "N. Liu et al., \"Lottery Ticket Preserves Weight Correlation: Is It Desirable or Not?\", in",
    "International Conference on Machine Learning",
    ", 2021, pp. 7011–7020.",
    "liu_2021",
    "pruning at initialization vs. during training",
    "J. Frankle, G. K. Dziugaite, D. Roy, and M. Carbin, \"Pruning Neural Networks at Initialization: Why Are We Missing the Mark?\", in",
    "International Conference on Learning Representations",
    ", 2021.",
    "frankle_2021",
    "pre-training, SSL",
    "T. Chen et al., \"The lottery tickets hypothesis for supervised and self-supervised pre-training in computer vision models\", in",
    "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
    ", 2021, pp. 16306–16316.",
    "chen_2021",
    "Texture Bias in CNNs",
    "Key Words",
    "Publication",
    "Link",
    "initial paper",
    "R. Geirhos, P. Rubisch, C. Michaelis, M. Bethge, F. A. Wichmann, and W. Brendel, \"ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness\", in",
    "International Conference on Learning Representations",
    ", 2018.",
    "geirhos_2018",
    "initial paper",
    "N. Baker, H. Lu, G. Erlikhman, and P. J. Kellman, \"Deep convolutional networks do not classify based on global object shape\",",
    "PLoS computational biology",
    ", vol. 14, no. 12, p. e1006613, 2018.",
    "baker_2018",
    "feature decodability",
    "K. Hermann and A. Lampinen, \"What shapes feature representations? exploring datasets, architectures, and training\",",
    "Advances in Neural Information Processing Systems",
    ", vol. 33, pp. 9995–10006, 2020.",
    "hermann_2020a",
    "intensive study",
    "K. Hermann, T. Chen, and S. Kornblith, \"The origins and prevalence of texture bias in convolutional neural networks\",",
    "Advances in Neural Information Processing Systems",
    ", vol. 33, pp. 19000–19015, 2020.",
    "hermann_2020b",
    "BagNet",
    "W. Brendel and M. Bethge, \"Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet\", in",
    "International Conference on Learning Representations",
    ", 2018.",
    "brendel_2018",
    "new perspective on Texture Bias",
    "C. K. Mummadi, R. Subramaniam, R. Hutmacher, J. Vitay, V. Fischer, and J. H. Metzen, \"Does enhanced shape bias improve neural network robustness to common corruptions?\", in",
    "International Conference on Learning Representations",
    ", 2020.",
    "mummadi_2020",
    "applying Texture Bias knowledge",
    "S. Jain, D. Tsipras, and A. Madry, \"Combining Diverse Feature Priors\", in",
    "International Conference on Machine Learning",
    ", 2022, pp. 9802–9832.",
    "jain_2020",
    "Texture Bias in Vision Transformer",
    "M. M. Naseer, K. Ranasinghe, S. H. Khan, M. Hayat, F. Shahbaz Khan, and \nM.-H. Yang, \"Intriguing properties of vision transformers\",",
    "Advances in Neural Information Processing Systems",
    ", vol. 34, pp. 23296–23308, 2021.",
    "naseer_2021"
]