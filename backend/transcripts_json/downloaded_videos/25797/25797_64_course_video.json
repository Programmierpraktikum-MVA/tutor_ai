[{"lecture": "25797_64_course_video", "Timestamps": [{"text": "  In diesem Video wollen wir uns die Schritte der Hauptkomponentenanalyse anschauen und  sie kurz zusammenfassen.  Vorab am Ende hinwahrs gibt es die Hauptkomponentenanalyse kurz PCA und das ist ein Algorithmus,  der auf der Singulaerwaertserlegung der SVD basiert.  Manchmal werden die Begriffe jedoch in der Literatur synonym verwendet, lasst euch davon  nicht verwirren.", "start": 0.0, "end": 28.0}, {"text": "  Die Hauptkomponentenanalyse der PCA ist eigentlich ein Verfahrensanalys der Daten und die Singulaerwaertserlegung  der SVD bezeichnet nur die eigentliche Matrixerlegung.  Waehrend die Notebook habe ich die Schritte der Hauptkomponentenanalyse aufgelistet, so  wie sie auch dann in der Hausvergabe implementiert werden sollen.", "start": 28.0, "end": 46.44}, {"text": "  Der erste Schritt besteht darin die Datenmatrix auszustellen, heisst wir koennen Daten als  Vektoren auffassen und speziell in unserem Beispiel haben sind die Daten neben Bilder  und diese Bilder koennen wir auch als Vektoren schreiben.", "start": 46.44, "end": 67.28}, {"text": " Also der Vektor X als Bild aufgefasst soll dann halt N-Eintraege haben und den Vektor  koennen wir halt bekommen, indem wir die Pixel-Zeilenweise hintereinander aufschreiben oder Schweitenweise  hintereinander aufschreiben.  Rundsaetzlich egal, wichtig ist, dass man das bei jedem Bild in diesem Algorithmus gleich  macht.", "start": 67.28, "end": 77.88}, {"text": "  Also die Reihenfolge der Pixel in diesem Vektor muss immer gleich geordnet sein.  Die Vektoren werden dann in die Zeilen der Datenmatrix X geschrieben, heisst in jeder  Zeile in dieser Matrix liegt dann ein Datenpunkt und wenn wir insgesamt K-Zeilen haben, also  K-Verschiedene Datenpunkte und N-Dimensionen pro Datenpunkt, also N-Spalten, dann haben  wir halt eine K-Memmatrix.", "start": 78.88, "end": 108.47999999999999}, {"text": "  Alternativ koennte man die Vektoren auch in die Spalten der Matrix schreiben, das wollen  wir aber in der Hausaufgabe eben nicht so machen, da sollte die Datenvektoren in die  Zeilen schreiben.  Der naechste Schritt ist dann die Mittelwertbefreugung und da muessen wir die Daten in den koordinaten  Ursprung legen.", "start": 108.47999999999999, "end": 126.11999999999999}, {"text": "  Dazu berechnen wir zuerst einmal den Mittelwert, heisst wir summieren Elementweise aller Vektoren  auf und teilen dann durch K, heisst im wederlichen berechnen wir den Durchschnitt oder den Mittelwert  von jeder Dimension einzeln und schreiben dann jeden Mittelwert halt in dieses X mit  ihrem Overline hier und genau da unten die Dimensionen ja genau Pixel sind berechnen", "start": 126.12, "end": 152.76}, {"text": " wir quasi fuer alle Bilder die wir haben in durchschnittlichen Pixel Wert und das schreiben  wir dann in dieses X-Stich und das ist dann der Mittelwert von anderen Daten und um die  eigentliche Mittelwertbefraunen durchzufuehren muessen wir die Mittelwert des Daten subtrahieren.", "start": 152.88, "end": 176.39999999999998}, {"text": " Das habe ich hier relativ simpel hingeschrieben, X-Stich ist dann quasi unsere Mittelwertbefreiten  Daten, die erhalten wir indem wir von X und der Mittelwert einmal abziehen.  Es koennte im Python und NumPy fast genau so schreiben, dann muesste ihr dort nochmal  auf die Dimensionen achten, dass die Dimensionen also zueinander passen.", "start": 177.4, "end": 192.28}, {"text": "  Ja das habe ich hier einmal visualisiert, auf der linken Seite haben wir eine Punterwolke  und auf der rechten Seite haben wir genau die gleiche Punterwolke und den gruenen habe  ich die bei Mittelwerte eingetragen und wir sehen das hier auf der linken Seite der Mittelwert  im genauen konditanten Wurschwund liegt, das heisst diese Datenwolke hier ist Mittelwertbefreit", "start": 192.28, "end": 208.48000000000002}, {"text": " und die rechts sieht diese Datenwolke ausserhalb.  Ich habe zusaetzlich die erste Hauptkomponente berechnet und jetzt sehen wir hier links entsprechend  die Hauptkomponente tatsaechlich der Dimensionen mit der hoechsten Varianz, also genau der Richtungen  in der die Daten die hoechste Varianz haben.", "start": 208.48000000000002, "end": 231.07999999999998}, {"text": "Die rechts ist das nicht der Fall, hier erhalten wir  irgendein Vektor irgendeine Hauptkomponente die in die Richtung der Daten zeigt, aber  weder was mit der ersten Hauptkomponente zu tun hat, noch was mit der zweiten Hauptkomponente  zu tun hat.", "start": 231.07999999999998, "end": 247.72}, {"text": "Also damit wir halt die Hauptkomponentenanalyse vernuenftig durchfuehren koennen und dann  wirklich die korrekten Ergebnisse bekommen, muessen wir immer die Mittelwertbefreien durchfuehren.  Der dritte Schritt ist dann der einfachste Schritt und zwar wollen wir jetzt die Singulaerwertelegung  berechnen.", "start": 247.72, "end": 264.0}, {"text": "Wir haben alle Daten in eine Zeile geschrieben, in einem Matrix, in dem Matrix  ist, koennen wir jetzt an die SVD-Funktion von zum Beispiel Numper uebergeben und dann bekommen  wir eben die Zerlegung in U, 7-mal und v-transponiert. Wenn wir die Daten in die Zeilen geschrieben haben,  dann stehen die Hauptkomponenten in den Spalten von V beziehungsweise in den Zeilen von v-transponiert.", "start": 264.0, "end": 280.4}, {"text": "  Wenn ihr die Numperfunktion verwendet, dann achtet darauf, dass ihr die Parameter richtig  uebergibt. Das heisst, ihr muesst schauen, dass ihr die Format-Fuesses richtig angebt und abhaengig  davon kriegt ihr halt die volle SVD oder die reduzierte SVD.", "start": 280.4, "end": 302.52}, {"text": "Es kommt dann darauf an,  auf welchen, was ihr genau erreichen wollt, was euer Anwendungsfall ist und da muesst ihr halt  darauf achten, dass ihr dann den richtigen Parameter setzt.", "start": 302.52, "end": 317.08}, {"text": "Der Schritt vier ist dann das  Auswaehnen der Hauptkomponenten und zwar wollen wir um die Dimensionen zu reduzieren, die Daten  landen der ersten P-Hop-Komponenten ausdruecken.", "start": 317.08, "end": 327.24}, {"text": "Wuerden wir die Daten entlang aller Hauptkomponenten  ausdruecken, also entlang quasi an der vollen Basis, dann wuerden wir keine Dimensionen sparen und  haetten quasi noch die volle Dimension der Daten.", "start": 327.24, "end": 336.76}, {"text": "Wir waehlen aber eben nur die ersten P-Hop-Komponenten  und stellen unter Daten anhand dieser ersten P-Hop-Komponenten da und dadurch sparen wir uns  dann halt ein paar Dimensionen und koennen dann zum Beispiel den Vergleich von Bildern effizienter  durchfuehren.", "start": 336.76, "end": 346.76}, {"text": "Der Anhaltspunkt dafuer, welche Hauptkomponenten wir waehlen sollen, sind die  Singulaerwerte, die geben naemlich an, wie gross die Varianten lang der Hauptkomponenten ist. Dafuer  habe ich hier diese Zeichnung vorbereitet. Auf der linken Seite sehen wir eine komplett runde  Punktewurke und auf der rechten Seite eine elliptische Punktewurke.", "start": 346.76, "end": 365.24}, {"text": "Also hier auf der linken  Seite sind die Datenpunkte quasi in alle Dimensionen gleichmaessig verteilt. Hier sind sie entlang einer  Dimension staerker verscheid, ist entlang der anderen Dimension.", "start": 365.24, "end": 384.48}, {"text": "Die beiden Hauptkomponenten V1 und  V2 habe ich hier auch fuer beide Datenpunkte eingezeichnet und wir sehen, dass sie halt autokonal  aufeinander stehen, so wie es sein soll. Und auf der linken Seite sind eben beide die Varianten  in Lande der Hauptkomponenten gleichmaessig ausgepraete.", "start": 384.48, "end": 394.76}, {"text": "Das heisst, die zugehoerigen Singulaerwerte,  der beiden Hauptkomponenten in ungefaehr gleich gross. Also sie mal einste singulaerwerte  der ersten Hauptkomponente, sieben Mal zwei ist der Singulaerwerte der zweiten Hauptkomponente.  Die sind jetzt hier genau gleich gross.", "start": 394.76, "end": 410.84}, {"text": "Genauso sind die Eigenwerte, die Quadrate der Singulaerwerte  ungefaehr gleich gross. Bedeutet eben, dass die Varianten in Lande Hauptkomponenten gleichmaessig  ausgepraete ist. Auf der rechten Seite ist die Variante nicht gleichmaessig ausgepraete. Da haben  wir eine hoehere Variante in Lande Richtung V1.", "start": 410.84, "end": 426.67999999999995}, {"text": "Dadurch ist der Singulaerwert jetzt zu dieser  ersten Hauptkomponente gehoert. Auch groesser ist der Singulaerwert, der zweiten Hauptkomponente gehoert.  Analog dazu ist eben der erste Eigenwert groesser, ist der zweite Eigenwert. Und wir wollen jetzt alle  Hauptkomponenten mit kleinen Singulaerwerten verwerfen.", "start": 426.67999999999995, "end": 445.08}, {"text": "Also wir sagen, wir wollen nur die  Hauptkomponenten haben, die eine grosse Variante haben und dann gehen wir davon aus. Diese Hauptkomponenten  haben eine hohe Aussagekraft ueber andere Daten. Also die sind signifikant.", "start": 445.15999999999997, "end": 462.52}, {"text": "Und die Hauptkomponenten  mit kleinen Singulaerwerten, die sagen eher weniger ueber andere Daten aus und die wollen wir dann eben  nicht benutzen fuer unsere Darstellung anhand der Hauptkomponenten.", "start": 462.52, "end": 475.08}, {"text": "In der Hausaufgabe wollen wir  dann genau die Hauptkomponenten waehlen, sodass die zugehoerigen Singulaerwerte 80 Prozent  der Summe aller Singulaerwerte ausmachen. Also wenn wir die Singulaerwerte der P-Hobkomponenten  aussehmieren, die wir verwenden, dann soll das genau 0,8 Mal, also 80 Prozent,  vor der Summe alle Singulaerwerte sein.", "start": 475.08, "end": 493.88}, {"text": "Ja, wenn man ueber die Anzahl der Hauptkomponenten spricht,  dann schaut man sich ganz haeufig auch das Singulaerwertspektrum an oder das Seibenwertspektrum.  Jetzt auf der x-Achse haben wir jetzt hier den Index von den Eigenwerten bzw. Singulaerwerten  auf der y-Achse. Haben wir die Magnitude der Singulaerwerte bzw. der Eigenwerte.", "start": 493.96, "end": 511.32}, {"text": "  Heisst das der Wert hier ist das groesste der Eigenwert. In dem Fall habe ich hier jetzt die  Albenwerte dargestellt.", "start": 511.32, "end": 534.12}, {"text": "Und was wir sehen ist, dass wir relativ wenige Hauptkomponenten haben,  die eine sehr, sehr hohe Signifikanz haben, also einen sehr hohen Eigenwert, aber sehr,  sehr viele Hauptkomponenten, die eher einen kleinen Eigenwert haben, also weniger aussergekraeftig  ueber eine Daten sind.", "start": 534.12, "end": 539.48}, {"text": "Und genau diese Hauptkomponenten, die jetzt sehr wenig aussergekraeftig  ueber die Daten sind, aber eben ein Grossteil der Dimensionen ausmachen, die wollen wir jetzt verwerfen.  Dadurch sparen wir uns dann viele Dimensionen und koennen uns wirklich auch die aussergekraeftigen  Dimensionen konzentrieren.", "start": 540.12, "end": 556.76}, {"text": "Ja, jetzt kommen wir zum wichtigsten Schritt, dem fuenften,  dem Basiswechsel. Hier soll es darum geben, wie wir dann unsere Daten tatsaechlich anhand  der Hauptkomponenten darstellen koennen. Dazu betreffen wir erstmal die Matrix V.  V ist eine autogonale Matrix, und zwar genau die Matrix, die bei der SVD herauskommt.", "start": 556.76, "end": 572.12}, {"text": "  Jetzt endlich verwenden wir nur eine Teilmenge von V, also nur die ersten P-Spalten-Vektoren.  Hier gucken wir uns aber nochmal die gesamte Matrix V an, und die ist eben autogonal. Das heisst,  die Spalten-Vektoren bilden im besten Fall eine Basis von dem Vektorraum, in dem wir uns aufhalten.", "start": 572.2, "end": 592.28}, {"text": " Muss sich immer der Fall sein, das kann zum Beispiel sein, dass sie weniger Datenpunkte  haben als Datenpunkte Dimensionen haben. In dem Fall koennen wir nicht genug Hauptkomponenten finden,  um tatsaechlich eine Basis zu haben von den Vektorraum, von den enddimensionalen Vektorraum.  Aber im Idealfall ist es so, dass wir genau eine Basis finden.", "start": 592.28, "end": 608.9200000000001}, {"text": "  Das heisst, V stellt auch eine Basiswechsel dar. Das heisst, ich koennte jetzt ein Vektor X auf V  rauf multiplizieren. Dann wuerde ich den Vektor X aus der Standardbasis wechseln in die Eigenbasis von  den Spalten von V. Und da wir jetzt wissen, dass V autogonal ist und V transponiert,  dann die Inverse dieser Matrix zu haben muss.", "start": 608.9200000000001, "end": 633.88}, {"text": "Das heisst, V transponiert eben auch wieder  im Basiswechsel, aber genau in die umgekehrte Richtung. SV transponiert mal X, das heisst,  genau ein Basiswechsel von X aus der Eigenbasis zurueck in die Standardbasis. Ja, und die Hauptkomponenten  in den Spalten von V werden dabei dann quasi mit den Koordinaten-Aktien zusammengefuehrt.", "start": 633.88, "end": 652.12}, {"text": "Also,  X lebt quasi in der Eigenbasis und wenn wir jetzt X mit V transponiert multiplizieren,  dann drehen wir X zurueck in den Koordinaten-Urschwung. Es entspricht quasi dem, dass die Achsen,  also die Hauptkomponenten, die Richtung der Hauptkomponenten mit den Koordinaten-Achsen  zusammenbringen. Gucken wir uns einmal gleich grafisch genauer an.", "start": 652.2, "end": 677.4}, {"text": "Hier noch einmal ausgedut,  V transponiert mal X. In V transponiert stehen ja in den Zeilen die Hauptkomponenten,  als V transponiert, V1 transponiert, die erste Zeile mit der ersten Hauptkomponente,  wenn wir jetzt mal X multiplizieren, daneben im wesentlichen, neben Eintrag das Skalaprodukt  zwischen der i-ten Hauptkomponente und X.", "start": 677.48, "end": 701.4}, {"text": "Weil es in dem ersten Eintrag steht, dann die Hauptkomponenten,  das Skalaprodukt zwischen der ersten Hauptkomponente und X. Und jetzt gucken wir uns das einmal  grafisch an, was ich meinte. Also, hier haben wir unsere Datenwolke eingetraben mit den ersten  beiden Hauptkomponenten.", "start": 701.4, "end": 717.4}, {"text": "Und wenn wir jetzt diese Datenwolke mit V transponiert multiplizieren,  in den Spreiten von V stehen eben genau diese beiden Hauptkomponenten, dann entspricht das ein  Basisfaechse, eine Dreherung in diese Standardbasis.", "start": 717.4, "end": 735.56}, {"text": "Also, heisst die beiden Hauptkomponenten,  diese beiden Koordinaten-Achsen und damit die Richtungen der hoechsten und der zweithoechsten  Varianz in den Daten. Und der ist genau zusammengebracht mit den Koordinaten-Achsen.  Ja, und wir haben ja eben gesehen, dass das, woops, dass das im Skalaprodukt entspricht hier.", "start": 737.72, "end": 747.4}, {"text": "  Und wenn wir uns jetzt mal vorstellen, dass uns jetzt in den zweidimensionalen Beispiel hier  diesen Punkt rausnimmt, da wo man koerde ist, nehmen wir ja das Skalaprodukt mit den ersten  Eigenvektoren. Heisst, wir projizieren den Datenpunkt auf den ersten Eigenvektoren. Dann nehmen wir  noch das Skalaprodukt mit den zweiten Eigenvektoren.", "start": 747.4, "end": 770.2}, {"text": "Das heisst, wir projizieren den Punkt hier auf  den zweiten Eigenvektoren. Und jetzt endet, drehen wir den Punkt in den Koordinaten-Urschwurm.  Heisst, diese Achse hier, diese Richtung ist genau die x-Achse und diese Richtung ist genau die  y-Achse.", "start": 770.2, "end": 787.0}, {"text": "Heisst, durch das Skalaprodukt kriegen wir genau den x-Wert und den y-Wert durch das  kreize Skalaprodukt von den Punkt. Dann werden wir dem bezueglich der Standardbasis auffassen.  Wir reden das Ganze dann halt in den Ursprung und dann sind die Koordinaten-Achsen zusammengebracht  mit den Hauptkomponenten.", "start": 787.96, "end": 805.24}, {"text": "Jetzt habe ich noch mal ein konkretes Beispiel fuer dreidimensionale Daten,  wo das noch mal hoffentlich klar wird. Mit dreidimensionalen Daten koennen wir maximal  drei Hauptkomponenten finden. Jetzt wollen wir den Punkt x mit den Eintraegen x, y und z auf  diese drei Hauptkomponenten raubmultiplizieren. Wir drehen diesen Punkt quasi in den Koordinaten-Urschwurm.", "start": 805.24, "end": 830.92}, {"text": "  Und dann haben wir das Skalaprodukt mit den ersten Eigenvektoren, mit den zweiten Eigenvektoren,  mit dem x, mit den dritten Eigenvektoren, mit dem x usw. Dann kriegen wir eben x' y' und z' raus.  Also genau dieses Skalaprodukt mit den Hauptkomponenten.  Und das Ziel hinter der Hauptkomponenten-Analyse ist ja jetzt, dass sie die Dimension reduzieren.", "start": 830.92, "end": 853.56}, {"text": "  Das heisst, wir wollen eben nicht alle Hauptkomponenten neben nur die ersten Pi-Hauptkomponenten,  die wirklich signifikant sind. Wir stellen uns mal vor, wir lassen diese dritte Hauptkomponente weg  und streichen die raus und berechnen quasi nur das Skalaprodukt mit den ersten beiden Hauptkomponenten.", "start": 853.56, "end": 872.92}, {"text": " Dann greifen wir nur noch x' und y' raus und diese Information, dieses Z-Strich,  also das Skalaprodukt mit der dritten Hauptkomponente, faellt eben weg.  Das ist trotzdem noch wohl definiert, weil ja die Hauptkomponenten genau drei Antraege haben,  genauso wie unser Vektor, mit dem wir multiplizieren. Das heisst, das ist kein Problem.", "start": 875.4, "end": 888.44}, {"text": "  Wir kriegen dann zwei dimensionale Daten raus, also mit einer Dimension weniger  und mit einer Dimension weniger, mit den zwei Dimensionen,  koennen wir jetzt zum Beispiel effizienter weiterarbeiten und effizienter,  zum Beispiel Beder oder diese Datenpunkte vergleichen.", "start": 888.44, "end": 911.4000000000001}, {"text": "Das Ganze kommt halt dann erst zum  Tragen, wenn wir wirklich viele Dimensionen haben, also zum Beispiel Taunendimensionen,  aber nur die ersten zwei Hauptkomponenten, 200 Hauptkomponenten verwenden.  Dann sparen wir uns quasi 800 Dimensionen in den rechten Operationen und koennen quasi nur  noch mit 200 Hauptkomponenten weiterarbeiten. Das ist also sehr effizienter.", "start": 911.4000000000001, "end": 927.24}, {"text": "  Vorausgesetzt natuerlich die Repraesentationen durch die Hauptkomponenten ist angemessen.  Und jetzt wissen wir ja, von V transponiert ist V genau die Inverse. Das haben wir jetzt wieder  auf eine Heilmenge von V abbilden, also auf die ersten beiden Spaltenvektoren,  also quasi diese Operationen in gewissen Sinne rueckgaengig machen.", "start": 927.88, "end": 951.08}, {"text": "Dann haben wir eine  Linearkombination entlang dieser ersten beiden Eigenvektoren, diese ersten beiden Spaltenvektoren  und dann kreben wir approximativ wieder den Vektor raus, den wir angefangen haben.", "start": 951.88, "end": 967.88}, {"text": "Aber  wir haben eben diese Informationen, die dritte Hauptkomponente verloren, die haben wir ja nicht  mit berechnet hier. Also diese Informationen hier faellt raus und jetzt endlich kombinieren wir nur  zwei Vektoren zusammen.", "start": 968.2, "end": 981.48}, {"text": "Also wir haben nur noch eine lineare Kombination von zwei Vektoren und dementsprechend  koennen wir auch nur noch einen zwei Dimensionalen Unterraum besetzen, dann in den drei Dimensionalen  Raum, den wir hier wieder raus bekommen. Als hier in dieser Bildmenge fehlt jetzt endlich ein  eine Dimension.", "start": 981.48, "end": 1001.0799999999999}, {"text": "Wir haben eben eine Dimension uns hier gespart haben und die nicht mehr mitgespeichert  haben. Diese Informationen koennen wir jetzt auch nicht mehr rekonstruieren.", "start": 1001.0799999999999, "end": 1016.04}, {"text": "Also jetzt endlich  macht das Sinn, weil wir haben ja dieses Skalarprodukt hier berechnet und ich habe gesagt, dass es quasi  die Informationen, wo wir uns entlang der ersten Hauptkomponente befinden, also wir berechnen  das Skalarprodukt mit der ersten Hauptkomponente, heute die Informationen an welcher Stelle wir", "start": 1016.04, "end": 1021.56}, {"text": " uns entlang der Hauptkomponente befinden. Das ist ja quasi genau der Punkt, in dem wir uns entlang  der X-Achse befinden.", "start": 1022.28, "end": 1041.72}, {"text": " So, genauso berechnen wir halt den Punkt und entlang der Y und Aachse nehmen wir uns befinden  und jetzt rekonstruieren wir das halb wieder, indem wir halt diese beiden Hauptkomponenten  nehmen und die halb wieder zusammen werfen.", "start": 1041.72, "end": 1050.44}, {"text": "Insgesamt nehmen wir nur die ersten P-Hauptkomponenten  fuer den Basiswechsel und das sind eben genau diese P-Hauptkomponenten, die wir im vierten Schritt  bestimmt haben. Da haben wir ja gesagt, wir wollen die Hauptkomponenten auswaehlen und  die, die nicht signifikant sind, also geringe, singulaerwerte Halt verwerfen.", "start": 1050.44, "end": 1063.16}, {"text": "Jetzt habe ich  aber hier ein dreidimensionales Beispiel dafuer erstellt, heisst hier in dieser Funktion zeige ich  mir dreidimensionale Daten und stelle die jetzt hier in einem Scatterplot dar.", "start": 1063.16, "end": 1079.48}, {"text": "Dann sehen wir,  wir haben jetzt hier also normal verteilte Daten und die haben jetzt entlang der Hauptkomponenten  verschiedene Varianten. Also wir sehen schon die erste Hauptkomponent, das ist quasi die hier,  wenn in der Richtung strengt, dann sind die Daten am meisten.", "start": 1079.48, "end": 1088.6000000000001}, {"text": "Also in der Richtung haben wir die  hoechste Signifikanz. Dann haben wir irgendwie noch eine Hauptkomponenten hier und noch eine dritte  Hauptkomponenten dann halt hier durch. Dreidimensionale Daten bedeutet mal zwei mal drei Hauptkomponenten.", "start": 1088.6000000000001, "end": 1106.2}, {"text": " So, jetzt koennen wir sagen, wir wollen die Informationen ueber diese dritte Hauptkomponenten  verwerfen. Also wir werfen die dritte Hauptkomponenten raus, pojizieren die Daten nur auf die ersten  beiden Hauptkomponenten, dann bekommen wir also einen x- und ein y-Stich. Damit koennen wir  jetzt weiterarbeiten.", "start": 1106.2, "end": 1121.88}, {"text": "Was wir jetzt in diesem Beispiel machen wollen ist dieses x- und dieses y-Stich  verwenden um die Daten wieder zurueck zu konstruieren. Als wir kombinieren, machen wir dann die Naehkomponenten  der Hauptkomponenten und koennen damit approximativ unsere Daten wieder zurueck.", "start": 1121.88, "end": 1131.72}, {"text": "Aber wie ich gesagt habe,  leben wir halt dann nur noch in einem zweidimensionalen Unterraum, weil wir ja nur zwei Hauptkomponenten  haben, die wir wieder linear kombinieren koennen. Wir koennen halt maximal einen zweidimensionalen  Unterraum ausspannen. Also lasse ich jetzt hier die dritte Hauptkomponente weg.", "start": 1131.72, "end": 1147.96}, {"text": "Also ich bemoove  eine Hauptkomponente und stelle das Ergebnis wieder in einem 3D-Plot dar. Ich schaue nochmal aus,  dass ich bewegen kann.", "start": 1147.96, "end": 1164.2}, {"text": "Und jetzt sehen wir halt hier, okay, wir haben halt immer noch diese beiden  Richtungen der hoechsten Sidenifikanz, also diese beiden ersten Hauptkomponenten, aber die Informationen  lang der dritten Hauptkomponenten, diese Richtungen, die ist jetzt verloren gegangen. Die haben wir  quasi aufgegeben und rausgeworfen.", "start": 1164.2, "end": 1173.56}, {"text": "Hier besprechen wir jetzt halt die Daten, wie ich hier wieder  rekonstruieren. Dann leben wir halt wieder nur in einem zweidimensionalen Unterraum,  jetzt endlich in den dreidimensionalen Raum.", "start": 1174.2, "end": 1189.08}, {"text": "Und genau, diese Riff, jetzt haben wir aber hier,  jetzt endlich trotzdem wieder drei Koordinaten, die wir brauchen, um diese Daten in den dreidimensionalen  Raum zu beschreiben. Hier koennte man jetzt nicht mehr effizient weiterarbeiten.", "start": 1189.08, "end": 1199.0}, {"text": "Das heisst,  haben wir Dimensionsreduktion durchfuehren wollen, um effizient rechnen zu koennen, dann projizieren  wir die zurueck. Dann projizieren wir halt nur einmal auf die ersten die Hauptkomponenten und mit  denen arbeiten wir dann. So, hier zur grafischen Veranschaulichung habe ich das halt zurueck  transformiert.", "start": 1199.0, "end": 1214.4399999999998}, {"text": "Und was Sie jetzt hier feststellen koennen und festhalten koennen, ist, haben wir die  Daten erstmal in der Dimension reduziert gehabt, dann koennen wir nicht mehr die Daten vollstaendig  wieder herstellen. Als wir haben einen gewissen Informationsverlust, den wir dann natuerlich mit  den Kauf nehmen. Dafuer, dass wir mit weniger Dimensionen arbeiten koennen.", "start": 1214.4399999999998, "end": 1231.9599999999998}, {"text": "Ja, dann kommen wir jetzt  zum letzten Punkt. Der gehoert eigentlich nicht mehr wirklich zu der Hauptkomponenten-Analyse dazu.  Der Vollstaendigkeit-Tarber, wer hier das aber auch in der Hausaufgabe mit implementieren soll,  wollen wir das noch einmal ganz kurz mit anschauen.", "start": 1231.9599999999998, "end": 1245.88}, {"text": "Da geht es jetzt halt am Schluss darum, die Bilder  effizient vergleichen zu koennen. Das heisst, wir haben jetzt ja die Dimension reduziert und koennen  jetzt quasi in der Bilder effizient vergleichen.", "start": 1245.88, "end": 1264.76}, {"text": "Und so schreiben wir die reduzierten Bilder als  i- bis i- von k, also wir haben k reduzierte Bilder, da haben wir es halt eben nur noch p Dimension,  ein Staffel an Enddimension, weil wir nur p Hauptkomponenten verwendet haben. Genau, also wir haben  diese Bilder i, dann quasi auch die ersten p Hauptkomponenten draufprojektiert.", "start": 1264.84, "end": 1277.48}, {"text": "Jetzt haben wir  ein Bild x, das gesucht ist, dass wir also eben ein Bild i1 bis ikh zuordnen wollen. Also wir wollen  das Bild finden, dass die im x am ehesten ist. Aber momentan noch das Problem, dass x, der trotzdem  wieder Enddimensionen hat, heisst auch x, muessen wir erstmal in den Dimensionen reduzieren.", "start": 1277.48, "end": 1294.92}, {"text": "Als x  projizieren wir auch auf genau die gleichen Hauptkomponenten wie die Bilder, die wir hier  schon auf die reinen Hauptkomponenten projiziert haben. Projizieren x auf die ersten p Hauptkomponenten  von den Bildern und dann koennen wir eben v-transponiert, v-, weil wir eben nur die ersten p  Hauptkomponenten nehmen, bei x.", "start": 1294.92, "end": 1320.76}, {"text": "Und das ergibt dann unter x-, das dann eben auch nur noch die  p Eintraege hat fuer die p Hauptkomponenten. Und jetzt koennen wir effizient quasi das Bild,  das Bild, das das Bild aus der Datenbank am ehesten ist, finden. In dem wir einfach den Abstand zu all  die Bilder berechnen.", "start": 1321.56, "end": 1337.8}, {"text": "In dem Abstand filmen wir eben, indem wir die Differenz zwischen den  Vektoren nehmen und dann die Normen bestimmen. In jedem Fall sogar die quadratischen Normen,  das reicht aus. Wenn die normale L2-Norm am kleinsten ist, dann ist auch die quadratische Norm  klein, weil die kleinste.", "start": 1337.8799999999999, "end": 1359.56}, {"text": "Weil die quadratische Funktion halt fuer positive Werte  monotonenwachsend ist, genauso wie die Wurzelfunktion hat monotonenwachsend ist.  Und ja, dann koennen wir einfach hier die quadratischen Normen nehmen.  Ja, genau.", "start": 1360.44, "end": 1378.28}, {"text": "Und dadurch, dass wir halt jetzt hier nur noch die Stattendimensionen haben,  koennen wir halt den Vergleich, also diese Normen halt effiziente ausrechnen,  muessen hier nur noch ueber p Eintraege summieren anstatt halt eigentlich N-Eintraege,  wie wir urspruenglich mal Dimensionen hatten.", "start": 1379.3999999999999, "end": 1392.44}, {"text": "Also soweit zu dem Algorithmus zur Hauptkomponenten-  Analyse. Viel Spass bei den weiteren Videos. Und genau, ich habe dann noch ein extra Video,  wo ich nochmal ein paar Admin-Specials bitte zeigen.", "start": 1392.44, "end": 1396.8400000000001}]}]