[{"lecture": "25797_67_course_video", "Timestamps": [{"text": "  Im letzten Video wollen wir uns jetzt noch mal drei Anwendungsbeispiele zu der Singulaer-Wertzerlegung  und der Hauptkomponenten-Analyse anschauen.  Vorab ihr muesst den Code nicht komplett verstehen und wahrscheinlich werden wir das Notebook hier auch nicht mit hochladen,  weil wir Daten mit verwenden, die wir so nicht mit veroeffentlichen koennen.", "start": 0.0, "end": 26.0}, {"text": "  Das erste Beispiel, das uns anschauen wollen, ist der Google Page-Rank Algorithmus in einer ganz urschwinglichen Form.  Dazu verwenden wir hier die Library NetworkX.  Mit NetworkX koennen wir einen Grafen definieren und aufstellen und auch dann gut grafisch darstellen und veranschaulichen.  Und NetworkX implementiert auch eine eigene Version von dem Page-Rank Algorithmus.", "start": 26.0, "end": 45.0}, {"text": "  Und dann haben wir noch eine eigene Variante von dem Page-Rank Algorithmus  und die wollen wir dann mit der Implementation von NetworkX vergleichen.  Die Idee hinter dem Page-Rank Algorithmus ist, dass wir verschiedene Webseiten haben.  Die werden hier in dem Grafen was Knoten dargestellt und die Webseiten verlinken sich untereinander.", "start": 45.0, "end": 72.0}, {"text": " Das heisst, eine Webseite verlinkt eine andere Webseite  und das mehr Verlinkung eine Webseite hat, desto wichtiger ist sie.  Und desto interessanter koennte sie sein fuer Suchergebnisse.  Und jetzt endlich berechnen wir fuer alle Knoten eine Wahrscheinlichkeitsverteilung,  wie wichtig und interessant diese Webseite ist.", "start": 72.0, "end": 89.0}, {"text": "  Oder wie wahrscheinlich es ist, dass eine andere Webseite quasi auf diese Webseite verlinkt.  Und genau, jetzt haben wir hier einmal den Page-Rank Algorithmus von NetworkX.  Ist hier gerade etwas unguenstig dargestellt.  Leider sind die beiden Plotze immer sehr unterschiedlich.  Jetzt sind sie ungefaehr aehnlich.  Unsere eigene Variante funktioniert so, dass wir die AdjZs Matrix aufstellen.", "start": 89.0, "end": 122.0}, {"text": "  Das heisst, wir lassen uns von der NetworkX die AdjZs Matrix zurueckgeben  und dann berechnen wir den Eigenvektor zum groessten Eigenwert durch die Power-AdjZs-AdjZs-Matrix.  heisst pr ist genau der Eigenvektor zu den groessten Eigenwerts.  Und das steht dann schon die Loesung von unserem Page-Rank Algorithmus hier in der einfachen Form da.  Wir geben dann oder in dieser Zeit renommieren wir erstmal.", "start": 122.0, "end": 153.0}, {"text": "  Also wir wollen ja eine Wahrscheinlichkeitsverteilung ueber diese Webseiten zurueckbekommen  und eine Wahrscheinlichkeitsverteilung hat immer die Eigenschaft, dass sie in ihrer Summe genau eins ergibt.  Das heisst, sie teilen hier immer durch die Summe und dadurch ist dann sichergestellt, dass sie genau auf eins kommen  mit einem Wahrscheinlichkeiten zusammenadiert.", "start": 153.0, "end": 174.0}, {"text": "  Dann stellen wir eben diese Wertschoepfung, die den Eigenvektor einmal da auf diesen verschiedenen Knoten.  Vielleicht nochmal ein Hinweis zu der Normalisierung.  Das ist etwas anderes, was wir Einheitslaenge fordern.  Da fordern wir bei der Einheitslaenge, dass sie normgenau eins ist.  Das ist ein Unterschied dazu, dass sie fordern, dass sie summagenau eins ist.", "start": 174.0, "end": 193.0}, {"text": "  Dann kann man sich an einem einfachen Beispiel am Anschauen und sieht man das relativ eindeutig.  Und jetzt sehen wir hier, dass die Werte von den Knoten genau gleich sind.  Also wir kommen sowohl ueber den Network X-Page-Rank Algorithmus hier oben  als auch bei unseren eigenen Page-Rank Algorithmus genau die gleichen Werte zurueck.", "start": 193.0, "end": 215.0}, {"text": "  Also die Wahrscheinlichkeit 0,29 fuer die den Turquis haben den Knoten, 0,19 fuer Blau, 0,13 fuer Gila  und dann 0,39 fuer den Gaebenknoten.  Das Ganze kommen wir jetzt nochmal fuer einen anderen komplizierteren Grafen ausfuehren.  Jetzt kommentiere ich das sehr einmal kurz aus.  Ich fuelle das nochmal aus.  Geben wir hier wieder zwei Knoten.", "start": 215.0, "end": 236.0}, {"text": "  Und ja leider ist die Darstellung hier wieder ein bisschen unterschiedlich.  Aber im Wesentlichen bekommen wir wieder fast die gleichen Werte raus.  Also 0,03 und 0,02, kleine Unterschied, aber fast das gleiche.  Wir haben 0,05 und 0,02 hier, wieder fast das gleiche.  Hier der Bogen hat ungefaehr auch die gleichen Werte.", "start": 236.0, "end": 261.0}, {"text": "  Im besten Fall spielte ich die Knoten zueinander zuordnen in den beiden Grafen.  Aber im Wesentlichen bekommen wir ungefaehr die gleichen Loesung.  Wir wollen jetzt nicht ins Detail gehen, warum genau das funktioniert mit den Eigenvektoren.  Da koennt ihr selber einmal in Ruhe drueber nachdenken.", "start": 261.0, "end": 281.0}, {"text": "  Ich persoenlich finde das nur sehr interessant, dass man eben auch durch Eigenvektoren Informationen ueber ein Grafen bekommen kann,  indem man sich eben die Adressionsmatrix anschaut.  Dann wollen wir uns ein weiteres Beispiel angucken, naemlich Komprimierung von Bildern.  Und zwar haben wir jetzt hier auf der linken Seite einen Graustufenbild.", "start": 281.0, "end": 296.0}, {"text": "  Auf der rechten Seite haben wir dasselbe Graustufenbild,  nur einmal kompromiert durch die Singulaerwaertserlegung.  Und ja, die Idee dahinter ist, dass ihr ja die Daten auf die ersten P-Hop-Komponenten projizieren koennt.  Und dann brauchen wir ja weniger Dimensionen.  Das heisst, wir sparen irgendwie Informationen, die wir irgendwie speichern muessen  und koennen damit unsere Daten komprimieren.", "start": 296.0, "end": 322.0}, {"text": "  Das Problem dahinter ist, um das tatsaechlich wieder Bild zu rekonstruieren,  brauchen wir wieder die Hop-Komponenten.  Also, wir haben ja dann quasi eine reduzierte Anzahl im Pixel.  Die koennen wir aber jetzt nicht das Bild darstellen und nicht auffassen.  Die koennen wir nicht direkt visualisieren.", "start": 322.0, "end": 338.0}, {"text": "  Wenn wir ein Bild aber komprimieren wollen, wollen wir uns das Bild ja immer wieder angucken koennen.  Damit wir das Bild wieder angucken koennen, muessen wir wieder zurueckprojizieren  in unsere eigentlichen Enddimensionen.", "start": 338.0, "end": 362.0}, {"text": " Das heisst, wir muessen die Hauptkomponenten, die wir am Anfang projiziert haben,  die wir fuer die Datenreduktion benutzt haben, die muessen wir uns trotzdem speichern,  damit wir die wieder lineaer kombinieren koennen, um eine Approximation von dem urspruenglichen Bild zu berechnen.  Und das macht dann den eigentlichen Gewinn.", "start": 362.0, "end": 373.0}, {"text": "  Also, die Information, die man nicht mit speichern muss, den Gewinn macht das wieder komplett zunichte.  Weil ich ja jetzt mehr Informationen speichern muss, naemlich all diese Hauptkomponenten,  all die Hauptkomponenten haben ja auch wiederum genau einen Einschraeger.  Heisst, jetzt endlich muss dann noch sehr viel mehr speichern.", "start": 373.0, "end": 392.0}, {"text": "  Also, kurzum, die SVD ist tatsaechlich nicht geeignet, um Bilder von dem Tee zu komprimieren.  Da gibt es Algorithmen, die das sehr viel besser umsetzen.  Sollte also ein kurzes Anwendungsbeispiel sein, wo eben die SVD oder die Hauptkomponenten-Analyse nicht geeignet ist.  Dann habe ich noch ein drittes Beispiel aus meinem eigenen Studium.  Hier geht es um Braincomputern der Fessinen Daten.", "start": 392.0, "end": 420.0}, {"text": "  Also, endlich hat man auf der Kopfhaut verschiedene Elektronen, in dem Fall bis zu 50 Elektronen,  und die messen irgendwie die Aktivitaeten von unterm Gehirn.  Und jetzt endlich hat man dann ja einen Datenschrom ueber die Zeit.", "start": 420.0, "end": 442.0}, {"text": " Das heisst, jeden Zeitpunkt misst man einmal auf den 650 Elektronen,  und jeder Zeitpunkt ergibt dann einen Datenpunkt, und jeder Datenpunkt besteht dann eben aus genau 56 Eintraegen.  In dem Fall haben wir hier sogar genau 118 Elektronen.  Das heisst, wir haben hier eine Datenmatrix 118 mal 5958.  In dem Fall haben wir die Daten, glaube ich, sogar in die Spalten geschrieben.", "start": 442.0, "end": 462.0}, {"text": "  Genau. Also, das entspricht nicht der Datendarsteller und so, wie wir sie haben wollen in der Hausaufgabe,  wo wir die Datenpunden in die Zeilen schreiben.  Also, ja, gesagt, analog kann man die Daten auch in die Spalten schreiben, so ist es hier gemacht worden.", "start": 462.0, "end": 482.0}, {"text": " Und ja, jeder Datenpunkt hat 118 Eintraege fuer die Einzel der Elektronen,  und dann haben wir 5958 Messungen zu unterschiedlichen Zeitpunkten gemacht.  Diese Daten koennen wir dann halt einmal darstellen.  Und was Sie jetzt machen wollen, also die urspruenglichen Daten sind dann hier in Rot dargestellt.", "start": 482.0, "end": 497.0}, {"text": "  Es ist allerdings nur ein Kernal, also nur eine Elektrode von diesen 118 Elektroden ueber der Zeit dargestellt.  Was Sie jetzt sehen hier ist, dass diese Signal relativ viel rauschend drin hat.  Also, hier ist relativ viel Bewegung mit drin.  In die Hop-Komponenten-Analyse koennen wir jetzt benutzen, um eben diese Bewegung, also dieses Rauschen mit rauszufuelltern.", "start": 497.0, "end": 520.0}, {"text": "  Und zwar haben wir, da nehmen wir an, dass in den Daten auch mit Archefakten mit drin sind, die zum Beispiel durch Muskelbewegungen entstehen.  Also, wir messen nicht nur direkt das mit, was von unseren Gehoern ausgestaltet wird,  wir messen zum Beispiel auch die Bewegung von unseren Kiefermuskeln,  wir messen die Bewegung von unseren Augen und so weiter.", "start": 520.0, "end": 541.0}, {"text": "  Und das sind eben Archefakte, und sehr viel staerker Auswahl als die eigentlichen Gehirn-Daten, also der eigentlichen Daten von den Gehoern.  Das heisst, die hoechste Varianz in den Daten entsteht durch die Augenbewegungen zum Beispiel.", "start": 541.0, "end": 559.0}, {"text": " Jetzt ist ein bisschen die Idee, dass Sie sagen, okay, die ersten beiden Hop-Komponenten mit der hoechsten Varianz,  es sind eben genau nur solche Muskeartefakte, also nur Bewegungen, die wir eigentlich nicht haben wollen.", "start": 559.0, "end": 573.0}, {"text": " Also andersherum als bei den Bildern, wo wir sagen, die ersten p-Hop-Komponenten sind am Aussagellkraeftigsten,  nehmen wir hier die Hop-Komponenten mit geringerer Varianz und sagen, das sind eben die Aussagellkraeftigen  aufgrund der Gegebenheiten von unserem Problem Wissen, dass eben diese ersten beiden Hop-Komponenten eher Daten sind", "start": 573.0, "end": 587.0}, {"text": " oder eher Dinge gemessen haben, die wir eben nicht in unseren Daten haben wollen.  Der Trick dahinter ist jetzt, ihr muesst ihr wie gesagt nicht genau nachvollziehen, was da passiert.  Ist das ja einmal, und die Daten auf die ersten beiden Hop-Komponenten projizieren,  danach die Daten wieder zurueckprojizieren.", "start": 587.0, "end": 613.0}, {"text": " Das heisst, Sie haben wieder die urspruenglichen 118 Dimensionen,  dadurch, dass Sie aber nur auf die ersten beiden Hop-Komponenten projiziert haben und danach wieder die ersten beiden Hop-Komponenten linear kombiniert haben,  geben wir quasi nur die Daten entlang dieser ersten beiden Hop-Komponenten, also nur die Daten, die quasi Rauschen enthalten, also Artifakte, die wir nicht haben wollen.", "start": 613.0, "end": 633.0}, {"text": "  Und der Trick ist jetzt, dass wir diese Daten, dieses Noise, einfach von unseren eigentlichen Daten, die wir haben wollen, abziehen.  Und dann sehen wir, dass wir halt dadurch die Daten umwieglitten koennen.  In dem naechsten Block von wissenschaftlichen Rechnungen wollen wir uns mit der Fourier-Transformation beschaeftigen.", "start": 633.0, "end": 652.0}, {"text": "  Und Fourier-Transformation macht es moeglich, bestimmte Frequenzen aus Signalen rauszufiltern.  Es kommt also, solche da beschaeftigen, wir uns mit sehr aehnlichem Signal, wie wir sie quasi hier haben, naemlich mit Audio-Signalen,  an der Frequenzen rausfiltern.", "start": 652.0, "end": 676.0}, {"text": " Und einmal als Beispiel, zudem, oder als Gegen- oder als Vergleich zu dem, was ich hier gemacht habe,  habe ich jetzt in diesem Beispiel die Daten einmal gesaeubert, in dem ich eben bestimmte Frequenzen rausgefiltert habe.  Also, das wird sehr eh nicht zu dem sein, was wir dann in dem naechsten Themenblock an wissenschaftlichen Rechnungen machen wollen.", "start": 676.0, "end": 698.0}, {"text": "  Jetzt sehen wir halt hier, dass die Variante mit der Hauptkomponentenanalyse die Daten in einer sehr aehnlichen Weise quasi saeubert und verbessert und aufwertet, aber nicht genauso.  Also, natuerlich ist das Ergebnis, dass die Hauptkomponentenanalyse leicht ein anderes, aber trotzdem koennen wir halt unsere Daten dadurch irgendwie aufwerten.", "start": 698.0, "end": 715.0}, {"text": "  Genau, also noch mal zusammengefasst, da haben wir hier drei sehr unterschiedliche Varianten, wie wir die Hauptkomponentenanalyse oder Eigenwerte nutzen koennen.  Im ersten Beispiel nutzen wir halt die Adhensmatrix an das Grafen, um Informationen ueber den Grafen zu bekommen.", "start": 715.0, "end": 730.0}, {"text": "  Im zweiten Beispiel haben wir gesehen, dass wir zum Beispiel darueber nachdenken koennen, Bilder zu kompromieren, haben aber festgeschaetet,  dass das halt im Allgemeinen nicht so gut funktioniert und die Hauptkomponentenanalyse eben da nicht die richtige Methode ist.", "start": 730.0, "end": 747.0}, {"text": "  Im dritten Beispiel haben wir gesehen, dass wir eben auch die ersten beiden Hauptkomponenten eben als sinnlose Informationen ansehen koennen  und die dann zum Beispiel aus Daten rausfiltern koennen, um die Daten aufwerten zu koennen.  Alles klar, das war es mit den Anwendungsbeispielen.", "start": 747.0, "end": 765.0}, {"text": "  Ich hoffe, das hat nochmal euch ein bisschen einen Ausblick gegeben, wofuer diese Methoden gut sind und warum es interessant sein koennte, die zu verstehen.", "start": null, "end": 765.0}]}]