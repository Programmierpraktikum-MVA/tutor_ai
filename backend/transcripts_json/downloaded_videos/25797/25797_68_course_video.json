[{"lecture": "25797_68_course_video", "Timestamps": [{"text": "  Tut mir leid, ich hatte es auch nicht mehr auf dem Schirm, dass wir heute woanders sind,  zumal die Mathematik auch den Raum gar nicht braucht.  Wir also unsinnigerweise umgezogen sind, aber gut.  Also, fuer heute machen wir jetzt Folgendes.", "start": 0.0, "end": 44.0}, {"text": " Wir muessen uns zuerst nochmal beschaeftigen, wie versprochen sozusagen mit der Frage danach,  was eigentlich bedeutet, wenn wir mit Matritzen rechnen,  Gleichungssysteme loesen, mit Matritzen multiplizieren und dabei Gleichbommerzahlen verwenden.  Okay, und der Begriff, auf den das letztendlich hinauslaeuft, heisst Kondition.  Und damit ist gemeint die Kondition einer Matrix.", "start": 44.0, "end": 60.0}, {"text": "  Okay, und die Situation ist folgende.  Wir haben irgendeine Matrix.  Und sagen wir mal, wir wollen ein lineares Gleichungssystem loesen.  Das, was wir uns nachher ueberlegen, gilt dann letztlich fuer jedes Produkt mit einer Matrix.  Also, diese Matrix wird multipliziert mit einem Vektor X.  Und wenn wir ein Gleichungssystem loesen wollten, dann waere das Gleichirben einem Vektor B.", "start": 60.0, "end": 91.0}, {"text": "  Und wir wollen das Ganze in Gleitkommazahlen machen.  Das heisst, A ist eine Matrix aus den Gleitkommazahlen, sagen wir mal N-Cross N.  Und X und B sind Vektoren aus den Gleitkommazahlen.  Und okay, was bedeutet das?  Was ich da oben an die Wand geworfen habe, ist eine Illustration von Vektoren, sagen wir mal, X aus G-Hof 2.", "start": 91.0, "end": 138.0}, {"text": "  Also, ich habe alle Punkte gemalt fuer ein bestimmtes Gleitkommazalenformat.  Und auf der X-Achse kann ich irgendeine Gleitkommazale waehlen.  Und wir wissen schon, bei den Gleitkommazalen wird es dann zu Null hendichter.  Und dann ist es weniger dicht, noch weniger dicht, noch weniger dicht und so weiter.  Also, auf der X-Achse stehen irgendwelche Gleitkommazalen.", "start": 138.0, "end": 157.0}, {"text": "  Und auf der Y-Achse stehen auch irgendwelche Gleitkommazalen.  Und ich kann mir das so vorstellen, dass ich in einem gewissen Gebiet, sagen wir mal, das Gebiet ist hier, das Gebiet 1 0 1\u00b2.  Also links unten waere die Null, rechts oben waere die 1.  Dann sind das alle Punkte, die ich in Null 1\u00b2 mit Gleitkommazalen darstellen kann.", "start": 157.0, "end": 181.0}, {"text": "  Ja, und das sind, das wussten wir schon vorher, endlich viele Gleitkommazalen.  Okay.  Jetzt moechte ich also Folgendes tun, sagen wir mal, A ist eine 2x2-Matrix.  Dann stehen in den Spalten dieser Matrix.  Wenn jetzt A als Beispiel eine 2x2-Matrix ist, dann steht da also A 0 0, A 1 0, A 1 1, sorry, A 0 1, A 1 1.  Und sagen wir mal, das ist A.", "start": 182.0, "end": 217.0}, {"text": "  Das heisst, hier steht so eine, in der Spalte, so ein Punkt aus dem Ding.  Und in der anderen Spalte steht auch so ein Punkt aus dem Ding.  Und was ich jetzt mache, ist, ich multipliziere das mit x.  x ist auch wieder ein Paar von Gleitkommazalen.  Das heisst, ich nehme irgendein Punkt hier und skaliere den mit einer Gleitkommazal.", "start": 217.0, "end": 238.0}, {"text": "  Und dann nehme ich einen anderen Punkt, skaliere den mit einer Gleitkommazal.  Und dann bekomme ich ein Paar von Zahlen.  Die Frage ist jetzt, welche Zahlen koennen das sein?  Und das sind nicht notwendigerweise Gleitkommazalen, das wissen wir schon.  Also das Produkt von zwei Gleitkommazalen ist nicht unbedingt eine Gleitkommazal.", "start": 238.0, "end": 258.0}, {"text": "  Und die Summe von zwei Gleitkommazalen ist nicht unbedingt eine Gleitkommazal.  Das heisst, wir muessen wieder runden.  Also was machen wir?  Wir bilden A mal x.  Und fuer x wissen wir schon, aus diesem Quadrat gibt es nur endlich viele Moeglichkeiten.  Das heisst, es gibt auch nur endlich viele Ausgabepunkte, die diese Matrix hat.", "start": 258.0, "end": 283.0}, {"text": "  Und jetzt koennte man denken, wenn ich hier mal tausend verschiedene Gleitkommazal-Vektoren mit A multipliziere,  dann kriege ich auch tausend verschiedene Punkte.  Und so ist es aber nicht, weil ja alle Punkte, die ich darstellen kann, das sind alle, die ich darstellen kann.  Aber diese Matrix-Vektor-Produkte, die fallen nicht unbedingt auf diese Punkte drauf,  sondern die fallen irgendwo hin.", "start": 283.0, "end": 306.0}, {"text": "  Das sind erstmal irgendwelche, sagen wir mal, realen Zahlen, die fallen irgendwo hin und jetzt muss ich runden.  Und was bei diesem Runden passiert ist,  dass fuer zwei verschiedene Vektoren x das Produkt mit A auf die gleiche Zahl faellt.  Das heisst, in Gleitkommazalen,  in Gleitkommazalen wird mehr Folgen des passieren.", "start": 306.0, "end": 336.0}, {"text": " Es gibt also, sagen wir mal, x0, x und x' beides Gleitkommazalen,  sodass A mal x gleich A mal x' ist.  In Gleitkommazalen.  Das wird mir passieren.  Weil eben das Ergebnis gerundet werden muss, um wieder eine Gleitkommazal zu ergeben.  Und wenn das so ist, das ist gar nicht so schlimm.  Der Teil ist nicht so schlimm.", "start": 336.0, "end": 364.0}, {"text": "  Der schlimme Teil ist, wenn hier fuer verschiedene Werte x das Gleiche rauskommt,  dann bedeutet das also, dass einige von meinen Punkten auf einen Punkt zusammenfallen,  was auch bedeutet, einige Punkte, die jetzt hier vorhanden sind,  die kann ich nicht beschreiben als A mal x.", "start": 365.0, "end": 390.0}, {"text": " Also wenn ich jetzt mir alle angucke, wenn es guenstig laeuft, passiert Folgendes,  sieht dann zum Beispiel so aus,  das sind jetzt alle Punkte, die rauskommen,  wenn ich A mit x multipliziere fuer eine halbwegs gutmuetige Matrix A  und dann habe ich eine nicht so gutmuetige Matrix A, die koennte zum Beispiel so aussehen.", "start": 390.0, "end": 410.0}, {"text": "  Und was dabei passiert, ist, dass ganz viele Produkte A mal x in die Naehe dieser Diagonale fallen,  die werden dann auf irgendeine Gleitkommazal gerundet  und dummerweise bedeutet es auch,  dass hier in diesem Gebiet neben der Diagonale ganz viele Produkte ueberhaupt nicht existieren.", "start": 410.0, "end": 430.0}, {"text": " Es gibt also keine Gleitkommazal x,  sodass A mal x einen Wert erreicht, der hier unten liegt, fuer ganz viele Werte.  Das heisst im Allgemeinen sind in Gleitkommazalen solche Gleichungen ueberhaupt nicht zu loesen,  im Sinne eines Gleichheitszeichens.  Das ist sozusagen die zentrale Beobachtung hier.", "start": 432.0, "end": 456.0}, {"text": " Wenn einige Punkte von meinen endlich vielen Gleitkommazalen,  wenn einige auf die gleiche Gleitkommazalen abgebildet werden,  dann gibt es andere, die ueberhaupt nicht mehr da sind.  Und das ist das Bloede,  weil wenn B jetzt so ein Punkt ist, der hier auf dem ersten Bild drauf war,  aber hier nicht mehr drauf ist,  dann gibt es kein x, sodass A x gleich b ist.", "start": 456.0, "end": 475.0}, {"text": "  Obwohl die Gleichung in reellen Zahlen absolut eine Loesung hat,  sogar in rationalen Zahlen.  Das heisst im Allgemeinen muss ich in Gleitkommazalen erwarten,  dass es ein Residoe umgibt.  Also das waere wichtig, einzusehen.  In Gleitkommazalen gibt es nicht unbedingt eine Loesung  und dieses Muster, das wir da sehen, haengt von der Matrix A.", "start": 475.0, "end": 506.0}, {"text": " Und jetzt ist die Frage, wie haengt es von der Matrix A ab?  Also es gibt so ein Residoe um...  Klar.  Interessant.  Es gibt so ein Residoe um...  Selbst wenn wir wissen, dass die Gleichung in reellen Zahlen eine Loesung hat.  Wir nehmen also an, das ist das Residoe um,  und wir nehmen auch an, es gibt ein x-Sternchen,  fuer das das gilt.", "start": 509.0, "end": 539.0}, {"text": "  Nur das Problem ist, x-Sternchen ist nicht unbedingt eine Gleitkommazal,  aber aus den reellen Zahlen gibt es eine solche Zahl.  Und die Frage, die wir uns jetzt stellen,  ist, wie gross kann dieses Residoe umwerden?  Und was hat das mit der Matrix A zu tun?  Und was wir uns dafuer angucken wollen,  logisch wie immer,  ist der relative Fehler von diesem Residoe um.", "start": 539.0, "end": 570.0}, {"text": "  Und das heisst, was ist der relative Fehler von dem Residoe um?  Das ist also r, geteilt durch b.  Das ist der relative Fehler vom Residoe um.  Und wir fragen uns jetzt, wie gross ist dieser relative Fehler vom Residoe um?  Jetzt setzen wir einfach ein.  Das ist der relative Fehler.  Und das ist jetzt irgendwie eine einfache Transformation.  Dieser relative Fehler hat also diese Groesse.", "start": 570.0, "end": 650.0}, {"text": "  Und jetzt gehen wir davon aus,  dass, wenn jemand versucht, dieses Leichungssystem in Gleitkommazalen zu loesen,  dass man logischerweise x so waehlen wird,  dass das Residoe um moeglichst klein ist.  Also wir gehen von einer optimalen Wahl von x aus.  Und wir fragen uns, was trotz der optimalen Wahl von x  hier im schlimmsten Fall als Fehler rauskommen kann.", "start": 650.0, "end": 680.0}, {"text": "  Also wir haben x so gut wie moeglich gewaehlt.  Und wir fragen uns, was kann hier passieren?  Das heisst, was wir hier machen wollen,  ist, wir wollen gucken, wie gross dieser Bruch werden kann.  Wollen diesen Bruch so gross wie moeglich machen.  Okay.  Und wie machen wir den Bruch gross,  indem wir ein paar finden, x minus x Stern und x Stern,  so dass der Zaehler gross wird und der Nenner klein.  Klar.", "start": 680.0, "end": 711.0}, {"text": "  Und wir machen eine Worstcase-Abschaetzung.  Das heisst, wir fragen uns nicht wirklich,  ob unsere Annahmen in der Praxis eintreten koennen,  sondern wir ueberlegen uns, was eben im schlimmsten Fall passieren kann.  Und dazu machen wir hier eine kleine Ueberlegung.", "start": 711.0, "end": 735.0}, {"text": " Was hat es eigentlich hier mit Aufsicht,  mit diesem Matrix-Vektor-Produkt und der Norm davon?  Eine Beobachtung ist offensichtlich,  wenn wir y in der Laenge verdoppeln,  wenn wir y verdoppeln, was passiert dann?  Wenn wir hier 2 y nehmen,  dann bekommen wir auch 2 mal diese Norm von a y.", "start": 735.0, "end": 769.0}, {"text": " Also fuer diese Vektoren gibt es irgendwie,  also das, was die Matrix hier macht,  das kann ich auch beschreiben durch irgendeinen Skalar.  Also ich koennte zum Beispiel sagen,  wo ist mein Lappen?  Wir schreiben uns das jetzt so hin.  Wir sagen fuer y gibt es irgendeine Konstante,  die ich dann mit der Laenge von y multiplizieren muss.", "start": 770.0, "end": 814.0}, {"text": " Also es gibt y als Vektor  und fuer dieses y gibt es eine Konstante,  die mir sagt, was die Matrix damit macht  und wir beobachten, es kommt jetzt nicht mehr auf die Laenge von y an.  Das heisst, diese konstanten Cy, die haengen nicht ab von y insgesamt,  sondern nur von der Richtung von y.  Also es reicht, dass wir uns hier ueberlegen,  diese Konstante fuer einen Vektor mit Einheitslaenge.", "start": 814.0, "end": 842.0}, {"text": "  Das ist das, was man typischerweise macht.  Also ich koennte jetzt also Folgendes machen.  Egal, bleiben wir mal dabei.  Sagen wir also diese konstanten Cy,  unsere Ueberlegung ist also Cy und Cy' sind mit Sicherheit identisch,  wenn Y irgendein Vielfaches ist von Y'  Machen wir mal so.", "start": 842.0, "end": 886.0}, {"text": " Okay, deswegen genuegt es, dass wir uns als Vektor  solche anschauen fuer diese Konstante, die Einheitslaenge haben.  Ja, ich kann es versuchen.  Ich kann die Betragstriche laenger machen.  Ich versuch es ja.", "start": 886.0, "end": 932.0}, {"text": " Also, das alles gesagt, bedeutet jetzt Folgendes,  wie koennen wir diesen Zaehler so gross machen, wie es geht?  Wir sagen, das hier ist also irgendeine Konstante,  die sich aus A ergibt, so eine Konstante C,  die unter der Wahl von allen Y mit Einheitslaenge maximal ist.  Also es gibt irgendein Maximum fuer Y gleich 1.  Das ist die Konstante von A mal Y, das ist die Konstante.", "start": 932.0, "end": 962.0}, {"text": "  Und die multiplizieren wir jetzt mit der Laenge von X minus X.  Und was wir hier gemacht haben, ist, wir haben sozusagen  das Verhalten genommen, was maximal ungluecklich ist.  Es gibt irgendein Y mit Einheitslaenge,  so dass dieses A mal Y maximal gross wird.  Und wir sagen, das ist dieser Faktor CY fuer unser X minus X Stern.", "start": 962.0, "end": 989.0}, {"text": " Das muss ja nicht sein, aber das waere der gluecklichste Fall,  um diesen Bruch gross zu machen.  So, und das teilen wir jetzt durch.  Das Ding, was am kleinsten ist, wenn wir A mit so einem Einheitsvektor multiplizieren.  Und hier unten steht jetzt nur noch X Stern.  Das ist die Idee.", "start": 989.0, "end": 1023.0}, {"text": " Also wir nehmen an, dass diese Richtung X minus X Stern ungluecklich ist,  im Sinne von, das multiplizieren mit A macht diesen Vektor viel laenger.  Und wir nehmen fuer den Vektor X Stern an,  dass Multiplikation mit A den so kurz wie moeglich macht.  Weil wir wissen, dann wird dieser Bruch so gross, dass es eben geht.  Und jetzt kommt eine kleine Beobachtung.", "start": 1023.0, "end": 1040.0}, {"text": "  Jetzt sagen wir, was ist eigentlich das hier?  X minus X Stern geteilt durch X Stern.  Wem kommt das als Ausdruck bekannt vor?  Wir haben hier eine Gleitkommazahl, wir haben hier eine Reellezahl.  Wir ziehen die Ab von der Gleitkommazahl, die ziemlich nah dran ist,  an dieser Reellenzahl und teilen durch die Groesse der Reellenzahl.  Ja.  Der relative Fehler.", "start": 1040.0, "end": 1069.0}, {"text": "  Aber dieser relative Fehler hat, den koennen wir nach oben abschaetzen.  Der relative Fehler, den wir machen, wenn wir eine Reellezahl in eine Gleitkommazahl uebertragen.  Und jetzt sagen wir, was ist der groesste, moegliche relative Fehler  bei der Uebertragung von einer Reellenzahl in eine Gleitkommazahl?  Ja.  Die Maschinengenauigkeit.", "start": 1069.0, "end": 1093.0}, {"text": " Und jetzt haben wir hier eine Vektornorm, aber das bedeutet nur,  dass wir hier eine Vektornorm irgendwie eine Diagonale von einem Wuerfel an.  Das heisst, da ist nur ein Faktor davor.  Aber im Grunde ist das von der Groessenordnung hier im ungluecklichsten Fall die Maschinengenauigkeit.  Ja, das heisst, hier darf natuerlich kein Gleichheitszeichen stehen.", "start": 1093.0, "end": 1119.0}, {"text": " Das heisst, hier steht jetzt dieser Bruch, Max,  mit den Vektoren mit Einheitslaenge von A mal Y,  geteilt durch Min,  von allen Vektoren mit Einheitslaenge von A mal Y,  mal die Maschinengenauigkeit.  Bis auf eine Konstante, die von N abhaengt, die aber nicht erheblich ist hier.", "start": 1119.0, "end": 1151.0}, {"text": " Und jetzt ist klar, wann immer wir mit A multiplizieren,  wann immer wir solche Sachen loesen,  muessen wir Fehler erwarten in der Groessenordnung der Maschinengenauigkeit.  Ja, also daran kommt sowieso nicht vorbei.  Das ist immer da, wenn wir mit Gleitkommazalen rechnen.  Und das, was hier steht,  ist das, was die Matrix obendrauf kaputt macht.", "start": 1151.0, "end": 1179.0}, {"text": " Und deswegen nennen wir das,  das hier die Kondition von A.  Also die Kondition einer Matrix ist die Laenge des Einheitsvektors,  die maximal lang wird, geteilt durch die Laenge eines Einheitsvektors,  der maximal kurz wird.  Wir koennen uns das zum Beispiel am Bild wieder so vorstellen.  In der Ebene, was ist die geometrische Figur aller Vektoren mit Einheitslaenge?  Ein Kreis.", "start": 1182.0, "end": 1225.0}, {"text": "  Und was macht diese Abbildung A?  Diese Abbildung A verformt den Kreis irgendwie.  Und danach ist der Kreis eine Ellipse.  Und diese Kondition ist jetzt also die Laenge dieser langen Achse geteilt  durch die Laenge der kurzen Achse.  Was ist die kleinstmoegliche Kondition einer Matrix?  Was ist die kleinste Zahl, die hier rauskommen kann?  Ich wuerde auch sagen 1.", "start": 1225.0, "end": 1273.0}, {"text": "  Im besten Fall geht dieser Kreis ueber in einen anderen Kreis.  Der kann groesser werden oder kleiner werden der Kreis.  Aber wenn ich dann eben schaue, was ist der groesste Abstand zum Ursprung,  was ist der kleinste Abstand zum Ursprung,  dann ist das Verhaeltnis fuer den Kreis, es ist 1.  Und fuer alles andere wird das Verhaeltnis groesser.", "start": 1273.0, "end": 1305.0}, {"text": " Also, so, jetzt kommt eine Frage,  jetzt kommt eine Frage, die ist ein bisschen kniffliger,  die ueberlegen wir uns noch schnell.  Und dann sind wir hier mit auch fertig.  Also wir nennen die Kondition, die schreibt man gerne als so ein Kappa.  Von A dieses Verhaeltnis  von der, von diesen beiden Werten.", "start": 1305.0, "end": 1356.0}, {"text": " Ja, und jedes Multiplizieren mit einer Matrix A  macht die Situation potenziell schlechter, wenn ich mit Glatkommerzahlen rechne.  Und dieses Verhaeltnis sagt mir,  was das Vielfache ist der Maschinenungenauchkeit, mit der ich rechnen muss.  Ich muss sowieso mit der Maschinenungenauchkeit rechnen, bei jeder Rechenoperation.  Aber wenn ich mit Matrizen rechne, dann kommt noch ein Faktor oben drauf.", "start": 1356.0, "end": 1379.0}, {"text": "  Und dieser Faktor ist die Kondition der Matrix.  Ja, und das kann beliebig gross werden.  Und jetzt ist mal eine Frage.  Also, und wenn ich dann nochmal mit einer Matrix multipliziere,  dann wird das eben typischerweise noch schlechter.  Ja, es wird nie besser, wenn ich nochmal mit einer Matrix multipliziere.", "start": 1379.0, "end": 1409.0}, {"text": " Und einen besonderen Fall ueberlegen wir uns mal,  was ist die Kondition von der Inversen von A?  Ja, es ist vielleicht nicht...  Ja, vielleicht verrate ich es, wenn man es nicht sofort sieht,  dass die Kondition der Inversen von A ist genau die gleiche wie die von A.  Ja, also, was ich damit insbesondere sagen moechte, ist Folgendes.", "start": 1409.0, "end": 1453.0}, {"text": "  Wenn ich mit einer Matrix multipliziere, die eine grosse Konditionszahl hat,  dann mache ich die Situation deutlich schlechter.  Und was auf keinen Fall hilft, ist mit der Inversen von A zu multiplizieren.  Dann mache ich die Situation im gleichen Masse nochmal schlechter.", "start": 1453.0, "end": 1478.0}, {"text": " Ja, und das ist aus meiner Sicht ein wunderschoenes Beispiel dafuer,  wo man sieht, dass das implementieren von solchen Algorithmen  und das Papier sich sehr unterscheiden.  Also, wenn ich in meinem Algorithmus irgendwo mit A hoch minus 1 und A multipliziere,  statt einfach mit der Identitaet zu multiplizieren,  das ist nicht das gleiche in Gleitkommazalen.", "start": 1478.0, "end": 1500.0}, {"text": "  Wenn A schlecht konditioniert ist, ist das hier eine Katastrophe  und das ist voellig in Ordnung.  Die Identitaet hat Kondition 1.  Waehrend mit A und mit A Inversen zu multiplizieren beliebig schlecht sein kann.  Und dieser Unterschied, den sehen wir in Gleitkommazalen.  Wuerden wir mit reellen Zahlen rechnen, waere es egal,  das hier ist die Identitaet, alles ist gut.", "start": 1500.0, "end": 1526.0}, {"text": "  Aber wenn wir mit Matrizen rechnen, macht das ein Riesenunterschied.  So, und jetzt moechte ich gerne kurz sagen,  was das, also es hat auch fuer heute, fuer den Rest von heute Konsequenzen,  aber ich moechte kurz sagen, was das fuer Konsequenzen hat,  fuer die letzten paar Wochen.  Einerseits koennen wir uns ueberlegen, Gaussillimination  ist ein schrittweiser Algorithmus.", "start": 1526.0, "end": 1551.0}, {"text": "  Ich multipliziere eine Zeile und alle unten drunter mit irgendwas.  Das kann ich ausdruecken als eine Matrix-Multiplikation.  Das ist im Grunde eine Matrix-Multiplikation.  Und die Pivotisierung, die wir verwenden,  die sagt, dass wir die Kondition, dieser Matrix, die wir waehlen,  um eben eine Spalte zu eliminieren, die machen wir so klein, es geht.", "start": 1551.0, "end": 1582.0}, {"text": " Das heisst, wenn wir bei der Gaussillimination  einen Schritt als das Produkt mit einer Matrix interpretieren,  dann haben wir diese Matrix so gewaehlt, durchpivotisieren,  dass die Kondition moeglichst klein ist.  Das heisst, wir koennen Gaussillimination ausdruecken  als eine Reihe von Produkten mit Matrizen  und jede einzelne Matrix hat in jedem Schritt eine Kondition,  die so klein wie moeglich ist.", "start": 1582.0, "end": 1604.0}, {"text": "  Das heisst nicht, dass das am Ende optimal ist,  aber es ist ein gescheiter greedy Algorithmus.", "start": 1604.0, "end": 1639.0}, {"text": " Und dann das andere, worauf ich kurz hinweisen moechte, ist,  wenn wir ein solches Gleichungssystem loesen wollen  und wir sagen, wir haben keine Loesung,  was haben wir uns ueberlegt letztes Mal, wie man damit umgeht?  Nehmen wir mal an A, A mal X, gleich B hat keine Loesung,  und zwar so richtig keine Loesung, jetzt nicht nur in Gleitkommazalen keine,  sondern es ist weit weg.", "start": 1639.0, "end": 1646.0}, {"text": "  Was hatten wir letztes Mal gemacht, damit wir eine Loesung bekommen?  Ja, dann einen Vorschlag.  Ja, genau, was heisst das?  Was mache ich? Ausgleichsrechnung.  Ich multipliziere mit A transponiert.  Ja, also wenn A, X, gleich B keine Loesung hatte,  dann hatten wir gesagt, okay, kein Problem,  ich kann von links mit A t multiplizieren, und das hat dann eine Loesung.", "start": 1647.0, "end": 1687.0}, {"text": "  Aus dieser Erkenntnis mit der Kondition heraus, wissen wir sofort,  wenn A, X, gleich B eine Loesung hat, was ist dann eine in Gleitkommazalen  denkbar unsinnige Idee, von links mit A t zu multiplizieren?  Wir machen die Kondition unseres Systems schlechter.  Also das ist kein universal Rezept hier.", "start": 1687.0, "end": 1716.0}, {"text": " Wenn A schlechte Kondition hat, dann tun wir uns nicht unbedingt einen Gefallen damit,  wenn wir noch mal mit A t multiplizieren.  Die Kondition wird schlechter.", "start": 1716.0, "end": 1735.0}, {"text": " Und die Frage waere, ob es irgendwelche Methoden gibt,  bei denen wir eben nicht mit A t multiplizieren,  sondern direkt irgendwie versuchen, das Residuum zu minimieren,  ohne dass wir dafuer von links mit A t multiplizieren muessen,  und damit unsere Kondition schlechter machen.  Und wie so oft wuerde ich diese Frage natuerlich nicht stellen,  wenn es solche Methoden nicht geben wuerde.", "start": 1735.0, "end": 1744.0}, {"text": "  Und im Laufe, na sagen wir mal,  bei der naechste Woche kommen wir bei so einer Methode an,  die uns eben auch das Residuum minimiert in so einer Situation,  ohne dass wir dafuer mit A t multiplizieren muessen.  Aber wie so oft gibt es nichts umsonst,  die Methode ist dann eben viel, viel teurer als die Scholeskiz-Erlegung,  die wir hier anwenden konnten.", "start": 1744.0, "end": 1775.0}, {"text": " Okay, das war die Geschichte zur Kondition,  die Geschichte von der Kondition.  Mehr sage ich dazu jetzt nicht.  Aber wir versuchen, ah doch, eine Sache sage ich vielleicht doch noch.  Wir koennen uns mal ueberlegen, welche Matrizen sind denn gut?  Ja, also wir hatten schon gesagt, Identitaet ist gut, Kondition 1, besser wird es nicht.  Ja, welche sind noch gut?  Ja?  Die Nullmatrix.", "start": 1775.0, "end": 1808.0}, {"text": "  Ich weiss nicht so recht.  Also, so ein bisschen schwer was zu sagen,  haben wir Null durch Null, ist die Kondition gar nicht definiert.  Also, finde ich, ist glaube ich auch selten eine praktische Loesung.  Ja?  Orthogonale Matrizen sind gut.", "start": 1808.0, "end": 1835.0}, {"text": " Orthogonale Matrizen, also Matrizen, die nur rotieren oder spiegeln,  die erhalten, also allgemein, was machen Orthogonale Matrizen?  Orthogonale Matrizen erhalten die Norm.  Ja?  Das ist eine wichtige Erkenntnis.  Eine Orthogonale Matrix  erhaelt Laengen und Winkel.  Und da sie Laengen und Winkel erhaelt,  erhaelt sie eben auch die Norm.", "start": 1835.0, "end": 1864.0}, {"text": " Ja? Also,  wenn Q eine orthogonale Matrix ist, das heisst, es gilt Q, TQ ist die Identitaet.  Ja?  Dann haben wir, dass fuer jeden Vektor V  die Laenge nicht veraendert wird, wenn wir mit Q multiplizieren.  Ja?  Das ist das, was Orthogonale Matrizen machen.  Und dann ist ganz klar, fuer eine Orthogonale Matrix ist das Maximum hier eins  und das Minimum hier eins, weil sich die Laenge ja nicht veraendert.", "start": 1864.0, "end": 1887.0}, {"text": "  Wenn ich ein Einheitsvektor reinstecke,  kriege ich ein Vektor mit Einheitslaenge wieder raus,  ist auch logisch bei diesem Bild mit dem Kreis, das ich gemalt habe,  wenn ich ein Kreis rotiere, aendert sich nichts.  Das heisst, das sind auch hier Good Guys.  Ja? Und die Frage ist, welche noch?  Wir koennen die Menge der Guten noch ein bisschen erweitern.", "start": 1887.0, "end": 1911.0}, {"text": " Ja?  Skalare vielfache von Orthogonalen Matrizen.  Skalare vielfache von Orthogonalen Matrizen.  Ja? Also, wir koennen noch mit einem Skalar multiplizieren, da passiert auch nichts.  Dann veraendert sich zwar die Laenge hier, aber da veraendert sie sich genauso wie da.  Und das sind sie dann auch.", "start": 1911.0, "end": 1931.0}, {"text": " Also, alles Skalaren vielfachen von Orthogonalen Matrizen  haben die Eigenschaft, dass sie Kondition eins haben und alles andere ist schlechter.  Ja? Deswegen, wenn wir gerne, wenn wir so was loesen wollen  und uns ist die Kondition wichtig und die numerik,  dann sollten wir versuchen, mit Orthogonalen Matrizen zu arbeiten.  Ja? Weil die, weil die fuer die Gleitkommarzahlen bedeuten, okay.", "start": 1931.0, "end": 1951.0}, {"text": "  Ja? Die Gleitkommarzahlen, wir machen immer einen Fehler,  aber zumindest ist die Matrix so strukturiert, dass wir,  dass wir nicht noch einen groesseren Fehler machen,  weil die Matrix irgendwie so ungluecklich gewaehlt ist.  Okay.  So.  Damit, mit dieser kleinen Vorgeschichte,  kommen wir jetzt zu unserem naechsten praktischen Problem.  Und das sieht wie folgt aus.", "start": 1951.0, "end": 2008.0}, {"text": "  Was wir machen wollen ist, wir wollen uns Bilder anschauen,  auf denen Gesichter sind.  Und wir wollen einerseits die Frage stellen,  wenn da jetzt was anderes kommt.", "start": 2008.0, "end": 2042.0}, {"text": " Ja?  Wollen wir die Frage stellen, ist das ein Gesicht?  Und wenn, wenn nochmal ein Gesichtskont, ja?  Wollen wir fragen, okay, ist das ein Gesicht?  Kennen wir das Gesicht?  Haben wir das Gesicht schon mal gesehen?  Ist das ein Element unserer Daten?  Okay. Das heisst, wir haben hier so Bilder.  Und was wir wieder machen, ist, wir diskretisieren die wieder.", "start": 2042.0, "end": 2049.0}, {"text": "  Klar. Wir legen da wieder ein Raster drueber.  Und wir stellen das Ganze wieder als Grau-Werte dar.  Ja? Das heisst, jedes dieser Bilder hier  ist ein Vektor G0, G1, G2.  Und die Dinger sind dann irgendeine Anfrage G.  Und all diese Gs sind Elemente aus dem R hoch N.  Ja?  Also, wieder das Gleiche.", "start": 2053.0, "end": 2090.0}, {"text": " Immer aufpassen, wir verstehen,  dass als eine Reihe von Grau-Werten,  die koennen wir irgendwie anordnen,  so sagen wir sie in allen Bildern gleich anordnen.  Es ist also keine Matrix. Ja, es ist wieder ein Vektor.  Und wir machen jetzt das Simpelste, was man sich vorstellen kann.", "start": 2090.0, "end": 2126.0}, {"text": " Wir schauen uns an den Unterschied, den Abstand,  wenn wir diese Gs als Punkte verstehen,  von G und irgendeinem Element in unserer,  sagen wir mal Datenbank von Gesichtern.  Ja? Das ist das, was wir machen wollen.", "start": 2126.0, "end": 2141.0}, {"text": " Und klar, was wir dafuer machen mussten,  damit das in irgendeiner Weise was Sinnvolles liefert,  ist, wir muessen dafuer sorgen,  dass die Gesichter in den Bildern alle ungefaehr gleich gross sind,  dass die Augen an den gleichen Stellen sind,  dass die Nasen an den gleichen Stellen sind,  dass der Mund an der gleichen Stelle ist und so weiter.", "start": 2141.0, "end": 2152.0}, {"text": " Und wir nehmen an, dieser Vorverarbeitungsschritt ist,  den hat jemand fuer uns gemacht.  Wir sagen, es gibt eine Methode, die das macht.", "start": 2152.0, "end": 2174.0}, {"text": " Okay, das heisst, die Bilder sind entsprechend aufbereitet,  Augen sind an der gleichen Stelle,  Nasen sind an der gleichen Stelle und so weiter,  Ohren sind an der gleichen Stelle,  sodass dieser euklideische Abstand,  dieser zwei Vektoren ansatzweise was Sinnvolles liefert.", "start": 2174.0, "end": 2183.0}, {"text": " Die Idee ist, der ist klein,  wenn die Gesichter hier sehr aehnlich sind wie die zwei  und der ist riesengross, wenn da ganz was anderes reinkommt.", "start": 2183.0, "end": 2204.0}, {"text": " Und dann kann ich jetzt zum Beispiel die Frage beantworten,  welches Gesicht ist das, wenn ich diese Abstaende  alle anschaue fuer alle i und dann sage, okay,  das ist das Gesicht, das hier ist das Gesicht,  weil hier der euklideische Abstand am kleinsten ist.  Okay, ich hoffe, bis hierhin ist es simpel.  Gut, jetzt machen wir folgende Beobachtung.  Das ist irgendwie maximal ineffizient.", "start": 2204.0, "end": 2234.0}, {"text": "  Wir haben typischerweise nicht so viele Gesichter,  sagen wir mal, wir haben M-Gesichter.  N ist die Anzahl der Pixel, wenn das ein Bild ist,  dann ist das eine Million.  Und M, wie viele Gesichter haben wir, sagen wir mal,  100 oder so, oder vielleicht 1000.", "start": 2234.0, "end": 2270.0}, {"text": " Und was jetzt ungluecklich ist aus meiner Sicht,  ist, dass wenn wir so zwei Gesichter vergleichen  und wir diesen Bild, den wir hier haben,  dass wenn wir so zwei Gesichter vergleichen  und wir diesen Abstand bestimmen, dass das zwei Vektoren sind,  die die Laenge eine Million haben  und ich also diesen eine Million Eintraegevektor  mit dem anderen eine Million Eintraegevektor vergleiche,", "start": 2270.0, "end": 2290.0}, {"text": " obwohl ich weiss, der Raum, der lineare Raum,  der hier aufgespannt wird, kann keine Dimension haben,  groesser als M.  Mehr als M-Dimensional kann der nicht sein,  wenn ich nur M-Bilder habe.  Und M war viel kleiner als M.  Und jetzt kommt es aber noch besser,  jetzt kommt der eigentliche Punkt, das ist sowieso klar.", "start": 2290.0, "end": 2318.0}, {"text": " Der eigentliche Punkt ist jetzt, dass ich vermute,  wahrscheinlich reichen noch viel weniger Dimensionen.  Denn selbst wenn ich hier 100 Gesichter oder so habe,  vielleicht kann ich, die Gesichter sind sich ja alles sehr aehnlich,  die sind ja nicht voellig unterschiedlich.", "start": 2318.0, "end": 2341.0}, {"text": " Die Frage ist, gibt es irgendeine magische Funktion F,  die aus 10 oder 20 Parametern alle moeglichen Gesichter erzeugt?  Ja, ein Parameterlaenge der Nase,  ein Parameterabstand der Augen, ein Parameter,  was weiss ich, ja, Hoehe der Stirn, ein Parameter Haarfarbe und so weiter.  Eventuell gibt es wenige Parameter, die alle Gesichter erzeugen.  Okay.", "start": 2341.0, "end": 2370.0}, {"text": " Und die Frage ist also, die wir uns jetzt stellen,  gibt es eine Funktion F,  die abbildet von dem R hoch O in den R hoch N  und dabei ist O kleiner als M und das war sowieso viel kleiner als N.", "start": 2370.0, "end": 2401.0}, {"text": " Wir fragen uns also, gibt es eine solche Funktion,  die aus wenigen Parametern all diese Bilder erzeugt?  Und logischerweise machen wir das wie immer,  so dass wir sagen, wir nehmen mal an,  wir machen den einfachsten moeglichen Fall  und sagen, die Funktion ist linear.  Das heisst, F und X  sieht so aus, also C00 mal X0 plus C01 mal X1 plus usw.  Und dann gibt es hier noch eine Konstante, B0.", "start": 2401.0, "end": 2444.0}, {"text": "  Und dann hier C10X0 plus C11X1 plus und eine Konstante.  Ja, das heisst, unsere Funktion und das liefert uns ein Vektor.  Ja, und dieser Vektor haette dann jetzt die Hoehe,  ja, also, das sind jetzt also insgesamt O-Zeilen.  Ja, also,  es gibt eine Matrix und ein Vektor und das ist unsere Funktion F.", "start": 2444.0, "end": 2488.0}, {"text": " Ja, und dabei ist C jetzt eine Matrix, die hat O-Zeilen  und die nimmt diese Vektoren aus dem R hoch N, ja, also mal N  und B ist ein Vektor aus dem R hoch O.  Okay, ja, und gut.", "start": 2488.0, "end": 2537.0}, {"text": " Was ich jetzt machen kann, wir nehmen mal an,  die gibt es die Funktion, was ich da machen kann,  ist, ich kann jetzt hier diese Abstaende ausrechnen  als F von X minus F von X, ja, und das ist also CX plus B minus CXI plus B.  Und das ist C mal X minus XI.  Ja, okay.  So, das hilft uns erstmal nicht weiter.  Wir muessen uns ueberlegen, wie kommen wir eigentlich zurueck.", "start": 2537.0, "end": 2626.0}, {"text": "  Ja, wir haben ja gesagt CX plus B ist ungefaehr G,  ja, das ist ja nur ungefaehr so und um jetzt sozusagen  diesen Uebergang korrekt zu machen, koennten wir die normalen Leichung nehmen.  Ja, das heisst wir sagen hier CTCX plus CTP ist CTPG,  ja, oder CTCX ist CTP von G minus B.  Ja, und jetzt.", "start": 2626.0, "end": 2709.0}, {"text": " Sie muessen aufpassen und mitdenken, ja, wenn ich dir Bloedsinn hinschreibe,  dann muessen Sie sofort sagen Moment, also die, die es merken.  Wo ist der Fehler?  Ja, wir wollen ja, dass F mit wenigen Parametern dieses Bild beschreibt.", "start": 2709.0, "end": 2749.0}, {"text": " Ja, das heisst X, X hat wenige Parameter  und F erzeugt ganz viele, ja, das heisst wir haben hier N-Seilen  und wir haben N-Seilen und der Vektor X, der hat nur O-Eintraege.  Ja, okay, jetzt ist auch meine eigene Verwunderung ueber das, was da steht, beendet immerhin, das ist gut.", "start": 2749.0, "end": 2778.0}, {"text": " Was wir jetzt also hier sehen ist, wir koennen diesen Abstand,  bei dem ich gesagt habe, das sind 1000 Eintraegen,  das sind 1000 Eintraege, das ist teuer, wir koennten den ausrechnen, indem wir den Abstand zwischen den beiden,  zwischen den beiden Vektoren X nehmen und von X hatten wir gesagt, X ist aus dem R hoch O und O ist klein.", "start": 2778.0, "end": 2794.0}, {"text": "  Ja, das heisst zuallererst mal koennte man, koennte man der Ansicht sein, das ist nicht schlecht, ja.  Hier muss ich jetzt die Differenz bilden von zwei Vektoren, die relativ klein sind.  Und gut, da muss ich mit C multiplizieren.", "start": 2794.0, "end": 2827.0}, {"text": " Und jetzt ist die Frage, also das war das, was ich hier angefangen habe, jetzt ist die Frage, wo kommt eigentlich das X her?  Woher kriege ich eigentlich das X?  Das X ist die Loesung dieser Gleichung, ja, und jetzt habe ich gesagt, okay, im Allgemeinen ist es nicht genau,  also mache ich eine normalen Gleichung und dann sehe ich hier, so kann ich X ausrechnen, dann schreibe ich noch insgesamt hin,", "start": 2827.0, "end": 2844.0}, {"text": " also das hier ist dann X, so kann ich X ausrechnen.  Ja, gut.  Aber das hier ist teuer, also wann immer ich ein Bild bekomme,  diese Matrix, die kann ich einmal invertieren, aber dieses Matrixprodukt ist teuer, das ist irgendwie nicht so schoen.  Okay, also irgendwie ist das noch nicht so schoen.", "start": 2845.0, "end": 2887.0}, {"text": " Und die Frage ist jetzt, wie retten wir das, wie retten wir diese Idee?  Also nochmal, die Idee ist zu sagen, anstatt mit diesen langen Vektoren G zu rechnen,  rechnen wir mit kurzen Vektoren X, ja, und X koennte ich mir zum Beispiel so aus diesem Vektor G und den Bekannten B holen, ja,  also X ist die Loesung von so einem linearen Gleichungssystem.", "start": 2887.0, "end": 2914.0}, {"text": "  Ja, und jetzt ist die Frage, okay, C, C spannt jetzt irgendeinen linearen Unterraum auf, ja,  und dieser lineare Unterraum, der ist irgendwie besonders gut geeignet, um meine Bildvektoren darzustellen.  Das heisst, C muss enthalten die Basis von diesem linearen Unterraum.  Und die Frage ist jetzt, ich habe damit aber ueberhaupt nicht C festgelegt, ja.", "start": 2914.0, "end": 2945.0}, {"text": "  Also wenn ich sage, okay, der lineare Unterraum, der mich interessiert im R3, das ist der hier, ja,  dann gibt es beliebig viele Matritzen, deren Spalten diesen Unterraum aufspannen.  Ja, also wir haben eine Wahl, wir brauchen irgendwelche zwei lineare unabhaengigen Vektoren aus diesem Unterraum, den das Papier darstellt,  und das ist okay, wir koennen irgendwelche zwei Vektoren nehmen, ja.", "start": 2945.0, "end": 2973.0}, {"text": "  Und die Frage waere jetzt, wie sollten wir C bauen, damit das hier eine vernuenftige Sache wird, ja.  Orthogonal, ja, Orthogonal ist die Idee.  Also von allen Moeglichkeiten C zu definieren, spannt immer den gleichen Unterraum auf, sagen wir, wir haetten C gerne Orthogonal.  Ja, warum, was passiert, wenn C Orthogonal ist?  Erstens mal, wenn C Orthogonal ist, was ist dann CTC, die Identitaet, ja.", "start": 2973.0, "end": 3014.0}, {"text": "  Also wenn C Orthogonal ist, faellt dieser Teil komplett weg, und wir haben, dass X die Rotation von G-B ist, ja.  Also wir haetten gerne, dass C Orthogonal ist.  Und was noch viel huebscher ist, wenn wir jetzt Abstaende ausrechnen, hatten wir gesagt, was wir tun sollten, ist,  wir nehmen den Abstand von dieser kleinen Repraesentation X, und dann multiplizieren wir mit C.", "start": 3015.0, "end": 3041.0}, {"text": "  Ja, wenn C Orthogonal ist, koennen wir uns das schenken.  Ja, wenn wir C Orthogonal waehlen, dann koennen wir statt die Abstaende auf dem Vektor mit einer Million Eintraegen zu messen,  koennen wir die Abstaende direkt messen auf unseren kurzen Vektoren, ja, und wir brauchen sonst weiter nichts zu machen.  Das heisst, unsere Strategie ist, so eine Orthogonalmatrix C zu finden, ja.", "start": 3042.0, "end": 3070.0}, {"text": "  C ist eine rechteckige Matrix, da muessen wir jetzt noch genau klaeren, was wir mit Orthogonal meinen, aber okay.", "start": 3071.0, "end": 3095.0}, {"text": " So eine Matrix zu finden, deren Spalten Orthogonal sind, und dann muessen wir mit dieser Matrix multiplizieren,  wann immer wir so ein Gesichtsbild bekommen, aber die Abstaende koennen wir alle messen auf unseren, ich sag mal, 20 Element-Vektoren,  anstatt sie zu messen auf unseren Vektoren mit laenger eine Million.  Das ist die Idee.", "start": 3095.0, "end": 3119.0}, {"text": "  Man kann es auch so zusammenfassen, dass man sagt, wir vermuten, diese ganzen Vektoren G liegen in einem Unterraum,  ja, und wir versuchen eine Orthogonalbasis fuer den Unterraum zu finden.  Ja, und also die Bedingung, die wir gerne haetten, ist, dass CTC eine Einheitsmatrix ist,  und das koennen wir auch haben, wenn C eine rechteckige Matrix ist.  Okay.  So, jetzt geht es weiter.", "start": 3119.0, "end": 3170.0}, {"text": "  Ich bleib mal hier in der Mitte, ja.  Das Ganze funktioniert natuerlich nur, ja.  Wie bitte?  Ich habe es akustisch nicht gehoert.  Ja, nie.  Also alles, was ich sagte, ist,  es ist schwierig von, also,  wenn ich sage, eine Matrix ist Orthogonal, dann meine ich normalerweise eine quadratische Matrix.", "start": 3184.0, "end": 3233.0}, {"text": " Orthogonale Matrizen sind quadratisch, ein Orthogonal haengt normalerweise, Detaminante ist eins,  ja, Detamin, wir koennen gar nicht von der Detaminante reden, wenn die Matrix rechteckig ist.  Ja, deswegen sage ich jetzt ein bisschen genauer, was ich meine, ich moechte nicht, dass C Orthogonal ist,  weil C, C ist ja eine rechteckige Matrix, ja, die hat ja mehr Zeilen, als sie Spalten hat.", "start": 3233.0, "end": 3257.0}, {"text": "  Ja, aber dieses Produkt, ja, das ist also C, so sieht C aus.  Aber wenn ich C jetzt mit ihrer transponierten Multipliziere, dann bekomme ich so eine kleine Matrix,  ja, und das kann durchaus die Identitaet sein.  Ja, das heisst einfach, dass die Spalten Orthogonal aufeinander stehen, ja, und normiert sind.", "start": 3257.0, "end": 3290.0}, {"text": "  Also dieses Spalten von der Matrix C, die sind paarweise Orthogonal, und jede Einzelne hat Laenge 1, das ist meine Forderung.  Ja, und dann habe ich diese Eigenschaften eben, dass hier die Norm erhalten bleibt, ja, und ich habe auch die Eigenschaft, dass das hier zu Identitaet wird.", "start": 3291.0, "end": 3322.0}, {"text": " So, das Ganze, also das ist alles schoen und gut, die Frage ist, also sozusagen jetzt die grosse Frage ist,  natuerlich, wo kommt die Matrix C her, ja, wie besorgen wir uns die Matrix C?  Ja, wir haben also diese ganzen Gs, und wir suchen die Matrix C, und das ist gar nicht so einfach.  Wir schreiben uns wie immerhin das Residuum.", "start": 3323.0, "end": 3365.0}, {"text": " Ja, das ist also,  das ist das einfach, und jetzt setzen wir das hier ein.  Ja, also hier CTC ist die Identitaet,  aber dann ist XCT mal G minus B, das koennen wir jetzt also hier einsetzen,  C mal CT mal entlammern, wie rum hatte ich es?  Wie rum hatte ich es?  Ja, das ist der Ausdruck.", "start": 3367.0, "end": 3420.0}, {"text": "  Und jetzt koennen wir noch ein bisschen zusammenfassen, wir haben hier C mal CT mal GI, und hier haben wir die Identitaet mal GI,  ja, also haben wir C mal CT minus die Identitaet mal GI, ja, und dann haben wir hier C mal CT mal GI,  haben wir hier C mal CT mal B, und hier haben wir die Identitaet mal B, also haben wir insgesamt das.  Ja, das ist also der Ausdruck fuer das Residuum.", "start": 3420.0, "end": 3461.0}, {"text": "  Okay, und was wir jetzt wollen ist, wie so oft, ja, wir wollen das Residuum klein machen,  wollen das Residuum klein machen, und als Wahlmoeglichkeiten dafuer haben wir B und C.  Ja, also wir versuchen C so zu bestimmen, dass das Residuum klein wird.  Und wir kriegen natuerlich ein Residuum fuer jedes Bild, Gesichtsbild GI.  Ja, das ist eine gute Beobachtung.", "start": 3461.0, "end": 3517.0}, {"text": "  Ja, sehr gut, sehr gut. C mal CT ist nicht die Identitaet, ja, und wir unterhalten uns da gleich drueber.  Also wenn C eine quadratische Matrix waere, dann waere es, und CT ist die Identitaet, dann ist auch C mal CT die Identitaet.", "start": 3517.0, "end": 3553.0}, {"text": " Aber waehrend also CT mal C Identitaet ist, und wir also annehmen, dass C vollen Spalten ran hat,  die andere Matrix hier, C mal CT, das ist ja jetzt so was mal so was, und das gibt uns so was grosses,  ja, also es ist eine N mal N Matrix, und da die aus so einer O mal N Matrix entsteht,  kann die gar nicht Rang groesser O haben, ja, also es kann niemals die Identitaet sein.", "start": 3553.0, "end": 3575.0}, {"text": "  Also es ist definitiv eine Matrix mit Rangdefizit, und also von daher kann es nicht die Identitaet sein.  Die Frage ist ein bisschen, was ist es, ja.  Hier stehen die Produkte der ganzen Zeilen drin, und die muessen nicht notwendigerweise orthogonal sein.  Aber ansonsten das Zusammensammeln ist okay, ja, okay.", "start": 3575.0, "end": 3611.0}, {"text": " Also, was ich sagen wollte ist, wir kriegen jetzt so ein Residuum fuer jedes i,  und was wir minimieren wollen, ist die Summe der Residulen, ja.  Und am einfachsten ist es, wir quadrieren die Residumslaenge, ja, wir quadrieren die Residumslaenge,  weil dann ist das, was wir uns anschauen wollen, einfach Ri transponiert mal Ri, das ist das Quadrat der Laenge, ja.", "start": 3611.0, "end": 3640.0}, {"text": "  So, jetzt, ich versuche es, Ri transponiert mal Ri, ja, und das Argument ist,  dass das die Laenge zum Quadrat ist.  Und was wir eigentlich machen wollen, ist jetzt die Summe ueber diese ganzen Residum zu minimieren.  Ich spare mir jetzt mal das Summenzeichen, weil es sonst noch irgendwie mehr wird.", "start": 3640.0, "end": 3668.0}, {"text": "  Aber wir schreiben es jetzt einfach hin, ja, wir schreiben jetzt einfach das Ding transponiert mal sich selbst.  Es ist also Gi minus B transponiert, Cct minus i transponiert, Cct minus i mal Gi minus B.  Das ist das Residum, ja.  Und was wir jetzt einfach machen, ist, wir rechnen diesen Ausdruck hier in der Mitte aus.", "start": 3668.0, "end": 3717.0}, {"text": " Ja, was ist das?  Das ist einfach Cct, Cct minus Cct minus Cct plus die Identitaet.  Ja, das ist der Ausdruck in der Mitte.  Und was ist das?  Wie vereinfacht haben wir das? Wer sieht es?  Was koennen wir jetzt ausnutzen?  Ja, jetzt haben wir an einer Stelle was stehen, was wir kennen.  Ja.  Ja, jetzt haben wir auf der Stelle was stehen, was wir sehen.", "start": 3717.0, "end": 3762.0}, {"text": " Genau, und wo steht das?  Da haben wir gleich eine Anzahl fuer die Identitaet.  Das ist auch der Ausdruck von Cct, wo wir es dann stehen.  All in Euler, Cct minus Cct.  So sehe ich das auch, ja, genau.  Ja, das hier ist auch wieder Cct, weil wir sagen, dass Ct, Ct die Identitaet ist.  Und dann fallen diese beiden Ausdruecke gleich.", "start": 3762.0, "end": 3792.0}, {"text": "  Und es bleibt nur noch das Ding stehen, das habe ich jetzt nur umgedreht.  Ja, das heisst, das ist das, was wir letztendlich haben.  Okay.  Das ist also der Ausdruck, den wir minimieren wollen.  Und eigentlich steht hier noch ein Summenzeichen davor.  Eigentlich wollen wir jetzt summieren ueber alle GI,  weil jedes einzelne Bild irgendeinen Fehler erzeugt.  So, so weit so gut.", "start": 3792.0, "end": 3834.0}, {"text": "  Jetzt brauche ich noch hoechstens, wie viel hoechstens ein Stuendchen oder so, dann haben wir es auch.  Und wir haben keine Stunde, das ist natuerlich schlecht.  So, wir muessen jetzt vorhin das tun, wir muessen uns kurz etwas ueberlegen, bevor wir hier weitermachen koennen.  Ja.  Wir muessen mehr lernen ueber diese Matrizen.", "start": 3835.0, "end": 3879.0}, {"text": "  Und was wir ausnutzen muessen, um hier vorwaerts zu kommen, ist, wir brauchen Eigenzerlegungen.  Ja, und deswegen brauchen wir ein paar Eigenschaften von Eigenzerlegungen.  Ja, also Eigenzerlegungen.  Eigentlich war das schon da, in der Linie an Algebra.  Aber wir sagen mal, die Matrix M ist symmetrisch.", "start": 3879.0, "end": 3921.0}, {"text": "  Ja, und dann wissen wir, wir koennen M schreiben als das Produkt aus einer Orthogonalmatrix, einer Diagonalmatrix und einer weiteren Orthogonalmatrix.  Also Lambda ist die Matrix der Eigenwerte.  So weit, also da moechte ich jetzt nicht irgendwie naeher nochmal herleiten oder erklaeren und so, das ist viel zu muehsam.", "start": 3921.0, "end": 3949.0}, {"text": "  Entweder sagen Sie, ja, ja, stimmt, so war das aus der Linie an Algebra, oder Sie sagen jetzt, okay, nehme ich hin.  Das war vielleicht so.  Also okay. Wichtiger fuer uns ist jetzt die Frage, was gilt eigentlich fuer diese Matrizen, mit denen wir es die ganze Zeit zu tun haben, fuer diese Produkte C, T, C oder A, T, A, die wir schon hatten, ja, fuer diese Dinger.", "start": 3950.0, "end": 3979.0}, {"text": "  Und wir hatten ja gesagt, die sind immer positiv semi-definit.  Ja, also wir hatten gesagt, es gilt X, T, A, T, A, X ist immer groesser, gleich null.  Ja, und wenn wir jetzt A, T, A so diagonalisieren, dann steht da mal Q, T, mal so eine Matrix-Lamda, mal Q, mal X ist groesser, gleich null.", "start": 3979.0, "end": 4030.0}, {"text": "  Ja, und QX ist irgendeine Orthogonaltransformation, ich nenne QX mal Y, dann ist es das Gleiche wie Y, T, Lambda, Y und das ist groesser, gleich null, immer noch, ja.  Und wir sehen, diese ganzen Lambda, I sind groesser, gleich null.  Ja, das heisst, diese Matrizen, A, T, A, diese Dinger haben immer nicht negative Eigenwerte.", "start": 4030.0, "end": 4055.0}, {"text": "  Und das ist eben auch genau dieses positiv semi-definit im Grunde sprechen wir bei positiv semi-definit ueber eine Eigenschaft der Eigenwerte.  So, jetzt beobachten wir noch was.  Nehmen wir also an, wir haben so ein Eigenwert Lambda gefunden von A, T, A, ja, also wir haben den Eigenwert von A, T, A, das heisst X ist Lambda X.  Ja, und jetzt multiplizieren wir das von links mit A.", "start": 4055.0, "end": 4114.0}, {"text": "  Und was wir dann sehen, ist es, dass Lambda auch ein Eigenwert ist von A, A, T.  Ja, also wenn wir ein Eigenwert von A, T, A gefunden haben, dann ist das immer auch ein Eigenwert von A, A, T.  Was heisst das? A, A, T und A, T, A haben die gleichen Eigenwerte.", "start": 4115.0, "end": 4159.0}, {"text": "  Wie kann das sein? Wir hatten ja gesagt, dass eine hier ist eine sehr kleine Matrix im Fall von C und das andere ist eine recht grosse Matrix.  Das heisst, wir muessen die Aussage etwas genauer fassen und sagen, die haben die gleichen von 0 verschiedenen Eigenwerte.  Oder anders, dieses grosse Ding hat alle Eigenwerte, das das hat und noch eine Menge Null Eigenwerte.", "start": 4159.0, "end": 4183.0}, {"text": "  Das sind die Eigenwerte von diesen Dingen an.  So, was heisst das jetzt konkret? CTC ist die Identitaet, was hat es fuer Eigenwerte? 1.  Alle Eigenwerte von CTC sind 1.  Was heisst das fuer die Eigenwerte von C mal C transponiert?  Ja, das hat nur Eigenwerte 1 und Null.", "start": 4183.0, "end": 4224.0}, {"text": "  Also die hat, wenn wir sagen, die Matrix hat die Dimension N mal O, dann hat diese Matrix O Eigenwerte 1 und N minus O Eigenwerte Null.  Okay.  Und jetzt vielleicht als Letztes noch.  Wenn wir jetzt angucken, hier i minus A mal A transponiert.  Und wir zerlegen den zweiten Teil.  Das war's mal so.  Und dann schreiben wir einfach hier vor die Identitaet mal frech Q i Q transponiert.", "start": 4225.0, "end": 4281.0}, {"text": "  Ja, und dann haben wir zum Schluss.  Das hier stehen.  Ja, das heisst, was sind die Eigenwerte von dem Dingen?  Die bekommen wir, indem wir die Eigenwerte von dieser Matrix A mal A t nehmen und sie von der Identitaet abziehen.  Okay.  Und das ist nicht ganz unwichtig.", "start": 4285.0, "end": 4323.0}, {"text": " So dieser ganze Exkurs war jetzt dafuer da, dass wir uns ueberlegen, was hat dieses Ding fuer Eigenwerte, was hier steht?  Ja, was hat es naemlich fuer Eigenwerte?  Das Ding hatten wir gesagt, hat die Eigenwerte 1 und Null.  Die hat nur die Eigenwerte 1.  Also hat das gesamte Ding auch nur die Eigenwerte 1 und Null.  Und mit hin ist es eine PSD Matrix.", "start": 4327.0, "end": 4348.0}, {"text": "  Also wir haben uns ueberlegt, das hier ist eine PSD Matrix.  Und hat nur die Eigenwerte 1 und Null.  So.  Ein bisschen was schaffen wir noch heute.  So.  So.  Die nennen wir mal M.  Die Matrix.  Und jetzt gucken wir uns wirklich das Problem an, diese Summe zu minimieren.  Also was haben wir?  Wir haben G i minus B mal M mal G i minus B.  Ja, das ist das, was wir minimieren wollen.", "start": 4348.0, "end": 4427.0}, {"text": "  Und wir haben zwei Dinge, die wir minimieren koennen.  Wir koennen minimieren die Eintraege von M.  M ist C mal CT, wobei C diese orthogonalen Spalten hat.  Und wir koennen minimieren, indem wir B variieren.  Ja, und ok.  Schreiben wir uns mal hin.  G i transponiert M mal G i minus B transponiert mal M mal G i minus G i  transponiert M mal B plus B transponiert M mal B.  Einfach ausmultipliziert.", "start": 4427.0, "end": 4486.0}, {"text": "  Und hier vor steht die Summe.  Und jetzt wird die Summe relevant, weil wir jetzt ueberlegen koennen,  welche Ausdruecke brauchen eigentlich die Summe  und welche Ausdruecke brauchen die Summe nicht.  Also der hier vorne, der haengt von i ab.  Also G i transponiert mal M mal G i.  Dann haben wir hier minus B transponiert M mal die Summe von den G i's.", "start": 4486.0, "end": 4531.0}, {"text": "  Ja, minus die Summe von den G i's transponiert mal M mal B plus B T M B.  Hier muss noch ein Faktor davor.  Den hatten wir o- genannt.  So, jetzt koennten wir noch, wir koennten hier noch ein bisschen spielen  und dann weiterrechnen, ich schlag folgenden Trick vor.  Ich schlag folgenden Trick vor, wir machen mal eine Annahme.", "start": 4532.0, "end": 4592.0}, {"text": " Wir sagen mal, wir haben die G i's so gewaehlt,  dass sie sogenannt Mittelwert frei sind.  Mittelwert frei.  Okay, wenn wir die so gewaehlt haben,  dann sehen wir, dass dieser Term Null ist  und dieser Term Null.  Ja, und wir bekommen Summe i G i T M G i plus unseren Faktor o,  das ist ungluecklich, B T M B.", "start": 4593.0, "end": 4648.0}, {"text": " Was ist also fuer diesen Fall, dass wir die G i's mittelwertfrei gewaehlt haben,  die beste Wahl fuer B?  Ja, also nochmal zur Erinnerung, das ist die Summe der Residuen, der q Residuen  und wir versuchen die Summe der q Residuen so klein wie moeglich zu machen.", "start": 4648.0, "end": 4670.0}, {"text": " Und wenn wir jetzt sagen, welches B, mal unabhaengig von dem M,  fuer welches B, machen wir diese Summe der q Residuen so klein wie moeglich,  jetzt haben wir eine Annahme gemacht, wir haben gesagt, okay,  wir haben die G i's so gewaehlt, dass die mittelwertfrei sind,  aber unter dieser Annahme, welches ist jetzt offensichtlich das sinnvollste B?", "start": 4670.0, "end": 4688.0}, {"text": " Ja, mit welchem, durch die Wahl von welchem B machen wir diesen Term so klein, es geht?  Ja, also systematisch jetzt durch die Veraenderung von B koennen wir hier nichts veraendern.  Das heisst, wir koennen nur hier was veraendern.", "start": 4688.0, "end": 4711.0}, {"text": " Und jetzt ist die Frage, was ist der kleinste Wert,  die wir hier nicht veraendern koennen?  Ja, das ist die Frage, was ist der kleinste Wert,  der wir hier nicht veraendern koennen?  Ja, das heisst, wir koennen nur hier was veraendern.  Und jetzt ist die Frage, was ist der kleinste Wert, den das hier annehmen kann?  Ja, Null, warum kann es nicht kleiner als Null werden?  So einfach ist es nicht.", "start": 4711.0, "end": 4743.0}, {"text": "  Es liegt an M, ja?  M ist positiv semi-definit.  Also, als Horistik haetten Sie sich fragen koennen, warum interessiert uns all das eigentlich?  All das interessiert uns, weil wir gesagt haben, hey, M ist positiv semi-definit,  und dann weiss ich, dieser Ausdruck ist groesser, gleich Null.  Und jetzt sozusagen kommen Sie und sagen, okay, das Beste, was mir passieren kann, ist, dass der Null wird.", "start": 4744.0, "end": 4774.0}, {"text": "  Also ist die beste Wahl fuer B offensichtlich Null.  Die beste Wahl fuer B ist offensichtlich Null.  Das war ja aber nur unter der Annahme, dass wir diese Dinger mittelwertbefreit bekommen.  Und die Frage ist jetzt, was machen wir, wenn das nicht so ist?  Wir sorgen dafuer, dass die, also was kommt denn hier vor?  Wir rechnen ja mit G i minus B.", "start": 4774.0, "end": 4811.0}, {"text": "  Jetzt hatten wir gesagt, wenn die G i mittelwertbefreit sind, dann ist B gleich Null die beste Wahl.  Oder allgemeiner, wenn wir uns diese Vektoren angucken hier, haetten wir gerne,  ja im Allgemeinen haben wir also als Beobachtung, wir haetten gerne, dass die Summe G i minus B Null ist.  Das ist unsere Beobachtung.", "start": 4811.0, "end": 4838.0}, {"text": " Wenn wir uns jetzt dieses Ding angucken, M ist eine positiv semi-definite Matrix,  kleiner als Null wird es nicht.  Wir sollten B so waehlen, dass G i minus B in der Summe Null ist.  Das ist die Beobachtung.  Und was heisst das?  Das heisst, wir sollten B waehlen als den Mittelwert der G i.  So, das ist Erkenntnis eins fuer heute.", "start": 4838.0, "end": 4878.0}, {"text": "  Und jetzt, in den verbleibenden minus vier Minuten, kommt der komplizierte Teil,  naemlich das Ausrechnen von B war ziemlich einfach.  Der komplizierte Teil ist offensichtlich diese Matrix M bzw. diese Matrix C zu finden,  von der wir gerne haetten, dass sie normierte, orthogonales Spalten hat  und dann eben auch diesen Ausdruck minimiert.", "start": 4878.0, "end": 4902.0}, {"text": "  Und klar, also da das komplizierter ist, als das B auszurechnen,  machen wir das dann naechste Woche mit frischem Kopf.  Erst mal, vielen Dank.", "start": 4902.0, "end": 4908.0}]}]