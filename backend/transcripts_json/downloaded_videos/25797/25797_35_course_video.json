[{"lecture": "25797_35_course_video", "Timestamps": [{"text": "  So, jetzt koennen wir zum Juttenverfahren zur Optimierung.  Wir haben das Juttenverfahren schon fuer die Nullstaermischung einer Funktion benutzt.  Jetzt wollen wir das Juttenverfahren benutzen, um das Optimum vor allem vor einer Medi-Nullstaermischung zu finden.  Dafuer gucken wir uns erstmal unsere generelles Juttenverfahren zur Nullstaermischung an, fuer eine Medi-Nullstaermischung.", "start": 0.0, "end": 30.0}, {"text": "  Wir haben einen Startwert x0, dann unser Naechsterwert, unser Wert xi plus 1,  einfach unser Iterwert minus der Funktionswert an dieser Stelle durch die Ableitung an dieser Stelle.  Das hier war gedacht als eine Teller-Aproximation ersten Grad.  Wir haben so ein guter Schritt.  Was wir wissen, ist, dass wir optimieren wollen, dann wollen wir die Nullstelle von der Ableitung finden.", "start": 30.0, "end": 57.0}, {"text": "  Wir koennen also einfach f, einfach f' nutzen, also statt fuer die Funktion, wir tragen die Ableitung.  Damit kriegen wir dann als Vorschrift xi plus 1 xi minus f' von xi durch f'2 von xi.  Wir haben einfach nur eingesetzt fuer f' und damit koennen wir dann die Nullstelle von der Ableitung.  Dann koennen wir uns auch anschreiben wie wir hier sind in hoch minus 1, warum sehen wir gleich.", "start": 57.0, "end": 89.0}, {"text": "  Wir haben jetzt eigentlich eine Funktion hoeher in Grades betrachten, von mehreren Variablen, also aus dem R auch n.  Was wir jetzt jetzt tun koennen, ist, wir setzen fuer die Ableitung den Gradienten ein,  und fuer die zweite Ableitung die erste Matrix, damit kriegen wir unser Juttenverfahren zur Optimierung wie folgt.", "start": 89.0, "end": 109.0}, {"text": "  Wir haben jetzt xi plus 1, ist xi minus die inversende Hessematrix mal den Gradienten.  Was man jetzt hier bemerken kann, ist, dass das eigentlich so aussieht wie der generell Gradientenabstieg,  wenn wir uns an den erinnern, das war einfach xi plus 1, xi minus alpha mal den Gradient.  Also was wir jetzt hier getan haben, ist, wir haben fuer alpha die diverse Hessema ist gebildet.", "start": 109.0, "end": 134.0}, {"text": "  Das ist einfach Gradientenabstieg, aber wir haben alpha als diverse Hessema ist gebildet.  Und wie ist das eine gute Idee, kann man sich fragen.  Wir wissen irgendwie die Hessema-Trix, wir haben hier Informationen ueber die Kruemung der Funktion,  weil es jetzt die zweite Ableitung ist.", "start": 134.0, "end": 153.0}, {"text": " Man koennte sich irgendwie vorstellen, dass wenn wir in der Hessema-Trix die Kruemung haben,  dann haben wir fuer die inverse Hessema-Trix die gegensaetzte Kruemung, um die zu zahlen.  Die Kruemung zu entzaehlen.  Hier ist die Idee, wenn wir unsere Funktion haben, das sind diese Konturleinen jetzt.", "start": 153.0, "end": 170.0}, {"text": " Und wenn wir das Optimum finden, dann gehen wir hier so lang, mit dem Gradientenabstieg,  dann springen wir hier so her.  Und dann gehen wir irgendwann in die Minium.  Ja, und wenn wir jetzt die hessema-Trix anwenden, das ist die inverse Hessema-Trix,  dann kriegen wir sowas.  Idealerweise kurzansche Kreise quasi.", "start": 170.0, "end": 202.0}, {"text": " Und wir gehen uns dann gerade aus Optimum, also direkt gerade anstatt zu zahlen,  so in diesem Kruempenraum, so hin und her zu springen.  Ja, das ist die Idee.  Das koennen wir auch nochmal an diesen zwei Bildern hier sehen.  Hier ist die, den die Konturleins vor der Entzaehlung.  Und hier ist sozusagen nachanwenden der versen-theta-Matrix.", "start": 202.0, "end": 222.0}, {"text": "  Dann werden wir jetzt hier so dann gerade auf das Minium zugehen.  Und deswegen ist es eine gute Wahl fuer den Parameter Alpha.  Das ist unser eigentliche Schritt.  Wir haben jetzt hier x1 plus 1 ist gleich xi minus diesen Wert.  Das ist eigentlich unser Schritt.  Das ist, wenn wir jetzt hier delta xi ist,  aber die inverse Hessema-Trix war im Gradient.", "start": 222.0, "end": 247.0}, {"text": "  Und das ist ja auch gleich hier wieder unser xi plus 1 minus xi.  Ist ja genau unser Schritt.  Und fuer den brauchen wir jetzt die inverse Hessema-Trix.  Und wie wir schon anderen Themen gesehen haben,  wir haben ein LGS angesprochen und auch ein SVD.  Das ist halt die Berechnung von den Versen sehr aufwendig ist  und auch numerisch instabil.", "start": 247.0, "end": 268.0}, {"text": " Und das ist jetzt endlich, was man dann macht,  ist man loest quasi ein gleichen System,  wenn man schon das wieder einen konstanten Vektor haben,  weil auf der einen Seite noch eine Matrix,  hier halt die EDT-Matrix.  Und dadurch wird es aufwendiger und hat zwar b, numerisch instabiler.", "start": 268.0, "end": 285.0}, {"text": " Fuer das Testen wollen wir dann etwas anders machen,  und zwar, wenn wir jetzt diesen Schritt gegeben als delta xi,  natuerlich wieder ein Parametervektor,  das wir jetzt zu testen machen,  ist fuer Multiplizieren beide Seiten mit einer Hessema-Trix  und kriegen dann diese Vorschrift.  Ja, das ist einfach die Hessema-Trix,  mal unserem delta xi ist gleich der Gradient.", "start": 285.0, "end": 302.0}, {"text": "  Und das loesen jetzt wieder von unserem Schritt,  weil jetzt unsere Unbekannten sind,  und dann kriegen wir auch den Schritt fuer das Verfahren.  Ja, das ist jetzt asymptotisch zwar gleich,  also ein bisschen beides so von Nhoch 3 Verfahren.  Aber trotzdem ist hier unser LGS konstant schneller,  und wir loesen nach einem Vektor schneller als einer Matrix,  ja, und wir sind auch numerisch stabiler.", "start": 302.0, "end": 327.0}, {"text": "  Wir muessen halt trotzdem erst die Hessema-Trix berechnen,  da werden wir immer noch in der Vorschrift.  Und das ist fuer komplexe Funktionen  einfach gar nicht erst wirklich moeglich oder halt sehr, sehr aufwendig.", "start": 327.0, "end": 345.0}, {"text": " Und deswegen gibt es ja noch andere Verfahren,  die dann, statt der zaertlichen Hessema-Trix,  ein Apoxemationen-Hessema-Trix berechnen,  ja, und es werden dann auch quasi Jueten-Verfahren genannt.  Ja, aber die werden wir jetzt nicht weiter betrachten,  ja, weil es ist ja wichtig zu wissen,  dass diese Hessema-Trix halt sehr aufwendig zu berechnen ist.", "start": 345.0, "end": 355.0}, {"text": " Das kann ich dir vorstellen,  um also irgendwie N\u00b2, Parzillabwertung berechnen,  ja, und dann ist es irgendwie,  es gibt ja eine analytische Sache,  was immer ein bisschen schwer ist, numerisch,  und dann wird es sehr aufwendig.  Ja, jetzt noch ein Beispiel dazu.  Wir haben wieder unsere Funktion f und y ist gleich.  x\u00b2, also unsere Parabel in Raum sozusagen.", "start": 355.0, "end": 374.0}, {"text": "  Ja, wir kennen schon aus vorigen Videos den Gradient,  und die Hessema-Trix davon,  also unser Gradient ist einfach 2x\u00b2 y,  ja, unsere Hessema-Trix ist 2,0,0,2.", "start": 374.0, "end": 394.0}, {"text": " Natuerlich jetzt noch brauchen wir die inverse Hessema-Trix,  weil die ist ja unsere Vorschlaefte drin,  als muessen wir diese Masse kompetieren,  da das in die Originalmassigkeit ist natuerlich nicht sehr wild,  ja, wir muessen einfach nur die einzelnen Mente invertieren.  Ja, also kriegen wir dann jetzt,  als Matrix 1,5,0,1,5 raus.  Ja, das wird die inverse Hessema-Trix sein.", "start": 394.0, "end": 407.0}, {"text": "  Und jetzt brauchen wir noch einen Startpunkt,  und als Startpunkt werden wir jetzt hier x0 ist gleich 1,2,  und wollen jetzt anhand von diesem einen Schritt  des Nuetenverfahrens ausfuehren, also x1 berechnen.", "start": 407.0, "end": 433.0}, {"text": " Ja, hier ist nochmal die Vorschrift des Nuetenverfahrens zur Optimierung,  ja, da ist jetzt jetzt ein,  mit unserer x0 haben wir den Punkt 1,2,  und dann brauchen wir die Hessema-Trix,  also die inverse Hessema-Trix von dem Punkt 1,2,  und eine Gradient von Punkt 1,2, ja,  hier haben wir unsere inverse Hessema-Trix,  und jetzt haben wir hier als Gradient von 1,2,2,4,", "start": 433.0, "end": 447.0}, {"text": " ja, wir hatten ja als Gradient 2x, 2y,  also muessen wir einfach elementweise  diesen Vektor mit 2 mitpezieren, ja,  jetzt muessen wir das noch auspezieren,  also kriegen wir hier natuerlich 1,5,  dann wieder von diesem 2, 1,5 von 4,  also kriegen wir ja auch 1,2 raus,  also kriegen wir als x1 gleich 0,0,  ja, also x1 ist gleich 0,0,  und wir wissen auch, ja, das ist direkt unser,", "start": 447.0, "end": 472.0}, {"text": " der war das Optimum, das haben wir vorher schon mal berechnet,  das heisst, wir sind hier auch schon fertig.", "start": 472.0, "end": 475.0}]}]