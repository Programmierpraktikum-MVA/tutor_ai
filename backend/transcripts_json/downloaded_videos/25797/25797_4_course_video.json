[{"lecture": "25797_4_course_video", "Timestamps": [{"text": "  Zu guter Letzt wollen wir noch ueber die Laufzeit und die numerische Stabilitaet  einige Algorithmen der linearen Algebra diskutieren.  Fangen wir an mit der Matrix-Multiplikation.  Das ist das, was ihr hoffentlich im ersten Praxisaufgabenblatt implementiert habt.", "start": 0.0, "end": 25.0}, {"text": " Zwei Matrizen lassen sich multiplizieren,  falls die Anzahl der Spalten der ersten Matrix gleich der Anzahl der Zeilen der zweiten Matrix ist.  Das Produkt, also die neue Matrix, uebernimmt dann die Anzahl der Zeilen der ersten Matrix  und die Anzahl der Spalten der zweiten.  Das heisst, bei einer n-Croids-M-Matrix und einer m-Croids-P-Matrix hat das Produkt dann die Dimension n-Croids-P.", "start": 26.0, "end": 45.0}, {"text": "  Also haben wir n mal p Elemente.  Fuer jedes dieser Elemente koennen wir dann mit Hilfe dieser Formel den Wert berechnen.  Das hier ist nichts anderes als das Skalarprodukt der i-Zeile der ersten Matrix, also von a,  mit der j-Spalte der zweiten Matrix.  Und da wir hier ueber m summieren, hat die Berechnung eines Elementes die Laufzeit o von m  und die Gesamtlaufzeit ist dann o von n mal n mal p.", "start": 45.0, "end": 87.0}, {"text": "  Sind unsere Matrizen quadratisch, dann betraegt die Laufzeit o von n hoch 3.  Das ist aber eine sehr naive Implementierung der Matrix-Multiplikation.  Bei quadratischen Matrizen koennen wir aber ein paar algorithmische Tricks anwenden  und bessere Algorithmen, zum Beispiel der Algorithmus von Strassen,  der kommt dann auf eine Laufzeit von o von n hoch 2,807.", "start": 89.0, "end": 110.0}, {"text": "  Das hoert sich erst mal nach einer sehr, sehr kleinen Verbesserung an,  aber bei sehr grossen Datenmengen kann das schon einen sehr grossen Unterschied machen.  Als naechstes schauen wir uns das Transponieren an.  Beim Transponieren tun wir nichts anderes als die Matrix an der Diagonalen zu spiegeln,  das heisst wir vertauschen das Element mit dem gegenueber, so wie hier.", "start": 110.0, "end": 134.0}, {"text": "  Und dadurch werden Zeilen zu spalten und Spalten zu zeilen  und die beiden Dimensionen der Matrix vertauschen sich, also eine n Kreuz-M-Matrix  wird so eine m Kreuz-M-Matrix.  Und wuerden wir das so naiv implementieren mit dem Vertauschen,  dann haetten wir eine Laufzeit von o von n mal m, bzw. o von n\u00b2 fuer quadratische Matrizen.", "start": 134.0, "end": 156.0}, {"text": "  In der Praxis hingegen sind Matrizen ein bisschen intelligenter implementiert,  und zwar, das hat dann zur Folge, dass wir tatsaechlich meistens in o von 1 transponieren koennen.  Beim Transponieren tun wir nichts anderes als die Achsen der Matrix zu permutieren,  und das hat dann in den meisten Faellen die Laufzeit o von 1.", "start": 156.0, "end": 180.0}, {"text": " NumPy bietet uns fuer Matrizen das Attribut gross t,  dadurch kriegen wir dann die Transponierter der Matrix,  und haben wir ein NumPy-Array, das 3 oder mehr Dimension hat,  dann koennen wir mit der transpose Function auch angeben, wie wir genau die Achsen vertauschen moechten.  Als naechstes schauen wir uns die Gleichungssysteme an.", "start": 180.0, "end": 202.0}, {"text": "  Wir wissen bereits, wir muessen zuerst die Gauss-Elimination anwenden,  diese hat eine Laufzeit von o von n hoch 3, und danach Rueckwaerts-Einsaetzen anwenden,  und das hat dann eine quadratische Laufzeit.  Die Funktion np.linear.solfe loest dann das Ganze, sie berechnet dann den Loesungsvektor,  und das erste Argument ist dann die Matrix, und das zweite Argument ist dann der Vektor.", "start": 202.0, "end": 229.0}, {"text": "  Jetzt gibt es auch ein paar Spezialfaelle von Matrizen, wo sich die Gauss-Elimination nicht lohnt.  Die erste Spezialfaelle waere eine Diagonalmatrix.  Wir sehen, dass alle Elemente ausserhalb der Diagonalen 0 sind,  d.h.", "start": 229.0, "end": 253.0}, {"text": "diese Gleichung in allen Zeilen ist dann nichts anderes als a k, k mal xk ist gleich bk,  und das koennen wir dann nach xk jeweils umstellen, und dann ist xk gleich bk geteilt durch akk,  und hier sieht man nochmal die Formeln, aber dies man mit i als Index,  und das hat dann die lineare Laufzeit, da wir ja n Elemente auf der Diagonalen haben,  und da wir insgesamt n die Vision durchfuehren.", "start": 253.0, "end": 272.0}, {"text": "  Mit NAPI kann man das so implementieren, dass man die Funktion np.diagnutzt,  diese extrahiert dann die Diagonale einer Matrix,  der Vektor b muss dann natuerlich auch die gleichen Dimensionen haben,  und dann mit Hilfe von NAPI kann man das mit dem Divisionsoperator nehmen,  und der macht dann nichts anderes als elementweise die Elemente aus den beiden Arrays zu dividieren.", "start": 272.0, "end": 293.0}, {"text": "  Der naechste Spezialfall ist die obere 3x-Matrix.  Der Gauss-Algorithmus formt ja die Matrix in eine obere 3x-Matrix um,  aber wenn sie schon eine obere 3x-Matrix ist, dann ergibt es keinen Sinn,  dass wir diese Umformung machen, das kostet uns nur unnoetig Zeit,  und hier koennen wir direkt rueckwaerts einsetzen benutzen.", "start": 293.0, "end": 318.0}, {"text": " Das hat dann wie gesagt quadratische Laufzeit,  und NAPI bietet dafuer keine Funktionen, dass man LGS in dieser Form direkt loesen kann,  aber die grosse Schwester von NAPI, naemlich PsiPi, bietet dann die Softrangle Funktion.  Und es gibt nicht nur obere 3x-Matrizen, sondern untere 3x-Matrizen.", "start": 318.0, "end": 336.0}, {"text": " Hier ist es anders als bei der obere 3x-Matrix,  dass wir nicht in den letzten Zeilen immer mehr Nullen haben,  sondern in der ersten Zeile haben wir dann am meisten Nullen.  Alle Elemente bis auf das allererste hier sind Null,  und in der zweiten Zeile haben wir nur Nullen bis auf zwei Elemente und so weiter.", "start": 336.0, "end": 355.0}, {"text": " Und hier bestimmen wir dann die Loesungsfektor aehnlich,  nur dass wir hier bei x1 anfangen und bei xn aufhoeren.  Das heisst, wir bestimmen es jetzt von oben nach unten.  Und deshalb nennt man das nicht mehr rueckwaerts einsetzen, sondern vorwaerts einsetzen.", "start": 355.0, "end": 371.0}, {"text": " Die Laufzeit ist natuerlich die gleiche, die ist quadratisch,  und PsiPi nutzt da die gleiche Funktion,  und wir muessen hier lower gleich true als Parameter dazugeben.  Als naechstes kommt die Inverse.  Die Inverse ist nur fuer quadratische Matrizen definiert,  und die Inverse einer Matrix muss nicht unbedingt existieren,  nur wenn die Matrix invertierbar oder regulaer ist.", "start": 373.0, "end": 391.0}, {"text": "  Und die Inverse der Matrix A nennen wir dann A hoch minus 1,  und falls sie existiert, dann gilt, dass die Inverse mal die Matrix,  das Gleiche ist wie die Matrix mal der Inverse,  und das muss gleich die Einheitsmatrix sein.", "start": 391.0, "end": 421.0}, {"text": " Und ein Nutzen der Inverse,  waere zum Beispiel, dass wir a ges loesen koennen,  denn wenn wir die Inverse von a kennen,  und wir wollen das a ges A mal x gleich b loesen,  dann koennen wir auf beiden Seiten die Inverse von links multiplizieren,  sodass wir dann haben, dass der Loesungsverletzung x nichts anderes ist  als die Inverse von a mal b.", "start": 421.0, "end": 438.0}, {"text": " Das ganze, also das Berechnende der Inversen,  hat dann die Laufzeit o von n hoch 3, also so wie der Gauss-Ergrythmus.  Das Problem ist, die Inverse ist numerisch sehr instabil.  Das heisst, falls ihr vorhabt, einmal die Inverse zu berechnen  und damit mehrere lineare Gleichungssysteme durch Motivikation zu loesen,  tut das nicht.", "start": 438.0, "end": 457.0}, {"text": " Warum ist das numerisch instabil?  Dazu kann man sich einmal anschauen,  wie die Invertierfunktion von NumPy funktioniert.  Die ruft intern nichts anderes als die Sow-Funktion auf,  und zwar mit dem zweiten Parameter, der diesmal kein Vector ist,  sondern die Einheitsmatrix.  Das heisst, wir loesen kein AGS-Matrix mal Loesungsvektor gleich Vector,  sondern wir loesen Matrix mal Matrix gleich Matrix.", "start": 457.0, "end": 486.0}, {"text": "  Das heisst, wir haben ein AGS mit ein erweitert kurvizierter Kurs,  wo wir einmal unsere Matrix a haben.  Da haben wir hier unsere Einheitsmatrix,  und dann loesen wir das so, dass dann links die Einheitsmatrix steht  und rechts die Inversen.  Das Problem, das sich jetzt hier raus ergibt, ist,  dass wir nicht gegen eine Seite loesen,  sondern gegen N-rechte Seiten.", "start": 486.0, "end": 507.0}, {"text": "  Wir loesen gegen alle Einheitsvektoren.  Das hat dann die Folge, dass das numerisch sehr instabil ist.  Falls ihr jemals auf die Idee kommt,  dass ihr einmal die Inverse berechnet  und daraus ganz viele AGS loesen koennt,  wir werden im Kurs viel bessere Methoden als die Inverse kennenlernen,  die numerisch viel stabiler sind.", "start": 507.0, "end": 532.0}, {"text": " Deshalb wuerde ich euch empfehlen, die Inverse niemals zu benutzen,  ausser wenn die Matrix orthogonal ist.  Denn dann wissen wir, dass die Inverse nichts anderes ist  als die Transponierte der Matrix.  Das heisst, wir koennen die Matrix in O von 1 invertieren,  und das Ganze ist sogar numerisch sehr stabil,  denn wir vertauschen nur ein paar AXen.", "start": 532.0, "end": 550.0}, {"text": " Das heisst, falls wir es hinkriegen,  dass unsere Matrix a orthogonal ist,  dann koennen wir damit sehr, sehr einfach  die Niere-Gleichungssysteme loesen.  Kommen wir dann nun zu einem Fazit.  Leider ist das so, dass die meisten AGS in der Praxis nicht loestbar sind.  Einer dieser Gruende ist zum Beispiel die Diskretisierung.", "start": 550.0, "end": 571.0}, {"text": " Wir haben beim Computertomographen uns ein Gitter ueber unser Gewebe gelegt,  und wir haben angenommen,  dass innerhalb dieser Gitter-Elemente die dichten Konstanz sind.  Das stimmt natuerlich nicht, und das fuehrt zu Fehlern.  Eine weitere Fehlerquelle sind unsere Messinstrumente.  Diese koennen auch nicht beliebig genau messen,  und auch da kommt es immer zu Messfehlern bzw. Messungenauchigkeiten.", "start": 571.0, "end": 591.0}, {"text": "  Der Dritte Grund sind natuerlich Gleichkommazahlen.  Bei jeder Operation passieren Fehler,  und bei jeder Operation bewegen wir uns immer weiter weg  von der tatsaechlichen Loesung.  Und ein vierter Grund waere die Groesse des AGS.  Die meisten AGS sind tatsaechlich so gross,  dass sie gar nicht in den Speicher passen  und dass wir sie nur approximativ loesen koennen.", "start": 591.0, "end": 613.0}, {"text": "  Eine Loesung dazu bietet die linaere Ausleihsrechnung,  und die wird dann im naechsten Tutorium behandelt.  Vielen Dank.", "start": 613.0, "end": 617.0}]}]