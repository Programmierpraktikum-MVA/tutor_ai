[{"lecture": "25797_60_course_video", "Timestamps": [{"text": "  In diesem Video wollen wir uns mit der Power-Iteration beschaeftigen.  Geht dabei um die praktische Berechnung von Eigenvektoren und Eigenwerten.  Wir wissen bereits aus der Linie ein Allgebra, dass die Eigenwerte z.B. analytisch bestimmen koennen.  Nehmen wir die Nullstellen von dem charakteristischen Polinom finden.", "start": 0.0, "end": 25.0}, {"text": "  Der charakteristische Polinom ist gegeben, als die Detaminante von A am Mindestlande auf der Hauptdiagonalen.  Wenn wir dann einen Eigenwertlammer gefunden haben, dann koennen wir die zugehoerigen Eigenvektoren finden.  Nehmen wir den Kern von A am Mindestlande auf der Hauptdiagonalen bestimmen.  Die Eigenvektoren sind dann eben die Vettoren, die in dem Kern liegen.", "start": 26.0, "end": 45.0}, {"text": "  Fuer den Computer ist das nicht einfach zu berechnen.  Allgemein das Berechnung von Nullstellen und Funktionen ist keine triviale Aufgabe fuer ein Computer.  Die Menschen koennen das gut, weil wir Erfahrungen haben, weil wir wissen, welche Schritte fuer Gewoehnungszimmerfolg fuehren.  Am Ende des Semesters werden wir uns mit Verfahren zur Bestimmung von Nullstellen beschaeftigen.", "start": 45.0, "end": 72.0}, {"text": "  Jetzt fuer diese Aufgabe zur Berechnung von Eigenvektoren ist die analytische Loesung von den charakteristischen Polinomen,  bzw. das Berechnung von dem Kern nicht die optimale Weste-Variante.  Besser funktioniert die Power-Iteration.  Die ist auch bekannt, dass das vom Mises-Verfahren und aber handelt es sich um eine algorithmische Methodik zur Bestimmung von Eigenvektoren und Eigenwerten.", "start": 72.0, "end": 99.0}, {"text": "  Die Idee dahinter ist, dass ein Vektor durch Anwendung einer Matrix immer in Richtung des Eigenvektors mit groessten Eigenwert gedrueckt wird.  Dann wird es hier eine Grafik, in der verschiedene Vektoren angezeichnet sind.  Vielleicht ist das ein Merkungen, die Vektoren gehen hier nicht von dem koordinanten Ursprung aus.", "start": 100.0, "end": 113.0}, {"text": "  Grundsaetzlich bestehen wir ja, dass Vektoren in eine Richtung von koordinaten Ursprung ausgehen.  Die Grafik hier anschautlich zu gestalten und uebersichtlich sind die Vektoren hier quasi kreuz und quer in den Vektorraum angezeichnet.  Was wir im wedentlichen sehen, ist, dass die der Vektorraum durch Anwendung der Matrix transformiert wird.", "start": 113.0, "end": 131.0}, {"text": "  Hier sieht es ein bisschen aus, dass wuerde hier so auseinandergezogen werden.  Wenn wir uns jetzt hier die verschiedenen Vektoren anschauen, dann sehen wir drei verschiedene Punkte.  Zum einen, dieser blaue Vektor, oder allgemein alle drei blaue Vektoren, werden durch Transformationen von dem Vektorraum einfach nur gestreckt.", "start": 131.0, "end": 154.0}, {"text": "  Als die Richtung dieser Vektoren bleibt gleich und nur die Laenge veraendert sich.  Dasselbe geht fuer die pinken Vektoren. Die Richtung dieser Vektoren bleibt gleich und es aendert sich, wenn ueberhaupt, die Laenge.", "start": 154.0, "end": 175.0}, {"text": " Aus dem Grund sind halt genau diese Vektoren, also die blauen und die pinken Vektoren Eigenvektoren,  weil sie nicht die Richtung veraendern, sondern nur skaliert werden durch Anwendung der Matrix.  Wenn wir uns die roten Vektoren anschauen, dann sehen wir, dass die auch eine bisschen ehrere Richtung aendern.  An dieser rote Vektor hier zum Beispiel wird auch ein bisschen gedreht.", "start": 175.0, "end": 191.0}, {"text": "  Als er aber auffaellt, auf allen roten Vektoren, nehmen wir die den Raum hier transformieren, also die uns auseinanderziehen,  dann leist sich die Richtung von den roten Vektoren, der Richtung von den blauen Vektoren an.", "start": 191.0, "end": 209.0}, {"text": " Wenn wir uns jetzt vorstellen, dass wir den Raum noch weiter auseinanderziehen wuerden,  dann ist es naheliegend, dass sich diese roten Vektoren in ihre Richtung immer weiter den blauen Eigenvektoren annaehern.", "start": 209.0, "end": 227.0}, {"text": " Es ist genau die Idee hinter der Power-Iteration, dass sie immer wieder die Matrix A, also die Matrix von der WD Albenwerte berechnen wollen,  immer wieder auf einen Vektor anwenden, und diese dann Stueck fuer Stueck in jeder Alteration immer weiter gegen den Albenwerk gedreht wird.  Zunaechst wollen wir eine mathematische Erklaerung uns anschauen, warum das tatsaechlich der Fall ist.", "start": 232.0, "end": 248.0}, {"text": "  Dazu muessen wir zunaechst ein paar Andamen treffen, und zwar soll A eine symmetrische Matrix-Saal mit Mn Eintraegen und die Matrix-Symmetrische.  Wissen wir, dass sie N-reelle Eigenwerte haben, die wir dann aussteigen und sortieren koennen.  Also Lambda N, der Betragsmessig groesste Eigenwert sein, und Lambda N-1, der Betragsmessig kleinste Eigenwert sein.", "start": 248.0, "end": 269.0}, {"text": "  Und wenn die Matrix A vollen Rang hat, dann sind die auch alle groesser als Null, echt groesser als Null, wenn A keinen vollen Rang hat,  dann koennte ja einer dieser Albenwerte, also Lambda N-1, und vielleicht auch ein paar weitere, genau gleich Null sein.  Auch wenn einige Albenwerte gleich Null sind, finden wir trotzdem immer genau N-orthogonale Eigenvektoren.", "start": 269.0, "end": 293.0}, {"text": "  Also reelle orthogonale Eigenvektoren, von Null bis vor M-1, wobei von Null eben der Albenwerk da zum Albenwerk Lambda Null sein soll.  Und da die orthogonale sind, wissen wir auch, dass sie lineaunabhaengig sein muessen.  Da ist in anderen Worten diese Vektoren von Null bis vor M-1, bilden eine Basis, weil wir haben ja genau N-lineaunabhaengige Vektoren in einem N-dimensionalen Vektor.", "start": 293.0, "end": 317.0}, {"text": "  Und wir sagen auch, dass wir eine Eigenbasis haben, weil diese Basis eben aus Albenvektoren besteht.  Jetzt koennen wir uns ein Vektor x Null nehmen, das soll der Startvektor von der Power-Iteration sein, den wir dann immer und immer wieder auf die Matrix A anwenden.  In dem Vektor x Null koennen wir jetzt ausdruecken bezueglich jeder beliebigen Basis.", "start": 317.0, "end": 338.0}, {"text": "  Wir waehlen eben genau diese Eigenbasis, die wir eben hier gefunden haben und sagen jetzt, die x Null koennen wir eben ausdruecken,  das ist eine Linearkombination der verschiedenen Eigenvektoren.  Und ja, also wir haben die Gewichte alpha Null bis alpha M-1,  Eigenvektoren, das ist dann eben genau eine Linearkombination, die x Null ergibt.", "start": 338.0, "end": 363.0}, {"text": " Und wenn wir jetzt dieses x Null anwenden wollen auf A,  dann koennen wir uns vorstellen, dass wir quasi einfach diese Linearkombination auf A anwenden, weil das ist ja genau x Null.  Und da A eine lineare Abbildung ist, koennen wir A direkt in die einzelnen Summanden von dieser Linearkombination hineinziehen.  Dann haben wir also alpha Null mal A mal V Null plus alpha 1 mal A mal V 1 und so weiter.", "start": 363.0, "end": 382.0}, {"text": "  Herr Clou, hinter der Power-Duration und hinter der mathematischen Anschauung hier,  ist jetzt das A mal V Null, der genauer ein Teil von einer Eigenwerk-Gleichung ist.  Also wir wissen ja, dass V Null ein Eigenvektor ist, das heisst A mal V Null,  dann spricht er einfach nur den Vektor V Null, es ist gelieht.  Also wir koennen V Null schreiben als lambda Null mal V Null.", "start": 382.0, "end": 403.0}, {"text": "  Anerlog koennen wir das fuer alle weiteren Summanden hier tun.  Da ist jetzt endlich verschwinden, all diese A sind in den Summanden  und es bleibt einfach nur eine andere Linearkombination stehen.  Also wir haben wieder eine Linearkombination der Eigenvektor, der Basis,  nur jetzt halt mit anderen Gewichten, weil wir ja eben auf x 1 kommen, also A x Null auf A abgebildet.", "start": 403.0, "end": 427.0}, {"text": "  Das koennt ihr euch vorstellen, wenn wir jetzt ein weiteres Mal dieses x 1 auf A anwenden,  dann koennen wir A wieder in die einzelnen Summanden reinziehen  und dann haetten wir hier A f Null mal lambda Null mal A mal V Null stehen, wieder.  Ja, auf der naechsten Seite mal aufgeschrieben.  Und jetzt koennen wir genau den selben Trick nochmal anwenden.", "start": 427.0, "end": 448.0}, {"text": "  Wir wissen ja wieder, dass diese V Is genau Eigenvektoren sind, das heisst A mal V I,  die gibt wieder genau lambda I mal V I.  Da wir hier zum Beispiel schon genau ein lambda Null stehen haben  und hier ja dann ein weiteres lambda Null erscheint, bzw. hier ein weiteres lambda 1,  ergibt sich dann lambda Null zum Quadrat.", "start": 448.0, "end": 471.0}, {"text": "  Also A f Null mal lambda Null von dem ersten Teil, A mal V Null wird wieder zu lambda Null mal V Null,  also A mal Null, A f Null mal lambda zum Quadrat mit V Null.  So, und das koennen wir jetzt wieder machen.  X, Y koennen wir wieder auf A anwenden, dann wuerde ich hier in der Stelle wieder ein weiteres lambda Null stehen.", "start": 471.0, "end": 489.0}, {"text": "  Und dann wuerde ich hier in der Stelle wieder ein weiteres lambda Null stehen,  also die Potenz, die wuerde wieder mal einsteigen werden, also lambda Null hoch 3.  Und das koennen wir quasi einfach T mal machen,  und dann haben wir hier irgendwann die T der Potenz von den Eigenwerten drinnen stehen.", "start": 489.0, "end": 510.0}, {"text": " Jetzt ist die Idee hinter der Power-Iteration, dass wir A immer wieder anwenden  und ich moechte jetzt in Forderung kurz einmal zeigen, wenn ich das immer und immer wieder mache,  dass wir am Ende uebrig ein Vielfaches von dem Eigenvektor zum groessten Eigenwert,  also genau dieses V Null bleibt am Ende uebrig, bezins was ein Vielfaches.", "start": 510.0, "end": 523.0}, {"text": "  Was das tatsaechlich so ist, sehen wir, wenn wir uns den Grenzwert anschauen,  also wir lassen T gegen und endlich laufen,  und damit das dann tatsaechlich hier bei dem Ergebnis rauskommt,  muessen wir erstmal das lambda Null hoch T ausklammern.", "start": 523.0, "end": 541.0}, {"text": " Wir wollen also einen kleinen Trick anwenden wieder,  wir machen jetzt mal einen kleinen Trick,  das wir jetzt mal in dieses lambda Null hoch T vorklammern.  In dem ersten Summanten kommt lambda Null hoch T genau einmal vor,  das heisst, nur alpha Null, mal v Null bleibt uebrig.  Einen anderen Summanten fehlt dieses lambda Null hoch T.", "start": 541.0, "end": 557.0}, {"text": "  Das heisst, hier muessen wir quasi durch lambda Null hoch T teilen,  damit mal lambda Null hoch T wieder genau das gleiche ergibt wie vorher.  Und wenn wir uns jetzt aber den Grenzwert von Aiden einzelnen Summanten hier anschauen,  dann sehen wir, dass sich hier lambda 1 geteilt durch lambda Null hoch T stehen haben.", "start": 557.0, "end": 578.0}, {"text": " Und wir haben mir die Eigenwerte aufsteigend sortiert,  da absteigend sortiert, wie auch immer, heisst, Sie wissen,  das lambda Null trage es maessig groesser sein soll als lambda 1.  Tatsaechlich haben wir vorher gesagt, dass es groesser gleich sein soll.  Wir gehen jetzt mal hier davon veranfachen,  dass lambda Null tatsaechlich groesser als lambda 1 ist.", "start": 578.0, "end": 596.0}, {"text": "  Und genau im Grenzwert lambda Null geteilt, lambda 1 geteilt durch lambda Null.  Und wenn lambda Null groesser ist, dann ist das irgendwas kleiner 1  und irgendwas kleiner 1 hoch T mit T gegen unendlich, vorlaeuft gegen Null.  Wenn damit, wenn hier quasi der eigentliche Buch Null wird,  dann wird halt alpha 1 mal v 1 auch Null, heisst, das ist ja ganz ein Summant fuer Null.", "start": 596.0, "end": 617.0}, {"text": "  Es gilt aber halt auch fuer alle anderen Summanten,  weil fuer alle Summanten gilt, dass das lambda k geteilt durch lambda Null kleiner ist, einserben muss.  Wir koennen den Trick quasi auf alle Summanten anwenden  und was dann uebrig bleibt, ist eben einfach nur noch lambda Null hoch T mal alpha Null mal v Null.", "start": 617.0, "end": 638.0}, {"text": "  Als was uebrig bleibt, ist genau ein Vielfaches von dem Eigenvektor zum groessten Albenwert.  Ist ja genau das, was wir quasi teilen wollen.  Wenn wir A unendlich oft anwenden,  dann bleibt irgendwann ein Vielfaches von dem Eigenvektor zum groessten Albenwert uebrig.", "start": 638.0, "end": 659.0}, {"text": " Und tatsaechlich ist es quasi das unendlich Vielfache,  weil lambda hoch T fuer T gegen unendlich, wenn lambda groesser als 1 ist,  dann geht das ganze ja quasi wieder gegen unendlich.  Das heisst, wir haben v Null unendlich fortgesetzt, also den unendlich langen Vector v Null.", "start": 659.0, "end": 675.0}, {"text": " Sonst hier aber egal sein, praktisch, wenn wir das dann so machen,  dass wir nach jeder Iteration, nach jeder Anwendung der Matrix A das Ergebnis normieren,  also auf Einheitslaenge bringen und damit vermeiden wir,  dass wir quasi einen unendlich langen Vector bekommen.  Aber wir bekommen eben genau die Richtung v Null und an diese Richtung sind wir interessiert.", "start": 675.0, "end": 689.0}, {"text": "  Da habe ich das hier noch mal gerhythmisch aufgeschrieben,  genauso sollte das auch in der Hausaufgabe dann implementieren.  Wir starten mit irgendeinen zufaelligen Vector, da haben wir x Null,  da hat irgendwelche zufaellige Werte als Eintraege und nehmen wir mal wieder auf diese Matrix A an.  Das heisst, x T plus 1 ist quasi einfach x von T angewendet auf A.", "start": 689.0, "end": 713.0}, {"text": "  Und wie ich es eben schon erwaehnt habe, wollen wir den Vector A mal x 2 T normieren.  Wir teilen direkt durch die Normen und dadurch hat x von T plus 1 genau Einheitslaenge.  Also wir stellen sicher, dass nach jedem Iterationsschritt genau Einheitslaenge haben.", "start": 713.0, "end": 734.0}, {"text": " Das koennen wir auch ohne Weiteres machen, weil wir wissen, wir sind nur in der Richtung des Vectors,  das Eigenvektor ist am Ende interessiert, weil wir wissen, jedes Vielfache von einem Eigenvektor ist auch wieder ein Eigenvektor.  Als die Laenge ist uns jetzt endlich egal, wir wollen wirklich nur die Richtung haben.", "start": 734.0, "end": 743.0}, {"text": "  Das heisst, wir koennen uns quasi auf Laenge 1 einschraenken und das bringt uns dann den Vorteil,  dass halt nun mehrere Stabile unterwegs sind.", "start": 743.0, "end": 762.0}, {"text": " Also wenn wir das nicht machen, wuerde der Vector immer laenger und laenger bleiben,  dann haetten wir irgendwann unter Umstaenden Overflow, koennten quasi diese grossen Werte nicht mehr mit  und dann verfuegbaren gleichkommazalen Darstellen und zum anderen bringen halt grosse Werte  wieder andere Genauigkeitshaluste mit und so weiter und so fort.", "start": 762.0, "end": 770.0}, {"text": "  Deswegen ist es herwuenschter wert, dass wir die Laenge immer auf 1 beschraenken.  Und dann koennen wir auch ohne Weiteres ganz einfach den Eigenwert bestimmen.  Also der Eigenwert, der approximative Eigenwert im Schritt T plus 1,  spricht einfach der Laenge von X und T, wie du rum angewendet auf A.  Genau, also der Laenge davon. Das kann man relativ einfach sehen.", "start": 770.0, "end": 796.0}, {"text": "  X von T hat ja einheitliche Laenge, das heisst, wenn wir X von T auf A anwenden  und X von T schon approximativ im Eigenvektor ist, dann wird ja XT eigentlich nur verlaengert, also nur skaliert.  Wenn wir dann nach der Anwendung von A messen, wie viel laenger A mal X geworden ist,  dann haben wir genau den Faktor, mit den dieser vermeintliche Eigenvektor skaliert wird.", "start": 796.0, "end": 824.0}, {"text": "  Und das entspricht genau den Eigenwert. Also nochmal vielleicht ein bisschen anschaulicher als Beispiel.  In V dann dieses X ist, also der Eigenvektor und wir mit A multiplizieren,  wenn V vorher Laenge 1 hat, und wenn man da hier, genau, V hat Laenge 1,  den multiplizieren mit A und dann haben wir hinterher Laenge 2,5,  dann skalieren wir dieses V ja quasi um den Wert 2,5.", "start": 824.0, "end": 848.0}, {"text": "  Dieser Wert 2,5 entspricht dann genau den Eigenwertlammder.  Ja, jetzt haben wir festgestellt, die Power-Iteration fuehlt uns immer zu den Eigenwerten mit groessten Albenwert.", "start": 848.0, "end": 865.0}, {"text": " Jetzt stellt sich aber die Frage, wie werden weitere Albenwerttoren ausgerechnet,  weil wir wollen ja in der Regel nicht nur den ersten Eigenvektor haben,  sondern auch den zweiten und den dritten. Ein einfacher Ansatz waere es,  den Startvektor orthogonal zu den bereits bekannten Albenwerttoren zu waehlen.  Das sieht man ganz gut hier in dieser Grafik vom Anfang.", "start": 865.0, "end": 877.0}, {"text": "  Diese pinken Vektoren liegen ja genau orthogonal auf den blauen Vektoren.  Das ist auch sinnvoll, weil fuer siemetrische Matrits und Wissen wird es die Albenwerttoren alle orthogonal aufeinander legen.  Jetzt sehen wir, die roten Vektoren, die zu keinem der Albenwerttoren orthogonal liegen,  leisen sich in ihrer Richtung immer mehr den blauen Vektoren.", "start": 877.0, "end": 895.0}, {"text": "  Aber diese pinken Vektoren, die orthogonal dazu stehen,  bleiben in ihrer Richtung genau gleich, weil sie sind ja auch schon Albenvektoren.  D.h. wenn wir einen Vektor finden, der genau orthogonal zum Albenvektor bleibt,  dann wird er nicht mehr in diese Richtung von dem Albenvektor gedrueckt,  sondern dann unter Umstaenden in die Richtung von den zweiten oder dritten Eigenvektor.", "start": 895.0, "end": 917.0}, {"text": "  So, haben wir uns jetzt hier zu Nutze machen.  Und wie jedenfalls die den Startvektor orthogonal zu den bereits fehlkanten Eigenvektoren,  dann koennen wir hoffen, dass dieser Vektor wieder gegen den zweiten, dritten, vierten Albenvektor steht.  Problem dabei ist dann wieder der numerische Fehler.", "start": 917.0, "end": 941.0}, {"text": " Aufgrund der numerischen Ungenauigkeiten bei der Matrix-Vektor-Multiplikation,  und bei den Normieren, durch den numerischen Fehler,  werden wir am Ende trotzdem gegen den ersten Eigenvektor streben.  Weil wir immer wieder kleine Ungenauigkeiten haben,  und der Vektor nie ganz exakt orthogonal auf den ersten Vektor stehen wird.  D.h. das fuehrt uns nicht zum Ziel.", "start": 941.0, "end": 962.0}, {"text": "  Und stattdessen muessen wir im jeden Schritt eine kleine Korrektur vornehmen.  Heisst in jedem Schritt, stellen wir zusaetzlich sicher,  dass der aktuelle Vektor orthogonal auf den bereits bekannten Eigenvektoren steht.  Wenn wir das schaffen, dann koennen wir eben auch den zweiten, dritten Eigenvektor appaktiv bestimmen.", "start": 962.0, "end": 982.0}, {"text": "  Aber dieses X-Dach hier ist quasi wieder genau die gleiche uebliche Interaktionsvorschrift wie vorher.  Wir werden das verheerende Xt an auf A normieren,  und dann graeben wir Xt plus 1.  Allerdings hier mit einem Dach.  Und hier wollen wir den Karten Eigenvektor bestimmen.  Heisst hier gehen davon aus, dass wir alle anderen Eigenvektoren von Lul bis K minus 1 bereits kennen.", "start": 982.0, "end": 1008.0}, {"text": "  Und jetzt haben wir ja gesagt, okay,  der soll optimalerweise immer orthogonal auf den anderen Eigenvektoren stehen.  Wir haben den X-Dach genannt, weil wir jetzt hier eine kleine Korrektur vornehmen wollen,  und jetzt alle nicht-orthogonale Komponenten abziehen.", "start": 1008.0, "end": 1032.0}, {"text": " Weil es zwar alle Eigenvektoren, die wir bereits kennen, von Lul bis K minus 1,  berechnen wir hier etwas, korrigieren also etwas,  und diese Korrektur ziehen wir dann halt von den Xk ab.  Und dadurch stellen wir sicher, dass wir in X dann immer einen Vektor haben,  der orthogonal zu einem bereits bekannten Eigenvektor an ist.", "start": 1032.0, "end": 1046.0}, {"text": " Dann machen wir in jedem Schritt, Schritt fuer Schritt,  und dann naeheren wir uns halt den zweiten, dritten Eigenvektor an.  Auch den vierten Eigenvektor.  Jetzt ist noch interessant, was hier steht.  Wir haben im Wedeligen das Skalaprodukt von VI und X.  Wir berechnen also hier das Skalaprodukt,  und wenn V und X bereits orthogonal sind,  dann wird dieses Skalaprodukt ja genau zu Null.", "start": 1048.0, "end": 1069.0}, {"text": "  Wenn die orthogonal sind, es wird das Null.  V mal Null ist wieder Null.  Heisst, wir nehmen dann in den Summanten hier keine Korrektur vor.", "start": 1069.0, "end": 1092.0}, {"text": " Wenn fuer Yodan ein V, aber V mal Xk, also V und X,  nicht genau orthogonal aufeinander stehen und hier einen kleinen Wert ergeben,  also Yodan einem Wert, zum Beispiel hat 0,25, also einen recht kleinen Wert,  wissen wir, okay, die sind nicht ganz orthogonal.  Also ziehen wir einen Viertel von V ab beispielsweise  und erreichen damit, dass X dann wieder orthogonal auf den V steht.", "start": 1092.0, "end": 1105.0}, {"text": "  Schauen also wie viel quasi nicht orthogonal es an den Vektoren  und diesen Anteil ziehen wir dann genau ab.  Das kommt euch vielleicht bekannt vor von den Verfahren von Graben Schmidt  zur Berechnung autonomaler Basen.", "start": 1105.0, "end": 1126.0}, {"text": " Ihr kennt die aus der Linie an Eilgebrad, da haben wir angefahren mit einer normalen Basis,  dann haben wir den ersten Vektor normiert  und von dem zweiten Vektor haben wir dann eben die nicht-autogonalen Komponenten abgezogen.  Jeden dann wieder normiert, von dem dritten Vektor haben wir die nicht-autogonalen Komponenten  von den ersten beiden Vektoren abgezogen, wieder normiert und so weiter.", "start": 1126.0, "end": 1137.0}, {"text": "  Das Ergebnis war dann wieder eine autonormale Basis.  Also eine Basis, deren Eigenvektoren, deren Vektoren alle autogonal aufeinander stehen  und die alle Eigen-Einheitslaenge haben.  Genau, da haben wir also quasi auch schon genau dieses Abziehen  von den nicht-autogonalen Komponenten verwendet.  Daher kommt euch das vielleicht bekannt vor.", "start": 1137.0, "end": 1163.0}, {"text": "  Am Schluss moechte ich noch einmal den Zusammenhang zu der Eigenzerlegung erlaeutern.  Die Eigenzerlegung kennt ihr bereits aus Jonas' Video.  Und da koennen wir, wenn wir in der Power-Iteration ein X auf ein A anwenden,  dieses A ja schreiben, das V mal Lambda, mal V transponiert.  Genau die Eigenzerlegung.", "start": 1163.0, "end": 1187.0}, {"text": " Und jetzt koennen wir uns mal anschauen, was quasi passiert mit dieser Eigenzerlegung,  wenn wir X immer und immer wieder auf dieses A anwenden.  Diese Eigenzerlegung kann man quasi unterscheiden in drei verschiedenen Operationen.  Die erste ist ein Basiswechsel, also wir rechnen V transponiert mal X.  Wir wissen ja, dass in V in den Spalten die Basisvektoren stehen.", "start": 1187.0, "end": 1207.0}, {"text": "  In V stehen die Eigenvektoren und wir wissen ja, wir haben genau N Eigenvektoren  und die bilden damit eine Basis.", "start": 1207.0, "end": 1230.0}, {"text": " Das heisst, in V in den Spalten stehen Basen  und wir wissen, steht eine Basis und wir wissen aus den Liner-Video ganz am Anfang,  dass wir dann einfach ein Basiswechsel vornehmen koennen, indem wir V mal in einem Vektor rechnen  und wegsehen wir quasi in die Spaltenbasis von dieser Matrix.  Und da V jetzt autogonal ist, wissen wir, dass V transponiert, der im Versen spricht.", "start": 1230.0, "end": 1239.0}, {"text": "  Heisst, hier nehmen wir ein Basiswechsel zurueck vor.  Wir koennen uns das so vorstellen, dass X bereits in der Eigenbasis von V liegt.  Durch V transponiert, wechseln wir wieder auf die Standardbasis, also genau auf die uebliche Standardbasis.  Das entspricht quasi dem Fall, dass die Eigenvektoren genau den Achsen dann entsprechen.", "start": 1239.0, "end": 1267.0}, {"text": " In der ersten Eigenvektoren entspricht genau der X-Achse,  in der zweiten Eigenvektoren genau der Y-Achse und so weiter.  Wir werden dann mit den Eigenwerten auf der Diagonalempfehlung in Landen skaliert.  Und da wir ja vorher eben in den Ursprung gedreht haben,  skalieren wir quasi den V Transponiert X dann an auf Lande.  Und dann haben wir den V Transponiert X dann auf Lande.", "start": 1267.0, "end": 1289.0}, {"text": "  Und dann haben wir den V Transponiert X dann auf Lande.  Und dann haben wir den V Transponiert X dann auf Lande.  Und da wir ja vorher eben in den Ursprung gedreht haben,  skalieren wir quasi den Langen der ersten Achse mit den ersten Eigenwerten.  Und das ist ja genau die Achse, die quasi dann dem ersten Eigenwerten entspricht.", "start": 1289.0, "end": 1310.0}, {"text": "  Also skalieren entlang des ersten Eigenvektors, entlang des zweiten Eigenvektors,  mit den ersten Eigenwerten, mit den zweiten Eigenwerten und so weiter.  Und danach wenden wir das Ergebnis hier von, also Lande,  mal V Transponiert mal X, wieder an und auf V.  Und jetzt hatte ich eben schon gesagt, V und V Transponiert verhalten sich genau im Wasser ineinander.", "start": 1310.0, "end": 1331.0}, {"text": "  Heisst im wedentlichen rotieren wir jetzt einfach wieder zurueck in die eigentliche Eigenbasis von vorher.  Wir werden jetzt wenn wir X aber eben nicht nur einmal auf A an, sondern mehr ver.  Das heisst wir koennen uns anschauen, was passiert, wenn wir A mal A schreiben.  Also A koennen wir einfach ausdruecken, als V mal Lande, mal V Transponiert.  A mal A ist dann eben zweimal diese Eigenzerlegung.", "start": 1331.0, "end": 1351.0}, {"text": "  Wenn der V jetzt autogonal ist, paedelt der Teil hier in der Mitte genau weg.  Also V Transponiert mal V wird genau zu der Einheitsmatrix.  Dann wird quasi dar stehen, V mal Lande, mal Lande mal V Transponiert,  ist genau V mal Lande squared mal V Transponiert.  Und genauso wie eben bei der mathematischen Erklaerung,  dann wird das jetzt quasi T mal machen.", "start": 1351.0, "end": 1375.0}, {"text": "  Also wir koennen einfach A ht rechnen, also A mal A mal A und all diese V,  all diese autogonalen Matrizen in der Mitte fallen dann immer weg und kollabieren zu Einheitsmatrizen.  Dadurch, dass der A halt immer wieder hintereinander schreiben.  Und was dann uebrig bleibt, ist einfach V mal Lande, ht mal V Transponiert.", "start": 1375.0, "end": 1398.0}, {"text": " Als wenn wir jetzt X oder wenn wir die Matrix A T mal auf X anwenden,  dann ist das nichts anderes, als wenn wir nur einmal rotieren,  dann zu der Potenz T skalieren und dann wieder zurueckrotieren.  Als die Rotationen haben keinen Einfluss, die bleiben genau gleich.  Als entscheidend ist es dann aber halt das Skalieren mit der Potenz T entlang der Eigenvektor.", "start": 1398.0, "end": 1419.0}, {"text": "  Und genauso wie bei der mathematischen Erklaerung im Anfang,  wuerde sich halt hier der groesste Eigenwert dominant verhalten.  Das waere halt das Dominant des Skalieren, das der groesste Eigenwert das skaliert am meisten.  Und damit wuerden wir immer am meisten skalieren entlang des Eigenwerts zum groessten Eigenwert.", "start": 1419.0, "end": 1445.0}, {"text": " Und damit wuerde jetzt aehnlich genauso wie wir es halt vorhin auch schon gesehen haben,  der Vektor X gegen den Eigenwert zum groessten Eigenwert streben.  Okay, das war es zu der Power-Iteration, dann viel Erfolg und Spass bei den anderen Videos.", "start": 1445.0, "end": 1451.0}]}]