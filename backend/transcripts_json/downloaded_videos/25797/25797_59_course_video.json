[{"lecture": "25797_59_course_video", "Timestamps": [{"text": "  Bekommst du im Tutorials Video zur Eigenzerlegung.  Ich bin Jonas und ich moechte euch in diesem Video ein bisschen erklaeren, wie man auf die  Eigenzerlegung kommt, was das ist und dann auch wie man sie berechnet oder wie man sie  benutzt in Nummpai.", "start": 0.0, "end": 32.8}, {"text": " Die Eigenzerlegung ist erstmal eine Operation, die wir aushoeren koennen fuer diagonalisierbare  Matrizen, zum Beispiel symmetrische Matrizen und diese Zerlegung basiert dann auf Matrizen  von Eigenwerten und Eigenvektoren von dieser Matrix.  Das heisst, wir werden diese Matrix mit anderen Matrizen darstellen.", "start": 32.8, "end": 46.64}, {"text": "  Und Eigenwertprobleme treten generell in vielen Disziplinen auf, da wo halt linearer Allgueba  benutzt wird, zum Beispiel auch in der Physik oder in der Informatik und dabei hilft eben  die Eigenzerlegung bei diesem Problem diese Probleme dann zu loesen.", "start": 46.64, "end": 72.92}, {"text": " Und wir werden auch noch eine Eigenzerlegung oder eine Art der Eigenzerlegung kennenlernen  in der Praxisaufgabe und zwar die sogenannten Eigenfaces fuer Gesichtsbilder.  Wir betrachten erstmal eine beliebige symmetrische Matrix, eine n-Cords n-Matrix und aus der  linearen Allguebra wissen wir dann, dass wir eben n-Paare aus orthogonalen normierten Eigenvektoren  und Eigenwerten von a finden koennen.", "start": 72.92, "end": 101.2}, {"text": "  Und der entsprechende Eigenvektor und der entsprechende Eigenwert ist dann durch diese  Eigenwertgleichung hier gegeben und wir haben eben n verschiedene Eigenvektoren mit  entsprechenden Eigenwerten.  Und wie gesagt, wir wollen jetzt also eine Zerlegung finden von dieser Matrix a mit den Eigenvektoren  und den Eigenwerten.", "start": 101.2, "end": 129.52}, {"text": "  Und dafuer betrachten wir erstmal die Matrix der Eigenvektoren, die steht hier und diese  Matrix v ist jetzt auch eine n-Cords n-Matrix und sie hat als Spaltenvektoren jeweils die  Eigenvektoren der Matrix a, die gegeben sind durch die Eigenwertgleichung oben.", "start": 129.52, "end": 158.20000000000002}, {"text": " Und wir wissen ja, dass diese Vektoren v0, v1 alle orthogonal zueinander sind und deswegen  ist auch diese Matrix v eine orthogonal Matrix, also ihre Transponierten entspricht ihrer Inversen.  Und jetzt koennen wir auch noch betrachten, was passiert, wenn wir eben das, wenn wir a  auf v anwenden, also wenn wir a mit v multiplizieren.", "start": 159.2, "end": 178.0}, {"text": "  Und wir werden sehen, wir koennen dann diese Eigenwertgleichung benutzen, um das Produkt  sprechen um zu formen.  Also v ist gegeben wie gehabt als Matrix der Spaltenvektoren, also der Eigenvektoren als  Spaltenvektoren.", "start": 178.0, "end": 205.96}, {"text": " Und wenn wir jetzt das Matrixprodukt bilden, sieht das wie folgt aus, also wir schreiben  das eben so, nach der Definition des Matrixproduktes, kann man die Matrix, die eben aus diesem  Produkt entsteht, wie folgt schreiben.  Und das ist dann wiederum nur eine Matrix, die als Spaltenvektoren jeweils die Matrix  mal den Eigenvektor hat.", "start": 205.96, "end": 222.76000000000002}, {"text": "  Da koennen wir jetzt die Eigenwertgleichung von vorher benutzen und kommen dann eben  auf die folgende Matrix einfach eingesetzt.  Jedes Mal steht jetzt hier nur noch der Eigenwert mal der entsprechende Eigenvektor  als Spalter.  Wenn man sich jetzt diese Matrix genauer anguckt, koennen wir diese Matrix wiederzerlegen  in das Produkt von zwei Matritzen.", "start": 222.76000000000002, "end": 255.76}, {"text": "  Und zwar einmal erkennen wir die Matrix der Eigenvektoren, die wir vorhin schon gesehen  hatten, mit den Eigenvektoren als Spalten und einer Diagonalmatrix mit den Eigenwerten  auf der Diagonalen.  Dieses Produkt ergibt dann eben genau diese Matrix oben.  Wir haben dann immer den entsprechenden Eigenwert multipliziert mal bei dem Spaltenvektor  der Eigenvektor.", "start": 255.76, "end": 289.04}, {"text": "  Und diese Matrix, diese Diagonalmatrix der Eigenwerte, nennen wir jetzt Lambda, GrossLamda,  das ist auch eine N-Kotz-N-Matrix.  Wir hatten also gesehen, dass als wir das Produkt A mal V gebildet haben, also die  Matrix A mal die Matrix der Eigenvektoren von A, dass wir ueber die Eigenwertgleichung  darauf kommen, dass das gleich V mal die Diagonalmatrix der Eigenverse ist.", "start": 289.04, "end": 326.04}, {"text": "  Jetzt koennen wir an die Gleichung oben von rechts die Inverse der Eigenvektormatrix  ranmultiplizieren, somit dass wir dann auf der linken Seite A stehen haben, auf der rechten  Seite V Lambda mal V minus 1.  Und da V eine Orthogelnalmatrix ist, koennen wir also die Inverse einfach ihrer Transmonierten  gleich setzen.", "start": 326.04, "end": 357.16}, {"text": "  Anschollig entspricht diese Zerlegung in die Richtung der Eigenvektoren, also hier gegeben  durch diese Matrix der Eigenvektoren und dann eine entsprechende Skalierung durch die Eigenwerte  und dann transformieren wir wieder zurueck.", "start": 357.16, "end": 380.24}, {"text": " Das heisst wir transformieren mit der ein quasi in die Eigenbasis, skalieren dann die  entsprechenden Eigenvektoren mit den entsprechenden Eigenwerten und transformieren dann wieder  zurueck, wenn V transponiert.  Das koennen wir jetzt mit NumPri auch umsetzen.  Dazu habe ich hier ein Beispiel vorbereitet.", "start": 380.24, "end": 407.16}, {"text": " In diesem Beispiel definiere ich mir eine symmetrische Matrix und print sie und wie  ihr sehen koennt, diese Matrix ist auf jeden Fall symmetrisch.  Und jetzt werde ich mithilfe der Eigenverlegung diese Matrix zerlegen.  Dafuer berechnen wir erstmal die Eigenwerte und die Eigenvektoren mithilfe von NumPri  und der Befehl dafuer fuer symmetrische Matrix ist NumPri.link.ik.", "start": 407.16, "end": 434.0}, {"text": "  Und wie ihr seht, kriege ich einerseits eben einen Array oder einen Vektor mit den Eigenwerten  zurueck sowie eine Matrix, die in den entsprechenden, die als Eintraege eben die entsprechenden  Eigenvektoren hat.  Die Eigenwerte sind auch schon sortiert und die Eigenwerte sind dementsprechend auch sortiert.  Wir hatten gesehen, dass das ist die Eigenzerlegung fuer A.", "start": 434.0, "end": 461.52}, {"text": "  Genau das implementieren wir jetzt in NumPri.  Also einerseits hier definieren wir die Diagonalmatrix zur Eigenwerte und dann implementieren  wir diese Formel.  Wir berechnen also das Produkt aus der matrix der Eigenwerte und der transminierten matrix  der Eigenvektoren und das ist unsere Eigenzerlegung nach der Gleichung oben.", "start": 461.52, "end": 491.72}, {"text": "  Wenn wir beide Ergebnisse ausgeben, also einmal die Eigenzerlegung und einmal die symmetrische  Matrix sehen wir, das ist ja genau das, die sind genau gleich und das ist, ergibt auch  Sinn.  Das hatten wir gezeigt, wieso das geht.  Genauso kann man jetzt auch ueber die Eigenzerlegung die Inverse bestimmen von einer Matrix.", "start": 491.72, "end": 513.32}, {"text": "  Und zwar, wenn wir die Inverse von der Matrix A bilden wollen, dann bilden wir einfach die  Inverse von der Eigenzerlegung und wenn wir die Inverse von diesem Produkt von drei Matrizen  bilden wollen, dann wenn wir diese Klamme aufloesen, dann vertauscht sich die Reihenfolge und  wir muessen auch die Inverse bilden und fuer die Matrix der Eigenvektoren ff ist das ja  einfach, weil sie symmetrisch.", "start": 513.32, "end": 539.9599999999999}, {"text": "  So koennen wir das direkt einsetzen.  So kommt wieder darauf, dass die Inverse direkt geben ist durch dieses Produkt von der Matrix  der Eigenvektoren, der Inversen diagonal Matrix, der Eigenwerte sowie der transminierten  Matrix Eigenvektoren.  Und so sieht die Inverse der Diagonalmatrix aus.  Also sie hat die Inversen Eigenwerte auf der Diagonal.  Und Nampei sieht das wie folgt aus.", "start": 539.9599999999999, "end": 578.04}, {"text": "  Ich berechne einmal die Inverse mit Nampei und dann werde ich einmal die Inverse mithilfe  der Eigenzerlegung berechnen.  Und ich definiere eben hier diese Diagonalmatrix der Inversen Eigenwerte und dann berechne  ich das, wenn wir uns die Ergebnisse angucken, sehen wir auch wieder, dass die identisch  sind.  Also diese Eigenzerlegung ist relativ nuetzlich.", "start": 578.04, "end": 605.52}, {"text": "  Genau in diesem Video hatte ich jetzt euch eben gezeigt, wie man auf die Eigenzerlegung  kommt mit der Matrix der Eigenvektoren, dann bildet man das Produkt von der Matrix  und der Matrix der Eigenvektoren und kommt auch auf Aha.  Da steht ja eigentlich auch nur die Matrix der Eigenvektoren mal die Diagonalmatrix  der Eigenwerte.", "start": 605.52, "end": 627.68}, {"text": "  Und es gibt auch noch eine generalisierte Form der Eigenzerlegung, die in spaeteren Videos  kennenlernen werdet.  Es ist die Singulaerwertelegung.  Und auch ist es so, dass diese Berechnung der Eigenwerte und Eigenvektoren auf Basis  der Eigenwertgleichung eben wie in vorherigen Videos gesehen hat mit dem charakteristischen  Polinom fuer grosse Matrizen sehr aufwendig wird.", "start": 627.68, "end": 652.24}, {"text": "  Und im folgenden Video werdet ihr dann ein Algorithmus kennenlernen, mit dem man das  effizienter machen kann, die Eigenwerte zu bestimmen und die Eigenvektoren.", "start": 652.24, "end": 656.96}]}]