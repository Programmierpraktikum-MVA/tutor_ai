[{"lecture": "25797_13_course_video", "Timestamps": [{"text": "  In dem letzten Video wollen wir die verschiedenen Methoden zum Loesen von den Leitungssystemen, die wir bisher kennengelernt haben, einmal verleichen und ueber die Konditionen zahlen, wenn wir trotzdem sprechen.  Ein lineares Gleichungssystem hat die Form AX ist gleich B, A ist eine MxM Matrix, die eine lineare Abbildung darstellt von R hoch N auf den R hoch M.", "start": 0.0, "end": 26.0}, {"text": "  Und wir wissen, so ein System hat M wieder eine Loesung, keine Loesung oder ein Endeschritt.  Die erste Methode, die wir kennengelernt haben und diese Loesung zu bestimmen oder eine Loesung zu bestimmen, ist der Gosselgeruppt-Must.  Ziel ist es, dass wir eine Matrix A in eine obere 3xM Matrix verwandeln.  Der A geschrieben das erweiterte Kulifizienten Matrix.", "start": 26.0, "end": 52.0}, {"text": "  Und fuer ein 3x3 Beispiel wollen wir dann zum Beispiel eine 3xM Matrix dieser Form hier bekommen.  B nehmen wir natuerlich mit und muessen wir auch anpassen.  Die operatoren teilen aufeinander, die ruecken sich natuerlich auch auf das B aus, das ist hier die Strich.  Und genau diese obere 3xM Matrix, wenn wir genau dann eine eindeutige Loesung haben.", "start": 52.0, "end": 74.0}, {"text": "  Wenn wir keine eine eindeutige Loesung haben, dann wuerde hier zum Beispiel eine Nullzahl entstehen.  Der Gosselgeruppt-Must wuerde auch fuer ein ueberbestimmtes System funktionieren, das heisst, wo wir ganz viele Gleichungen haben.  Und weniger variablen.  Und was da dann passieren wuerde, ist, dass wir M wieder 3 Koffelemente haben, also wirklich 3 Zeilen und ganz viele Nullzahlen.", "start": 74.0, "end": 96.0}, {"text": "  Koennen wir das wieder eindeutig loesen.  Oder wir haben mehrere Zeilen, die einen Widerspruch geben.  Dann wissen wir, dass wir keine Loesung haben.  Aber auch in dem Fall, wo wir kein quadratisches System haben, das haben wir uns jetzt bisher nicht genau angekuert fuer den Gosselgeruppt.  Aber auch in dem Fall koennten wir das halt durchwuehren und koennten einen Rueckschluss auf diese Loesung ziehen.", "start": 96.0, "end": 122.0}, {"text": "  Das kennt ihr wahrscheinlich aus den Nihnern eingebraten, dass man es an eine luetische Aufhoehre nicht quadratische Matrixen durchfuehrt.  Die Laufzeit fuer den Gosselgeruppt muss erst mal N hoch 3.  Und dann muessen wir eigentlich noch mal rueckwaerts einsetzen, um die tatsaechliche Loesung x zu bestimmen.  Das haette eine Laufzeit von N\u00b2.  N hoch 3 verstueckt natuerlich dieses N\u00b2.", "start": 122.0, "end": 142.0}, {"text": "  Das heisst grundsaetzlich, als im Tote, haben wir eine Laufzeit von N hoch.  Dann haben wir die Inverse noch mal mit eingeschaut oder wiederholt aus der Linerne Algebra.  Das heisst, wenn wir eine Matrix A hoch minus 1 finden,  dann koennen wir als Gleichungssystem A x ist gleich B loesen, indem wir von links den Versen ueberdiplizieren.  Das ist halt nach Definition genau die Identitaet.", "start": 144.0, "end": 187.0}, {"text": "  Und dann geben wir halt die Loesen durch A investiert mal B.  Auch die Inverse hat eine Laufzeit von N hoch 3.  Und genau das Problem ist hier hat zum einen, dass es halt eine merisch instabil sein kann.  Und zum anderen geben wir absolut nur dann eine Loesung, wenn wir eine eindeutige Loesung haben.  Das heisst, nur fuer eindeutige Loesungen koennen wir ueberhaupt investieren.", "start": 187.0, "end": 215.0}, {"text": "  Und ansonsten sehen wir halt nur, wir haben keine Loesung, da wir haben unendliche Loesungen.  Aber wir koennen auch die Faelle nicht direkt unterscheiden.  Also wir wissen nicht, ob wir keine Loesung oder unendliche Loesungen haben.  Wir sind nur okay, wir koennen das System nicht eindeutig loesen.  Also die Inverse ist auch genauso, wenn ich wieder Gauss ergerut muss.", "start": 215.0, "end": 235.0}, {"text": "  Und jetzt, ich kann das halt zu numerischen Schwierigkeiten.  Dann haben wir die Scholeskelzerlegung kennengelernt.  Hier brauchen wir eine symmetrische Matrix A, das heisst A mal A.  A ist halt transponiert.  Und A ist positiv definit.  In speziellen Faellen funktioniert das auf viel positiv semi definierte Matrizen.", "start": 235.0, "end": 266.0}, {"text": "  Aber da wissen wir aus dem letzten Video von mir, dass wir dann eigenwert gleich Null haben und sowieso das nicht eindeutig loesen koennen.  Deswegen beschraenken wir das mal auf positive definierte Matrizen.  Und wir wollen A da legen in eine untere Dreiecksmatrix L, die obere Dreiecksmatrix L transponiert.  Und dann ist die Idee, dass wir halt Reichungssystem loesen koennen.", "start": 266.0, "end": 290.0}, {"text": "  Durch einmal vorwaerts und einmal rueckwaerts einsetzen.  Das heisst, wir loesen erst nach L mal Y ist gleich D.  In dem zweiten Schritt koennen wir loesen nach L transponiert X, durch Y.  Und dann haben wir da durch die Loesung, das heisst, wir muessen einmal vorwaerts einsetzen und einmal rueckwaerts einsetzen.  Das ist also quasi beides dann der Laufzeit O und N\u00b2.", "start": 290.0, "end": 318.0}, {"text": "  Und da vor muessen wir da erst mal ueberhaupt die Scholeskidelegung berechnen.  Also diese Zerlegung hier, das hat trotzdem eine Laufzeit von N hoch 3.  Allerdings ist es dennoch wesentlich effizient, dass der Gausserl gewirkt muss.  Das wesentlich ist vielleicht schwierig zu sagen.  Aber dieses N hoch 3 hat ein wesentlich geringer verstanden Faktor.", "start": 318.0, "end": 341.0}, {"text": "  Das heisst, der Aufwand bei den Gausserl gewirkt muss, da ist er totisch gesehen trotzdem,  Nein eben nicht an so einem Totisch gesehen, aber Gausserl gewirkt muss, der ist trotzdem noch aufwendiger,  weil wir halt vorhin den N hoch 3 einen groesseren Faktor haben, aber die Landau-Symbolen versturken ja den Faktor.  So Lesky-Zerlegung ist insgesamt dann in der Regel noch ein bisschen zuegiger.", "start": 341.0, "end": 364.0}, {"text": "  Und vor allem hat sie den grossen Vorteil, dass sie nur mehr wurschtabiler ist aus der Gausserl.  Und vor allem ist Scholesky dann sinnvoll, wenn wir so ein Gleichungssystem fuer verschiedene B loesen wollen.", "start": 364.0, "end": 392.0}, {"text": " Also wenn wir das einmal fuer B gleich eins loesen wollen, wenn wir das einmal fuer B gleich zwei loesen wollen,  dann ist Scholesky gut, dann berechnen wir einmal die Scholesky-Zerlegung  und dann koennen wir in quadratischer Laufzeit das fuer alle B loesen.", "start": 392.0, "end": 403.0}, {"text": " Bei Gauss funktioniert das nicht so einfach, weil wir uns da ja merken muessen, welche Operationen wir durchgefuehrt haben,  also Vertauschen von Zeilen, aufeinander die von Vielfachen von Zeilen und so weiter,  das machen wir auch alles auf der rechten Seite fuer den Vektor B mit, das muessen wir hier nicht machen.", "start": 403.0, "end": 413.0}, {"text": "  Hier berechnen wir nur einmal die Zerlegung, dann koennen wir das direkt fuer mehrere Vektoren halt durchfuehren.  Also gerade in jedem Anwendungsfall waere Scholesky sehr gutstich.  Wolltest du mir das noch mehr Roeschenregel noch besser ist?  Genau, und dann, was ich dir noch nicht kennengelernt habe, das ist die Psaudeinverse.", "start": 413.0, "end": 436.0}, {"text": "  Die Psaudeinverse werden wir uns speziell in Kontext und der Hauptkomponentenanalyse und der SVD anschauen.  Dann moechte ich jetzt noch mal so viel zu sagen, dass wir auch hiermit die der Gleichungssysteme loesen koennen.", "start": 436.0, "end": 455.0}, {"text": " Das heisst, der Log zu dem, was wir hier gemacht haben, hier oben bei den Inversen,  dann haben wir mit der Psaudeinverse geschrieben, das A plus auch unser Gleichungssystem loesen.", "start": 455.0, "end": 472.0}, {"text": " Das heisst, anstatt von der Inverse hier auf der rechten Seite verwenden wir die Psaudeinverse,  das ist A plus, die, und wir schreiben halt A plus, und die Psaudeinverse sind insofern sehr praktisch, dass sie uns immer eine Loesung liefert.  Als egal, ob wir eine Loesung haben, keine Loesung oder endlich viele Loesungen, die Psaudeinverse liefert uns immer direkt eine Loesung.", "start": 472.0, "end": 492.0}, {"text": "  Bei einer unendlichen Loesung ist es die Loesung mit der kleinsten Norm und im Fall von keiner Loesung im Sinne Ausgleichsrechnung.  Es sind halt interessante Eigenschaften, sie sind auch mehrere stabil, naehere Statue, werdet ihr dann halt im naechsten Themenblock lernen.", "start": 492.0, "end": 510.0}, {"text": "  Nochmals hinwahrts, die Scholesky-Talegrum, die funktioniert natuerlich nur, wenn A positiv diegaeniert ist, also eine eindutige Loesung hat.  Wenn unser System wieder eine eindutige Loesung hat, den Spezialfaellen von der positiven Sebenen, die definitiv trotzdem funktionieren kann, da finden wir dann halt am Ende hier trotzdem keine Loesung.", "start": 511.0, "end": 538.0}, {"text": "  Wenn wir mit Gauss, der in Versen oder Scholesky eine eindutige Loesung bestimmen wollen, fuer ein ueberbestimmtes System mit keiner Loesung, dann kommen wir immer wieder auf die normalen Gleichen.  Heisst die ueberbestimmten Systeme, also bei Gauss hat sich gesagt, finden wir jeden Ueberwiderspruch und wenn wir unendliche Loesungen haben, dann koennen wir die analytisch in Loesungen bestimmen.", "start": 539.0, "end": 559.0}, {"text": "  Bei der in Versen sind wir das gar nicht, weil Scholesky haben wir halt am Ende, wie gesagt, auch einen Widerspruch oder wir finden gar nicht erst die Scholesky-Zerlegung.  Heisst, wenn wir ein ueberbestimmtes System loesen wollen, dann brauchen wir die normalen Gleichung.", "start": 560.0, "end": 581.0}, {"text": "  Und auf diese normalen Gleichung, wo das quasi in der A-Stich ist und das hier in der B-Stich, da koennten wir dann aber trotzdem wieder Scholesky im Versen oder Gauss haben.  Das wollen wir jetzt einmal in einem kurzen Beispiel tun.  Da habe ich dafuer dieses Notebook einmal vorbereitet.", "start": 581.0, "end": 597.0}, {"text": "  Zuerst haben wir halt ein paar Imports und dann geht es hier los, wobei einmal hat die 4 Algoribben in ihrer Laufzeit vergleichen wurden.  Hier am Anfang habe ich einmal fuenf Listen, um einfach die gemessenen Zeiten zu protokollieren und diese gemessenen Zeiten werden dann hier spaeter dafuer dargestellt.", "start": 598.0, "end": 620.0}, {"text": "  Da haben wir hier eine Schleife, eine Ende dieser Schleife produzieren oder erzeugen wir immer zufaellig Matritzen unterschiedlicher Groesse.  Wir fangen an mit Endgleich 1, dann werden wir immer 100 Groesse bis 1000, bis also eine Endgleich 1000 haben und wir erzeugen immer eine 2xM-M-Matrix.  Das heisst wir haben mehr Zeilen als Spalten und das wuerde in einem ueberbestimmten System reduzieren.", "start": 621.0, "end": 659.0}, {"text": "  Also im wesentlichen Moment, ich bin noch mal ganz kurz, im wesentlichen, weil es hier ein quadratischen Stock, quadratischen Stock, genau entschwalten.  Und bei Endzeilen, mal x ist gleich b, das ist dann halt ein ueberbestimmtes System mit mehr Zeilen als Spalten, also mit mehr Gleichung als unbekannt.  Und das werden wir also in der Regel gerade mit zufaelligen Werten halt nicht eindeutig loesen.", "start": 659.0, "end": 685.0}, {"text": "  Also du muesst eine Apoximative loesen, bestimmten Durchhaeltst, zum Beispiel vor der Inverse oder normalen, gleich mit einem der ueblichen Algoribben.  Genau und bei diesen verschiedenen grossen Matritzen, speichern wir halt hier in den Listen die Laufzeit, dann werden die Grafisch dargestellt.  Wir fangen an mit der Grauselimination, den WDListen.", "start": 685.0, "end": 714.0}, {"text": "  Wir messen die Startzeit, dann bestimmen wir halt unter A' bzw. B' fuer die Cholesse-Zerlegung und hier uebergeben wir das in wenigen NumPy den jahre Algorisolve.  Und hier an der Stelle gleich ein Hinweis, NumPy hat zu loesen von gleichen Systemen schon sehr viele Obstigierungen mit drin.", "start": 715.0, "end": 730.0}, {"text": "  Am Ende laeuft das hier auf eine L-U-Zerlegung heraus, also das ist quasi Equalent zu dem, was wir mit rausmachen.  Und trotzdem ist das Ganze halt optimiert und dadurch erwaehnen sie schneller, als zum Beispiel eure Implementationen aus der Hausfugabe.  Wenn wir das die Loesung dann stimmt haben, schabt man halt wieder die Zeit und haengen die halt die Liste mit an.", "start": 731.0, "end": 754.0}, {"text": "  Und als Hinweis hier kopiere ich A und B immer wieder um halt zu verhindern, dass fuer alle drei Algoripmen oder vier Algoripmen genau gleich eine Tritzel uebergehen.  Ja, dann loesen wir das Ganze einmal mit Cholesky.  Das heisst, wir bestimmen wieder die Normalingleichung, also sind wir auch mit Cholesky halt.", "start": 755.0, "end": 768.0}, {"text": "  Mit Cholesky koennen wir halt auch nur eine eindeutige Loesung bestimmen, wir machen auch wieder die Normalingleichung.  Und dann zerleben wir ATR in L.  ATR hat ja eben die Eigenschaften, dass immer positiv semi definiert ist und wir den zufaelligen Werten in der Regel auch positiv definieren.", "start": 769.0, "end": 790.0}, {"text": "  Dadurch koennen wir Cholesky anwenden und koennen danach hier mit Cholesky solf rueckwaerts einsetzen und eine Loesung bestimmen.  Weil die beiden von so um wurden in dem Fall hier von Cypher.  Dann haben wir die dritte Variante mit der Imverse.  Das heisst, wir berechnen wieder eine Einleichung, invertieren ATR und multiplizieren dann mit ATB.", "start": 790.0, "end": 821.0}, {"text": "  Also das wird quasi umgeformt, so x, ATR, invertiert, neu ATB.  Genau, hier stoppen wir wieder die Zeit, haengen das Ganze an, kopieren das Ganze und dann haben wir einmal die Zeugenverse,  wo wir die Zeugenverse berechnen, das fuer euch jetzt erstmal nicht interessant, das werde ihr im naechsten Themenblock erkennen.", "start": 827.0, "end": 852.0}, {"text": "  Dann haben wir hier die Zeugenverse A plus und multiplizieren B drauf und haengen dann halt unter x, 3.  Und auch hier haengen wir wieder die Zeit an.  Zusammengefasst, wir haben die vier Algorippern, was halt auffaellt hier bei den ersten Reihen, die wir halt auf die Normalen-Gleichung angewiesen, die ich gesagt habe, um halt die Approximative-Loesung fuer das ueberbestimmte System zu finden.", "start": 853.0, "end": 876.0}, {"text": "  Und die Zeugenverse koennen wir halt direkt verwenden, weil sie in dem Fall auch die Approximative-Loesung im Sinne der Normalen-Gleichung.  Genau, das koennen wir dann einmal ausfuehren und dann sehen wir jetzt hier ein relativ interessantes Ergebnis.", "start": 877.0, "end": 895.0}, {"text": "  Also erstmal die Zeugenverse ist jetzt hier relativ langsam, bis die daran, dass wir hier halt einen sehr hohen Overhead haben, um die Erstbahn zu berechnen.  Das ist also insgesamt eher relativ ineffizient, aber auch nur mirrohstabil.  Und wenn man die SVD-Zerlegung, also ich denke, dass der gespringende Punkt ist, dass man hier erstmal die SVD, also die Zerlegung berechnen muss.", "start": 896.0, "end": 913.0}, {"text": "  Das wird halt hier relativ lange dauern.  Wenn man die Zerlegung aber sowieso schon berechnet hat, dann ist halt die Zeugenverse wieder sehr effizient, weil man nun dieses Matrixrektor-Produkt hat.  Und da hat man halt dann wieder eine Lauchzeit nur von nq fuer das Matrixrektor-Produkt.", "start": 914.0, "end": 939.0}, {"text": "  Das ist also aehnlich wie Scholesky, da kann ich Scholesky, da kann ich halt auch einmal die Zerlegung berechnen und dann ist dann fuer viele Bs loesen.  Und dann hat dann diesen den Aufwand von Fowers und Rupfer zu einsetzen.  Hier ist es aehnlich, hier brauche ich einmal die Zerlegung, sondern dann habe ich jetzt nur noch den Aufwand von Matrixrektor-Produkt.", "start": 940.0, "end": 953.0}, {"text": "  Genau, also das ist relativ ineffizient.  Dann sehen wir jetzt hier, dass die Inverse und Scholemme, die Inverse und Nampolinaere eingebrarst voll relativ gleich auf sind.  Die Inverse zu berechnen hat eine Lauchzeit von n hoch 3 und Gauss in der Regel auch.", "start": 954.0, "end": 978.0}, {"text": "  Also die Inverse kann ich ja quasi durch Gauss zum Beispiel berechnen, indem ich halt auf der linken Seite die Matrix schreibe, auf der rechten Seite die Einheitsmatrix.  Durch Gauss und dann habe ich links die Einheitsmatrix und rechts die Inverse.  Das ist eine Rung aus Selina und Algebra.  Und deswegen sind die halt jetzt endlich relativ aehnlich in der Lauchzeit.", "start": 978.0, "end": 1001.0}, {"text": "  Weil das natuerlich jetzt nur hier approximative Berechnung der Lauchzeit sind, also Versuche empirisch,  natuerlich auch hier von meiner Maschine mit ab auf Daecher ausgefuehrt hat und so weiter.  Und das ist auch bei jedem Versuch jetzt endlich mit dem Unterschied.", "start": 1002.0, "end": 1020.0}, {"text": " Und zu guter Zeit haben wir die Scholesky-Zerlegung, da hatte ich versprochen, okay, gut,  da hat halt eine Lauchzeit auch von n hoch 3, also cubisch, aber sehr viel geringeren konstanten Faktor.  Und das sehen wir jetzt hier, die haben wir doch ziemlich zuegig berechnen.", "start": 1021.0, "end": 1039.0}, {"text": " Und da jetzt endlich trotzdem alle Algorithmen wenigstens eine cubische Lauchzeit haben,  weil da halt hier wirklich den Gewicht, dass er halt bei Scholesky die den geringeren konstanten Faktor haben,  als bei zum Beispiel der Gauss-Elimenzen.  Und dann sehen wir halt, dass ich hier mit den geringsten, die geringste Lauchzeit haben empirisch gesehen.", "start": 1040.0, "end": 1057.0}, {"text": "  So, jetzt als naechstes kommen wir zu der Matrix-Kondition.  Und dafuer wuerde ich jetzt erst noch mal ganz kurz wiewechseln, hier auch meine Slides quasi.  Und zwar habe ich an mir dieses Beispiel hier vorbereitet.  Und zwar haben wir einmal die Matrix, die Einheitsmatrix und dann diese Matrix.  Beide Matristen, wenn wir uns in sehr aehnlicher Form gleich dann nochmal in den Notebook anschauen.", "start": 1060.0, "end": 1087.0}, {"text": "  Und jetzt hier sehen wir, okay, wenn wir dieses Leichtungssystem loesen wollen mit der Einheitsmatrix, dann geht das sehr, sehr einfach.  Also wir wissen hier sofort, dass x ist gleich 2, 1.  Das koennen wir sehr einfach loesen und diese Matristen sind insgesamt eine sehr einfache oder sehr angenehme Kondition,  weil die beiden Spaltenwettoren orthogonal sind und beide diese Belaengel haben.", "start": 1087.0, "end": 1114.0}, {"text": "  Und dadurch koennen halt ohne weiteres unkompliziert dieses Leichtungssystem loesen.  Genau so gut wuerde das funktionieren fuer eine Rotationsmatrix.  Die Rotationsmatristen sind jetzt endlich auch orthogonal.  Heisst auch hier koennen wir unter Gleichungssystemen einfach durch zum Beispiel transponieren von dieser Rotationsmatrix-Belechnung.", "start": 1114.0, "end": 1136.0}, {"text": "  Und insgesamt hat die Matrix damit auch eine sehr angenehme Kondition.  Das merken wir uns einfach mal quasi.  Originalmatritzen, immer was Feines sind, weil sie halt eine gute Kondition haben.  So und jetzt haben wir hier ein anderes Beispiel.  Und das ist quasi genau das Gegenteil von dem hier.", "start": 1136.0, "end": 1173.0}, {"text": "  Hier sind die beiden Spaltenwettoren zwar sehr aehnlich lang, aber sie stehen ueberhaupt nicht autogonal fernan.  Also dann sie zeigen eine sehr, sehr aehnliche Richtung.  Und wenn wir das loesen wollen, dieses Gleichungssystem, dann muessen wir jetzt endlich haben wir wieder eine Linearkombination aus den beiden Spalten.  Und die Faktoren in X dann da drin stehen ein unheimlich gross.", "start": 1174.0, "end": 1199.0}, {"text": "  Also um das hier zu loesen, die sehr aehnlichen Spaltenwettoren brauchen wir unheimlich grosse Werte in X.  Hier stehen dann insgesamt sehr grosse Werte drin.", "start": 1199.0, "end": 1226.0}, {"text": " Und Problem dahinter ist, wenn wir in X quasi hier mit drauf multiplizieren oder diese grossen Werte haben und diese Linearkombination im Bereich nehmen,  haben wir grosse Werte und wenn wir das hier drauf multiplizieren, haben wir halt sehr grosse Werte und dann wieder die grossen Werte so plagieren oder addieren und dann wieder auf die kleinen Werte hier zu kommen.", "start": 1226.0, "end": 1231.0}, {"text": "  Dann haben wir halt diese typischen numerischen Probleme, die wir aus den ersten grossen Themenbock kennen.  Beispiel koennen das jetzt zur Ausloeschen kommen und so weiter und so fort.  Und ich wuerde untergeben, dass das Ganze dann sehr, sehr ungenau.  Und wir koennen halt mit dieser Mathis dann sehr schlecht rechnen.", "start": 1232.0, "end": 1248.0}, {"text": "  Das werden wir uns gleich nochmal anschauen, grafisch, was wir ausfluegeln, das hat.  So, nochmal formal aus dem Script.  In dem Script steht nochmal einiges zu der Mathis Kondition mit drin.  Das moechte ich nicht wiederholen, das koennt euch in Google durchlesen.  Jetzt endlich wird diese Definition hierher geleitet.", "start": 1249.0, "end": 1272.0}, {"text": "  Die Kondition von Mathics A ist gegeben, es ist ein Maximum und ein Minimum und zwar berechnen wir das Maximum.  Wir nehmen ein Vektor, ein Einhaltsvektor.  Das ist, nehmen wir den Einhaltsvektor so, dass die Norm von dem abgebildeten Einhaltsvektor maximiert wird.  Also wir suchen den Einhaltsvektor, der nach Abbildung von A moeglichst lang ist.", "start": 1273.0, "end": 1301.0}, {"text": "  Das dividieren wir durch die Norm, sodass die Norm bei entsprechender Wahl von Y moeglichst klein wird.  Das ist hier relativ absakt und um schwierige Worte zu fassen, das wird auch mit der grafischen Veranstaltung klar.  Da wechseln wir ganz kurz rueber, ich lasse das hier trotzdem mal nebenbei offen, dann wird man das naemlich gleich gut sehen.", "start": 1302.0, "end": 1322.0}, {"text": "  Hier zunaechst haben wir eine Plotting Funktion, also eine Funktion, die einfach die Vektoren plottet, V wird uebergeben.  Und dann plotten wir in unterschiedlichen Farben hier spaeter Vektoren.  Wir stellen jeden einzelnen Vektor einmal da, also plotten den mit den Plot.crever.  Und dann legen wir halt die entsprechenden Dimensionen von dem Plot fest.  Das ist denke ich nicht interessant.", "start": 1323.0, "end": 1349.0}, {"text": "  Interessant ist dann hier die Berechnung von der Mathex Kondition.  Da verwenden wir auch einmal die SVT.  Warum das ganze die Mathex Kondition auch so berechnet werden kann, das koennte ihr hoffentlich nach dem naechsten grossen Themenblock wieder nachvollziehen.", "start": 1350.0, "end": 1370.0}, {"text": "  Ein spannender oder einfacher Gehalten ist, dass wir hier die Kondition auf dieser Mathex M mit uebergeben bekommen, auch mit Nam-Py berechnen koennen.  Nam-Py spruegt uns also quasi genau das Ergebnis hier von dieser Definition.", "start": 1371.0, "end": 1394.0}, {"text": "  Was wir empfindlich machen hier ist die Kondition einmal pruenden und dann erzeugen wir Vektoren und bilden diese Vektoren ab durch die Mathex M und stellen diese Vektoren in Grafischdahl durch diese Vertenngfanschen.  Und zwar haben wir hier einmal alle Winke von 0 bis 2P in Abstand von 100.  Also wir erzeugen uns hier einen 100, wir haben 100 verschiedene Winke zwischen 0 und 2P.", "start": 1394.0, "end": 1426.0}, {"text": "  Wir erzeugen uns mit dieser Funktion, dann etere wir darueber und berechnen mit den Cosinos und Insinos an Einheitsvektor, das heisst wir erzeugen quasi alle Einheitsvektoren mit genau laenger Eis.  Da sind ja quasi genau die Vektoren, die hier fuer dieses Maximum und dieses Minimum in Frage kommen.", "start": 1427.0, "end": 1438.0}, {"text": "  Und wenn wir all diese Vektoren erzeugt haben, dann bilden wir die halt einmal durch die Mathex M ab und schaemen die da.  Dass hier C transponiert steht, liegt da, dass wir die hier als 10 mal 2 Mathex erzeugen, aber quasi 2 mal C Mathex, also die ganzen Spaltenvetoren hier rauf multiplizieren wollen.", "start": 1438.0, "end": 1461.0}, {"text": "  Also als ihr das nachvollziehen wollt, das liegt daran, dass wir hier die quasi als in den einzelnen Zeilen erzeugen.  Also hier ist eine Mathex mit diesen Vektoren hier in den Zeilen.  Und jetzt endlich diese beiden Funktionen werden dann einfach hier in dem Beispiel einmal ausgefuehrt.", "start": 1461.0, "end": 1482.0}, {"text": "  Das heisst hier habe ich vier Mathexen definiert, einmal 2, also die Einheitsmathex, die zwei Einheitsmathex, dann die Mathex 2 0 0 1.  Im Wesentlichen stehen hier die Vektoren auch orthogonal aufeinander, aber mit dem Unterschied ist ein Vektor wesentlich laenger als das.  Das waere die Standardbasis.  Wir haben aber hier 2 1, also ein Vektor ist wenig laenger als andere.", "start": 1482.0, "end": 1502.0}, {"text": "  Und das wird auch gleich Auswirkungen haben wir gesehen.  Und dann haben wir einmal eine Mathex, wo die Vektoren halt sehr aehnlich sind.  Und dann haben wir einmal Mathex, wo die Vektoren wirklich fast quasi die gleichen sind, momentan sehr fein unterschiedlich.  Und dann diese Mathexen.", "start": 1503.0, "end": 1525.0}, {"text": "  Und dann stellen wir die hier einmal da, das ist quasi einfach die Einheitsmathex abgebildet und jeder Vektor mal der Einheitsmathex ist wieder der Vektor selbst.  Hier sehen wir eigentlich genau die Vektoren, die wir hier erzeugt haben.  Und die Condition Number, also die Kondition dieser Mathex ist genau wieder 1.  Hier stehen zwei Werte.", "start": 1525.0, "end": 1546.0}, {"text": "  Einer ist ja der Berechnete durch den Nampel und der andere ist der Berechnete, der durch die SVD hier berechnet wird, aber immer recht identisch.  Und hier ist relativ offensichtlich, dass das eins sein muss.  Fuer jeder Vektor hier, jeder Vektor Y, durch die Einheitsmathex, wird wieder auf den gleichen Vektor Y abgebildet.", "start": 1547.0, "end": 1567.0}, {"text": "  Das heisst, die Laenge veraendert sich nicht und das Maximum von A bei Y ist immer genau 1, genauso das Minimum.  In den Entstrecken haben wir hier 1 durch 1.  So, das haben wir dieses Beispiel hier und jetzt sehe ich, dass hier eine Sache etwas unguenstig ist.  Zwei.  Passt das mal jetzt in Live an.  Okay, willkommen zurueck. Ich habe das Video kurz angehalten und das beriegiert hier in der Stelle.", "start": 1568.0, "end": 1628.0}, {"text": "  Genau, was man jetzt hier schoen sieht, hier waren vorher die Dimensionen jetzt hier umpassen.  Was man jetzt hier schoen sieht, ist, dass jetzt Vektoren entlang der X-Achse auf 2 gestreckt werden und Vektoren entlang der Y-Achse quasi genau gleich bleiben.  Also, der Vektor hier ist immer noch genau eins lang.", "start": 1629.0, "end": 1654.0}, {"text": "  Die hat daran, dass Vektoren, die quasi nur einen X-Antrag haben, mit zwei Skalieren und Vektoren, die nur einen Luebsklang-Antrag haben, quasi mit ein Skalieren, also gleich behalten.  Und wenn wir das jetzt nochmal beziehen auf die Definitionen, dann sehen wir halt, okay, genau dieser Vektor 1, 0 wird halt auch 2, 0 gestreckt.", "start": 1655.0, "end": 1667.0}, {"text": "  Und das ist genau das Maximum, das heisst hier oben haben wir genau nur zwei stehen.  Und das Minimum ist quasi dieser Vektor 0, der wieder auf 0, 1 abgebildet wird.  Das heisst, die minimale Laenge hier ist genau eins und wir haben zwei geteilt durch eins, dementsprechend gibt es sich die Konditionsnummer 2, 0.  Also, die Kondition hier ist schon mal ein bisschen schlechter.", "start": 1668.0, "end": 1699.0}, {"text": "  Oder in anderen Worten, um einen bestimmten Vektor B, die da zu kombinieren, braeuchten wir hier ein X, die groessere Werte, als wenn wir quasi mit der Einheitsmatrix arbeiten wuerden.  So, und dann haben wir ein weiteres Beispiel, naemlich das, wo hier die beiden Vektoren, die spalten Vektoren sehr, sehr aehnlich sind.", "start": 1702.0, "end": 1717.0}, {"text": "  Also, hier der Fall, jetzt sehen wir, okay, hier werden einige Vektoren schon sehr lang abgebildet und einige sehr, sehr kurz.  Das heisst, die Konditionsnummer hier wird entsprechend relativ gross, weil wir hier sehr, sehr grosse Werte haben fuer Vektoren, die sehr stark skaliert werden und aber auch Vektoren haben die sehr stark gestaucht werden.", "start": 1718.0, "end": 1745.0}, {"text": "  Also, in anderen Worten wieder, um mit dieser Matrix hier bestimmte Gleichungssysteme zu loesen, braeuchten wir wiederum groessere Werte und X, nicht groessere numerische Probleme.  Das Ganze kann man jetzt natuerlich noch mal ins Extrem treiben hier mit dieser Matrix.", "start": 1746.0, "end": 1765.0}, {"text": "  Die ist grundsaetzlich noch nicht singulaer, das heisst, grundsaetzlich sind die beiden Spalten-Vektoren noch nicht unabhaengig, aber wir sehen halt, dass bestimmte Vektoren mit der sehr stark gesteigt und andere Vektoren mit der sehr stark gestaucht werden, wenn wir insgesamt dann eine sehr, sehr stelle Kondition haben.", "start": 1766.0, "end": 1775.0}, {"text": "  Das heisst, wenn wir mit dieser Matrix rechnen und vier rechnen, also mehrere Berechnungen hintereinander durchfuehren, dann koennen wir erwarten, dass sie hier einen sehr, sehr grossen numerischen Fehler bekommen.", "start": 1775.0, "end": 1790.0}, {"text": "  Okay, das war's so weit. Zu den Angleichungssystemen, dann sollte es in den naechsten Wochen weitergehen mit der Hauptkomponentenanalyse der Singulaerwetterlegung. Dann werdet ihr auch die Hiebs-Heulen-Werze kennenlernen.", "start": null, "end": 1790.0}]}]