[{"lecture": "25797_42_course_video", "Timestamps": [{"text": "  Dann geht es heute ein letztes Mal um Optimierung und auch ein letztes Mal um ueberhaupt irgendein  neues Thema oder neue Themen oder neuen Stoff.", "start": 0.0, "end": 42.04}, {"text": "Und naechste Woche gibt es dann nochmal Wiederholung  zum Stoff, insbesondere als Vorbereitung auf die Klausur, also Diskussion von typischen Klausurfragen  und so weiter naechste Woche am Freitag Nachmittag hier.", "start": 42.04, "end": 58.400000000000006}, {"text": "Und heute genau die letzten drei Unterkapitel  nach wie vor machen wir Optimierung und im Grunde koennten wir das, was ich heute erzaehlen will,  in jeder Reihenfolge machen.", "start": 58.4, "end": 70.0}, {"text": "Aber ich habe gedacht, ich mache das, was irgendwie am nettesten ist,  vielleicht zuerst, ja, nicht nicht zuletzt, sondern zuerst, weil mein Gefuehl ist, diese Idee das Beste  kommt zum Schluss, ist Quatsch. Zum Schluss hat man eigentlich keine Lust mehr und dann kriegt man  eh nix mit und deswegen machen wir das das nette vielleicht besser am Anfang.", "start": 70.0, "end": 86.4}, {"text": "Also der Kontext  ist nach wie vor unser Werkzeug um Funktionen zu minimieren, ist Gradientenabstieg und damit  beschaeftigen wir uns heute noch einen Moment.", "start": 86.4, "end": 120.84}, {"text": "Gradientenabstieg und was war das? Wir versuchen  iterativ den Funktionswert typischerweise kleiner zu machen und die Idee war, die Idee war folgende,  wir wissen, der Gradient der Funktion zeigt in Richtung des steilsten Anstiegs, also ist es unter  Umstaenden eine plausibli Idee entgegengesetzt dieser Richtung zu laufen, also in Richtung, in die Richtung,", "start": 120.84, "end": 135.2}, {"text": " in der die Funktion am meisten abnimmt und das Einzige ist, wir wissen nicht so recht, wie weit wir  da laufen sollen. Ja und ich hatte letztes Mal schon gesagt, da gibt es dann verschiedene Ideen, was man  hier machen kann.", "start": 135.2, "end": 152.08}, {"text": "Eine Idee ist zu sagen, ich suche irgendein Delta, das moeglichst nicht zu klein ist  und den Funktionswert kleiner macht und das nehme ich dann und es ist okay und eine andere Strategie  koennte sein, eine andere Strategie koennte sein, Delta so zu bestimmen, dass diese Funktion moeglichst  klein wird.", "start": 152.08, "end": 183.52}, {"text": "Ja also ich koennte fragen, ich koennte sagen, ich versuche folgendes Problem zu loesen,  ich suche den kleinsten Funktionswert ueber alle Delta, sagen wir mal groesser Null von F, von in dem  Fall Xi minus Delta Gradient von F an der Stelle Xi. Ja und das ist ein eindimensionales Optimierungsproblem.", "start": 183.52, "end": 208.04000000000002}, {"text": " Ja es gibt hier nur eine Veraenderliche und das ist Delta und von daher kann man jetzt die Frage  stellen, wie loest man ein Minimierungsproblem, wenn man nur eine Veraenderliche hat, bisher hatten wir  in diesen Problemen immer viele Veraenderliche und typischerweise in vielen Anwendungen kann ich  nicht einfach eine Ableitung ausrechnen von diesem Ausdruck.", "start": 208.04000000000002, "end": 236.64000000000001}, {"text": "Ja und das liegt dann, das haengt davon ab,  wie die Funktion F beschaffen ist. Koennte ich sagen, na klar, wenn ich partielle Ableitung von F nach Delta  machen kann, dann geht das irgendwie, aber im Allgemeinen ist es schwierig und man versucht  dieses Problem zu loesen, ohne irgendwelche Ableitungen zu betrachten. Das heisst man moechte gerne  minimieren in einer veraenderlichen ohne Ableitung. Okay.", "start": 236.64000000000001, "end": 265.24}, {"text": "  Ja und jetzt versuchen wir was zu finden und das ist im Grunde die Analogie von Bisektion.  So viel Tafel brauchen wir heute nicht.", "start": 265.56, "end": 291.08}, {"text": "Bei Bisektion haben wir versucht die Nullstelle einer  Funktion zu finden und wir wussten nichts, ja wir hatten keine Ableitung oder so und wir haben  einfach gesagt, wir teilen das Intervall immer in der Mitte und wir mussten anfangen mit einem  Funktionswert der kleiner als Null war, einem Funktionswert der groesser als Null war und dann", "start": 291.12, "end": 295.03999999999996}, {"text": " haben wir gesagt, okay jetzt teilen wir das Intervall immer in der Mitte und schauen halt,  ob der Wert groesser oder kleiner Null ist und dann nehmen wir eben die linke oder rechte Seite  von diesem geteilten Intervall und dann kann man die Nullstelle so einkaesteln und jetzt ist die  Frage, gibt es ein aehnliches Verfahren, um ein Minimum zu finden? Ja und was wir da brauchen,", "start": 295.03999999999996, "end": 321.59999999999997}, {"text": " ist ein bisschen mehr, zwei Werte reichen jetzt nicht, sondern jetzt brauchen wir drei und da sagen  wir mal, wir haben eine linke Intervallgrenze und eine rechte Intervallgrenze und dann haben wir  irgendeinen Wert dazwischen, ich mache mal den links ein bisschen weiter nach links  und dann haben wir irgendwo einen Wert dazwischen, sagen wir mal hier, den nennen wir mal x Stern", "start": 321.59999999999997, "end": 369.32}, {"text": " und jetzt haben wir drei Werte und wir muessen die beschaffen sein, die muessen so beschaffen sein,  ja also was haben wir jetzt vom Problem, wir haben jetzt also minimieren in einer Variablen  und wir fangen an mit drei Werten xl, x Stern, xr, also wir haben xl, x Stern, xr, die Kommas  brauche ich nicht, kann ich gleich sagen, die sind so geschachtelt, ja xl ist kleiner als x Stern und", "start": 369.32, "end": 422.48}, {"text": " x Stern ist kleiner als xr, die sind alle aus r und wir haetten gerne fuer diese Werte, dass dieser  Wert hier kleiner ist als der und als der, ja so das f von xl kleiner als f von x Stern und f von xr  kleiner als f von x Stern, logisch, okay das sind die Startbedingungen und genau wie bei Bisektion,", "start": 422.48, "end": 458.48}, {"text": " wo ich gesagt hatte, dass es manchmal gar nicht so einfachen Wert zu finden, also zwei Werte zu finden,  von denen einer kleiner als 0 ist und der andere groesser als 0, kann es sein, dass es gar nicht so  einfach ist drei Werte zu finden, so dass der mittlere kleiner ist als die aeusseren beiden,  aber wenn ich das gefunden habe, dann kann ich weiter machen und das geht jetzt so, wir suchen uns", "start": 458.48, "end": 486.32}, {"text": " hier irgendein anderen Wert, ich habe diese beiden Intervalle jetzt ja ungleich geteilt, ja und das war  kein Zufall, sondern wir werden herausfinden, dass das gut ist so und ich habe es jetzt so gemacht,  dass das linke kleiner ist als das rechte und das kann man auch andersrum machen, aber das machen", "start": 486.32, "end": 505.32}, {"text": " wir jetzt mal fuer den Moment so und und jetzt ist die Idee, dann suche ich mir einen Wert in dem  groesseren der beiden Intervalle, ja hier ungefaehr, den er nicht jetzt mal x Strich und jetzt kommt es  darauf an, was ich dafuer ein Funktionswert finde, ja und es gibt zwei Moeglichkeiten, die eine ist,", "start": 505.32, "end": 529.32}, {"text": " sagen wir mal hier und die andere ist, sagen wir mal hier, ja und die unterscheiden sich eben dahingehend,  ob der neue Funktionswert in ist x Strich kleiner ist als x Stern oder ob der groesser ist als x Stern,  ja das ist die die Unterscheidung, habe ich einen neuen Wert gefunden, der kleiner ist als alle anderen,", "start": 529.32, "end": 550.5200000000001}, {"text": " die ich bisher gesehen habe, ja oder nein, ja das heisst die die Unterscheidung ist jetzt,  wir waehlen also x Strich und wir fragen ist f von x Strich kleiner als f von x Stern,  was machen wir, wenn das so ist, wenn das so ist, dann haben wir jetzt ein neues Trippel,  das ist der beste im Moment, der rechts davon ist groesser, denn der war ja schon groesser als x Stern", "start": 550.5200000000001, "end": 587.92}, {"text": " und der links davon, das ist jetzt x Stern, ist auch groesser, das war ja gerade unsere Bedingung,  das heisst wenn das so ist, dann machen wir die linke Schranke, wird das was vorher x Stern hiess  und x Stern wird das was wir jetzt gerade x Strich genannt haben, ja und wir haben ja  so, ja aber ich hier ist richtig, ja okay, okay danke, also das ist die Idee, wir haben einen", "start": 588.8399999999999, "end": 643.24}, {"text": " neuen besten Punkt und jetzt haben wir eben hier die Intervallgrenze, hier die Intervallgrenze x R  bleibt x R, das hier wird x L, das hier wird x Stern, ja und der andere Fall ist der ist groesser  ja und jetzt haben wir hier unser Intervall, der da ist dann kleiner als der, der war sowieso  schon kleiner als der und jetzt haben wir hier drei Punkte, ja und die Beobachtung ist,", "start": 643.24, "end": 673.4}, {"text": " das Intervall wurde kleiner, ja also wenn umgekehrt x Strich groesser oder gleich als f x Stern ist,  den Gleichfall koennen wir auf irgendeine Seite schieben, das ist egal, dann machen wir folgendes,  x L bleibt wie es ist, x Stern bleibt wie es ist und x R wird das was, was wir eben noch x Strich genannt", "start": 673.4, "end": 704.0}, {"text": " haben und fertig ist der Algorithmus, ja so und das einzige was jetzt bleibt als Fagesstellung ist,  an welcher Stelle in diesem Intervall bestimmen wir unsere neuen Werte und wir hatten, also bei der  Bisektion haben wir immer das Gleiche gemacht, wir sind immer in die Mitte gegangen, das war  irgendwie der eine Teil der Strategie und wir sind immer in die Mitte gegangen, warum, weil wir gesagt", "start": 704.0, "end": 736.88}, {"text": " haben, dann ist das Worst Case Behavior immer das Gleiche, ja also ich mach das Intervall,  ich muss, egal was passiert, ob ich nun ins linke oder rechte Intervall komme, ich habe immer,  ich habe es immer halbiert, ja waehrend wenn ich es nicht in der Mitte teilen wuerde, dann kaeme es  drauf an was rauskommt und dann haette ich entweder was gewonnen, weil ich, weil ich weniger als die", "start": 736.88, "end": 758.64}, {"text": " Haelfte jetzt habe im Intervall, im guenstigen Fall, aber im unguenstigen Fall haette ich eben nicht mal  halbiert, ja und dann habe ich gesagt, die Mitte ist irgendwie eine gute Idee, dann ist egal was  passiert, egal was ich von Funktionswert bestimme, ich habe immer mindestens halbiert oder genau  halbiert, so und so eine aehnliche Strategie wollen wir jetzt hier auch machen, wir wollen sagen,", "start": 759.24, "end": 775.48}, {"text": " erstens machen wir immer das Gleiche und zweitens haetten wir gerne das Worst Case Behavior optimiert  und deswegen nennen wir mal diesen Abstand hier L, ja den von XL bis X Stern und diesen Abstand R  ja und dann nennen wir vielleicht noch, ich koennte es jetzt so machen wie im Skript, ich koennte es auch", "start": 776.08, "end": 809.48}, {"text": " einfach machen wie es mir passt ja und gucken ob das richtige rauskommt am Ende, versuchen wir mal  zweites, ja ist ja heute das letzte Mal, da kann man, kann man sowas riskieren, diesen Abstand hier  den nenne ich mal S, so was wollen wir jetzt, wir wollen immer auf die gleiche Art und Weise teilen,  das heisst wir wollen gerne das L durch R, ja das kleinere Intervall durch das groessere", "start": 809.48, "end": 836.6800000000001}, {"text": " die kleinere Teil durch den groesseren Teil, das gleiche ist wie das was ich dann hier mache,  ja das heisst es soll sein S geteilt durch und das was hier uebrig bleibt ist dann R minus S,  ja das ist die Idee einfach immer das gleiche Teilverhaeltnis zu verwenden in jedem Schritt  und jetzt kommt diese Worst Case Idee, ja da waere die Strategie jetzt zu sagen", "start": 836.6800000000001, "end": 869.28}, {"text": " egal was passiert ich bekomme immer irgendwie die gleiche Intervallbreite, ja halbieren klappt  hier nicht mit den drei Werten aber also was habe ich in einem Fall, im einen Fall wenn der hier  kleiner ist als der dann bekomme ich als neue Intervallbreite von X Stern bis X R, ja also R  und das soll jetzt das gleiche sein wie der andere Fall, wenn der hier groesser ist, dann bekomme ich", "start": 869.28, "end": 906.52}, {"text": " das Intervall und das ist L plus S, ja das sind also die Bedingungen und jetzt kann ich mal einsetzen  versuchen mal S loszuwerden, ja also S ist R minus L und dann haben wir hier R minus L geteilt durch  R minus L oder auch R minus L durch L und okay das kann ich jetzt nochmal umfragen wenn ich moechte  ich kann es auch gleich sehen vom drauf schauen das heisst hier das Verhaeltnis von L und R soll", "start": 906.52, "end": 970.5200000000001}, {"text": " sein der goldene Schnitt, ja also wir wer es nicht glaubt formt es noch ein bisschen um, ja das bedeutet  hier das ist der der goldene Schnitt, ja im uebrigen die Zahl von der man mit am einfachsten zeigen kann  dass sie irrational ist, ja wenn man das hier umgekehrt schreibt nur um das nochmal so am  Rande zu erzaehlen, wenn man das hier umgekehrt schreibt R durch L ist gleich L durch R minus L,", "start": 970.52, "end": 1015.6}, {"text": " ja das ist sozusagen ein einzahliger Beweis dass diese Zahl die ich hier definiert habe irrational  ist, ja wie geht der Beweis ich vergesse mal die rechte Seite und kuerze R und L so lange also wenn das ein  Verhaeltnis von ganzen Zahlen ist dann kann ich jetzt R und L als ganze Zahlen so klein machen wie es eben nur", "start": 1015.6, "end": 1045.4}, {"text": " geht ja ich kuerze so lange bis ich nicht mehr kuerzen kann es sind jetzt also die zwei kleinsten  positiven ganz Zahlen die diese Zahl ausdruecken als Bruch und jetzt gucke ich auf die rechte Seite  und im Nenner steht eine die kleiner ist und das ist der Widerspruch Beweis ist fertig, ja waere das  eine rationale Zahl so gaebe es zwei ganz Zahlen die kann ich kuerzen bis sie nicht mehr kleiner gehen", "start": 1045.4, "end": 1074.72}, {"text": " und hier auf der rechten Seite steht der Widerspruch ja denn dieser Nenner ist kleiner als dieser Nenner  ja fertig existenz der irrationalen zahlen nachgewiesen so deswegen also das ist das Verfahren ja man  man holt sich diese triple und das beste worst case verhalten bekommt man wenn man x Stern im", "start": 1074.72, "end": 1103.72}, {"text": " goldenen Schnitt waehlt ja deswegen heisst das ganze auch im Endeffekt dann hier die goldene Schnittsuche  das ist also ein vernuenftiges Verfahren analog zur bisektion mit dem ich fuer den gradienten  Abstieg dieses delta ausrechnen koennte algorithmisch ich brauche am Anfang Werte typischerweise macht  man beim gradienten Abstieg xl gleich null gar keinen Schritt und dann braucht man irgendeine", "start": 1111.72, "end": 1140.8000000000002}, {"text": " rechte schranke bei der es irgendwie wieder schlechter wird und dann braucht man irgendeinen  wert x Stern also man sucht dann so lange xr bis diese stelle x Stern tatsaechlich kleiner ist als xr  und dann kann man dieses Verfahren starten und dann gibt es analog zu wir hatten dann irgendwie  sekanten Verfahren wir legen da eine gerade rein und gucken wo die schneidet und das gibt es hier", "start": 1140.8, "end": 1169.48}, {"text": " alles auch moechte ich jetzt nicht angucken dann legt man natuerlich wenn man minimieren moechte  keine gerade da rein sondern im parabel und dann kann man sich das minimum von der parabel anschauen  und anschauen ob das besser wird und wenn ja nimmt man das und wenn nein macht man wieder  goldene Schnittsuche und so weiter ok ja damit hat man ein werkzeug um fuer gradienten Abstieg", "start": 1169.52, "end": 1192.72}, {"text": " das minimum auszurechnen wenn man gerne moechte ja eine garantie dass es so schneller geht gibt  einem niemand ja also dieses delta so auszurechnen dass das minimal wird kann gut sein muss aber  nicht ja es kann genauso gut sein irgendeinen Schritt zu machen und dann sich einen neuen  gradienten auszurechnen und dann entlang der neuen richtung zu gehen es ist nicht klar im", "start": 1192.72, "end": 1219.28}, {"text": " vorhinein was besser ist insbesondere und damit sind wir beim naechsten beim zweiten von drei  letzten themen insbesondere koennen wir uns mal ueberlegen wann das eigentlich gut ist dieses  delta richtig auszurechnen und und und wann vielleicht auch nicht ja und was man machen kann  also dazu machen wir jetzt folgendes", "start": 1219.28, "end": 1254.1200000000001}, {"text": " wir betrachten mal eine spezielle funktion wie habe ich sie hingeschrieben so wir betrachten  mal eine funktion f von x c minus x transponiert b plus ein halb x transponiert ax so habe ich  sie glaube ich ja das ist eine funktion und davon suchen wir jetzt mal das minimum wer moechte  kann das interpretieren als die tailorentwicklung von einer funktion bis zum quadratischen term", "start": 1254.1200000000001, "end": 1291.84}, {"text": " und warum steht hier ein minus letztlich ist es egal das kann ich ja immer in diese konstante  stecken ja und warum steht hier ein halb das das ist nur das ist nur dafuer da jetzt rechnen wir  naemlich den gradient aus der gradient von dieser funktion an der stelle x ja ist was  wer wer wagt es den gradient zu nennen", "start": 1291.84, "end": 1323.3999999999999}, {"text": " ja was passiert wenn wir die funktion f von x nach x ableiten wir fangen an mit dem teil c  ja was ist die ableitung von c nach x richtig null ja es bleibt nichts uebrig wenn wir die  konstante nach x ableiten was bleibt uebrig wenn wir x transponiert mal b nach x ableiten  bitte minus b ja besser als x genau minus b so um was bleibt uebrig wenn wir x ta x ableiten wer", "start": 1323.4, "end": 1369.04}, {"text": " wer moechte kann es sich so vorstellen wie a mal x quadrat mal ein halb guenstigerweise und es bleibt  stehen a mal x das ist der gradient und und das erklaert so ein bisschen die vorzeichenwahl wir  haben jetzt also als notwendige bedingung fuer das minimum die bedingung ax gleich b ja ok das  nutzen wir natuerlich nicht aus ja wir argumentieren hier jetzt auch sowas wie das ist eigentlich sowas", "start": 1369.04, "end": 1400.48}, {"text": " wie eine hesse matrix hier dieses a und die hesse matrix ist furchtbar schwierig auszurechnen in  der praxis wir kennen sie ueberhaupt nicht die hesse matrix es geht jetzt nur erst mal darum  ein bisschen was zu ueberlegen ok das ist also der gradient was machen wir jetzt jetzt ueberlegen  wir uns wir machen mal bei dieser funktion gradientenabstieg ja und wir nennen die funktion", "start": 1400.48, "end": 1440.0}, {"text": " jetzt mal g von delta und das soll also sein f ja und jetzt machen wir so so einen gradienten  abstiegsschritt x minus delta gradient von f von x ja also das ist jetzt die idee wir haben diese  funktion wir wollen gradientenabstieg machen und und wir ueberlegen uns jetzt mal was waere die  schrittweite was was muesste die schrittweite eigentlich sein bei so einem verfahren das das ist", "start": 1440.0, "end": 1487.8799999999999}, {"text": " also die die funktion ne ja jetzt koennen wir sie hinschreiben das habe ich hier gemacht glaube  ich ja muehsam aber so ist es machen wir das mal also c minus dann haben wir hier x minus delta  gradient von f von x transponiert mal b plus ein halb x minus delta gradient von f von x  transponiert mal mal diesen ausdruck noch mal ja das ist diese funktion g von delta und was", "start": 1487.88, "end": 1535.3200000000002}, {"text": " jetzt ganz huebsch ist das ist jetzt das haben wir natuerlich so gebastelt im vorhinein eine  funktion die ich tatsaechlich bei der ich das minimum tatsaechlich bestimmen kann ja naemlich  eine quadratische funktion in delta das ist ein lin eine linearer linearer term fuer delta und das  hier ist ein quadratischer term fuer delta das heisst strategie ableitung null setzen", "start": 1535.3200000000002, "end": 1567.76}, {"text": " ok jetzt leiten wir mal ab ich mache das mal so wie hier c haengt nicht von delta ab waehlt  also raus x t b haengt auch nicht von delta ab was hier uebrig bleibt ist also gradient von f von x  transponiert mal b und hier hinten ist wieder dieses quadratische ding ja und also ich kann erst den  teil ableiten dann mit dem hier multiplizieren oder umgekehrt und die beiden dann aufadieren und", "start": 1567.76, "end": 1608.0}, {"text": " da muss man ein bisschen aufpassen dass man jetzt mit der dimensionierung der vektoren nicht  durcheinander kommt deswegen macht man das so wieder oben man nimmt einfach den einen doppelt  jetzt muss mal gucken welchen ich hier genommen habe ok also wir nehmen den hier plus gradient von  f von x transponiert mal a mal das was da steht das ist die ableitung", "start": 1608.0, "end": 1643.6}, {"text": " und jetzt kann man auch ein bisschen weitermachen  der erste schritt ist  wir setzen hier mal b ein  ja b ist  ax minus gradient von f von x  hat gesagt ax minus gradient von f von x  plus da hinten setzen wir nichts an glaube ich  so  und jetzt muessen wir in anfuehrendes zeichen nur noch term sammeln und  nur noch term sammeln und hinschreiben  habe ich hier irgendein fehler gemacht", "start": 1647.1999999999998, "end": 1729.52}, {"text": " habe ich war klar  so da ist ein minus dann muss da auch ein minus sein und da auch ein minus  so  ja warum ist mir das wichtig dieses minus weil jetzt folgende term rausfaellt wir haben hier  gradient f von x transponiert mal a mal x und wir haben hier gradient von f von x transponiert mal  a mal x aber mit diesem minus ja das heisst diese beiden term gehen verloren und was dann uebrig", "start": 1737.08, "end": 1792.08}, {"text": " bleibt ist hier gradient von f von x transponiert minus mal gradient von f von x und jetzt hier plus  minus minus delta gradient von f von x transponiert mal a mal gradient von f von x und das heisst  jetzt koennen wir delta hinschreiben  delta ist also dieser teil durch diesen teil gradient von f von x transponiert mal gradient von", "start": 1792.08, "end": 1830.76}, {"text": " f von x und im nennar steht leider nicht das gleiche sondern da steht das bloede a dazwischen  so das heisst wir koennen wir koennen diesen diesen ausdruck ausrechnen und das ist unser  ausdruck fuer delta koennten wir also die hesse matrix ausrechnen dann dann waere das gar nicht so  verkehrt dann koennte man immer auch die optimale schrittweite ausrechnen", "start": 1831.36, "end": 1869.4}, {"text": " so ich lasse das da mal stehen und machen wir da drueben weiter weil was ich jetzt gerne zeigen  moechte ist dass wenn man die optimalen schritte macht das was wir jetzt machen koennen ist wir  koennen den gradienten vergleichen und wir koennen den vergleichen mit dem naechsten gradient ja was  ist der naechste gradient der gradient steht da das ist ax minus b und fuer x setzen wir jetzt ein", "start": 1869.4, "end": 1925.08}, {"text": " unseren optimal gemachten schritt hier x minus delta mal gradient von f von x x minus delta  delta haben wir gerade ausgerechnet das ist das hier gradient von f von x transponiert mal  gradient von f von x geteilt durch das ganze mit dem a dazwischen ja und das nehmen wir jetzt mal  den gradient von f von x mal x minus b so das ist der neue gradient ja den schreiben wir noch ein", "start": 1925.08, "end": 1975.4}, {"text": " bisschen anders den schreiben wir naemlich so dass wir uns ueberlegen na ja a mal x minus b ist der ist  der gradient von f von x das heisst den schreibe ich jetzt hier so hier irgendwann geht dann die  klammer zu und dann steht da gradient von f von x den schreibe ich jetzt noch nicht hin und dann  kommt jetzt noch dieser teil und da muss ich jetzt eben das a reinziehen ja dann steht hier also", "start": 1975.4, "end": 2015.8}, {"text": " minus diesen ausdruck mal a ja und hier draussen steht jetzt der gradient von f von x ja und dieser  dieser term a mal x minus b ist jetzt hier identitaet mal gradient von f von x ok das ist der neue  gradient ja und jetzt kann ich folgendes tun ich kann diesen neuen gradient mal mit dem alten  gradient multiplizieren ja also ich gucke mir an gradient von f von x transponiert mal diesen", "start": 2015.8, "end": 2064.92}, {"text": " ausdruck so mal gradient von x  ok und  wie geht es weiter  den linken teil den kann ich gut hier das ist gradient von f von x transponiert  demal gradient von x das ist einfach dieses i hier mal das da und dann was machen wir mit dem  rechten teil ah wer falsch abschreibt hat natuerlich auch keine chance das richtige raus zu bekommen", "start": 2064.92, "end": 2159.68}, {"text": " da war ja noch ein a jetzt jetzt wird es besser ja ok jetzt ist alles gut und jetzt haben wir hier  gradient von f von x transponiert mal a mal gradient von f von x gradient von x transponiert mal a mal  gradient von f von x mal diesen bruch so und jetzt sehen wir die haben wir den gleichen  ausdruck das bleibt stehen das und das ist das gleiche das heisst das ganze ist null passt da", "start": 2159.68, "end": 2213.32}, {"text": " hinten gerade so noch eben hin glaube ich so null was heisst das das heisst der gradient wenn ich den  optimalen schritt mache den ich bei dieser funktion ausrechnen konnte aber ganz allgemein  wenn ich den optimalen schritt mache dann steht der gradient im schritt i des verfahrens  orthogonal auf dem gradienten im schritt i plus eins des verfahrens das heisst wenn ich gradienten", "start": 2213.32, "end": 2241.6800000000003}, {"text": " abstieg mache so wie es da oben steht ich waehle mein delta optimal ich laufe in irgendeine  richtung ich laufe in richtung des gradienten ich laufe solange bis ich eine perfekte stelle finde  an der es nicht mehr weiter geht an der ich wenn ich weiterlaufen wuerde oder auch ein schritt  zuruecklaufen wuerde die funktion nur groesser machen wuerde dann ist es so dass im naechsten schritt", "start": 2241.6800000000003, "end": 2263.56}, {"text": " der gradient jetzt orthogonal steht auf dem gradienten den ich gerade benutzt habe das ist  unsere erkenntnis das ist auch nicht so unklassibel irgendwie  ja ich mal jetzt mal hier ist so ein optimum und um das optimum herum gibt es gibt es niveau linien  ich laufe jetzt hier also so durch die gegend auf irgendeiner gradienten richtung", "start": 2263.56, "end": 2305.72}, {"text": " und irgendwann ist es so dass wenn ich weiterlaufen wuerde die funktion wieder groesser wird wenn ich  nicht mehr weiterlaufen wuerde die funktion wenn ich zuruecklaufen wuerde die funktion auch groesser  wird und was das heisst ist dass diese bewegungsrichtung an der stelle eine tangente ist zu den  niveau linien von dieser funktion und das heisst im naechsten schritt wenn ich mir jetzt einen", "start": 2305.72, "end": 2333.32}, {"text": " gradienten ausrechnen steht er eben orthogonal auf den diesen niveau linien und im naechsten schritt  wuerde ich dann in diese richtung laufen das heisst eine eigenschaft des gradienten  abstiegsverfahrens ist es wenn ich optimale schrittweiten bestimme dass ich immer in  orthogonalen richtungen laufe und welche frage stellt man sich jetzt logischerweise an dieser stelle", "start": 2333.32, "end": 2362.4799999999996}, {"text": " mit dieser erkenntnis ja ist es eigentlich gut oder schlecht dass man immer orthogonal richtungen  waehlt und dann merkt man manchmal gut manchmal schlecht und dann stellt man sich logischerweise  welche frage kann ich das charakterisieren ja wann es ist gut wann es ist schlecht jetzt daher die  frage wann ist das toll wenn ich in jedem schritt orthogonal laufe was gibt es fuer funktionen bei", "start": 2362.48, "end": 2402.2000000000003}, {"text": " denen das genau das richtige ist oder wir fragen mal anders wie muessten diese niveau linien beschaffen  sein damit es gut ist konzentrische kreise genau ja wenn diese niveau linien konzentrische kreise  waeren dann waere es total gut so eine strategie zu waehlen ja wenn das kreise sind dann bin ich im  r2 mit zwei schritten fertig ich laufe irgendwo los ich bin hier sagen wir mal funktion weit weg", "start": 2402.2000000000003, "end": 2444.36}, {"text": " es hat irgendeinen anderen gradienten egal ich laufe also los und dann bin ich hier und jetzt laufe  ich orthogonal und dann bin ich fertig allgemein wenn das isotrop ist ja wenn wenn diese matrix  ein vielfaches der einheitsmatrix ist wo ist da die hier wenn diese hessermatrix ein vielfaches  der identitaet ist dann wuerde dieses verfahren in in d schritten zum erfolg fuehren wenn die die", "start": 2444.36, "end": 2473.4}, {"text": " dimension des raumes ist ja und besser besser wird es nicht was ich dann sozusagen machen koennte  ist ich koennte sagen meine funktion ist eine funktion von von n veraenderlichen x 0 x 1 x 2  x 3 x 4 und ich kann das optimum finden indem ich das optimum bezueglich x 0 finde dann das  optimum bezueglich x 1 dann das optimum bezueglich x 2 dann das optimum bezueglich x 3 und so weiter", "start": 2473.4, "end": 2500.8}, {"text": " und wenn ich bei der letzten variabler angekommen bin bin ich tatsaechlich sicher fertig und und es  ist aber im allgemeinen ist es nicht so ja das geht nur dann wenn das konzentrische kreise  werden ja dann koennte ich sozusagen in jeder richtung entlang jeder axe diese funktion optimieren  und wenn ich das mache ich endschritte und dann bin ich fertig so wie sieht es aus wenn ich keine", "start": 2500.8, "end": 2532.24}, {"text": " so konzentrischen kreise habe sondern sagen wir mal solche ellipsen dann ist nicht so klar  da kann ich immer noch grosses glueck haben und ich laufe hier auf so eine hauptachse drauf und bin  fertig wenn ich nicht so ein grosses glueck habe und in so einer richtung anfange dann laufe ich hier  in 726 millionen kleinen schritten entlang dieser hauptachse und bin dann irgendwann da und dann", "start": 2532.24, "end": 2562.56}, {"text": " bin ich immer orthogonal gelaufen aber irgendwie hat diese elliptische funktion mir das leben  komplett kaputt gemacht ok was koennte man tun was ist zu tun offensichtlich ist unser grosser  wunsch selbst wenn die situation so ist irgendwie diese situation herbeizufuehren das sollten wir  versuchen das heisst wir stellen uns die frage wie kommen wir von hier nach hier und wir wissen", "start": 2562.56, "end": 2608.16}, {"text": " schon diese form von den kreisen die wird beschrieben von dieser matrix a das ist das was diese was  diese quadratische form letztlich beschreibt ok  ja was machen wir also wir wir wir schauen uns das a scharf an und wir schreiben das a  wie so oft in solchen faellen wir haben es gemacht so doch so rum wir schreiben es", "start": 2608.16, "end": 2666.2}, {"text": " in eigenbasis ja wir schreiben also a so hin und wir gucken uns jetzt an was passiert wenn wir  solche ausdruecke ausrechnen x t ax ist dann x t qt lambda q x ja jetzt machen wir noch was  wir ziehen hier die wurzel das ist relativ einfach hier die wurzel zu ziehen weil weil lambda eine  diagonal matrix ist x t qt lambda hoch ein halb mal lambda hoch ein halb q x", "start": 2666.2, "end": 2732.1600000000003}, {"text": " also wer moechte kann jetzt noch um das gleich besser zu sehen dieses transponiert Zeichen hier noch  so rum schreiben ja das heisst wir koennen wir koennen diese Produkte diese in Produkte  interpretieren als transformierte in Produkte die wir jetzt einfach wie habe ich die hier genannt  y ich nenne dieses ding jetzt also y und das ist genau die transformation was macht", "start": 2742.1600000000003, "end": 2787.4}, {"text": " diese transformation jetzt hier mal zurueck zu diesen bildern ja a beschreibt also diese  elliptische situationen ja und was ich jetzt mit dieser diagonalisierung von a erreiche ist dass  ich diese ellipsen interpretiere als eine rotation meines systems dann irgendeine skalierung und  stauchung und dann wieder eine rotation ja das ist also das hier aufgedroeselt und alles was", "start": 2787.4, "end": 2822.52}, {"text": " ich hier in diesem system mit x mache kann ich jetzt eben interpretieren als ein transformiertes  bild ja und diese transformation die ich mache eigentlich steht jetzt hier ja und damit kann  ich jetzt dieses bild uebersetzen in dieses bild also anstatt dass ich mit x in diesem bild  rechne kann ich auch mit y in diesem bild in konzentrischen kreisen rechnen ja das ist die", "start": 2822.52, "end": 2861.4}, {"text": " idee also hier hier sind die x hier ist diese transformation und dann sind hier die y das ist  die idee so und jetzt hatten wir gesagt in diesem bild in diesem bild ist orthogonalitaet genau das  was ich gerne haette von irgendwelchen richtungen ja also wenn ich im ersten schritt so laufe da  moechte ich gerne im zweiten schritt orthogonal dazu laufen und was bedeutet das jetzt in diesem", "start": 2861.4, "end": 2891.1200000000003}, {"text": " bild ja ich haette also gerne dass die diese produkte von zwei sagen wir mal jetzt zwei  verschiedenen richtungen da haette ich jetzt gerne y thema y strich irgendein anderes y ich  haette gerne dass die zwei richtungen in denen ich da laufe orthogonal sind und was bedeutet das  eben hier das bedeutet das fuer zwei x und x strich dieses produkt null ist das ist also die", "start": 2891.1200000000003, "end": 2927.3599999999997}, {"text": " idee was wir lieber haetten als dass die erste richtung und die zweite richtung orthogonal  sind ist also dass die erste richtung und die zweite richtung orthogonal sind bezueglich dieses  durch a gegebenen innenproduktes und wir sagen dann auch x und x strich sind konjugiert  das ist also die das ist die idee wir wuerden nicht so gerne entlang von richtungen laufen die", "start": 2927.3599999999997, "end": 2961.7200000000003}, {"text": " orthogonal sind sondern wir wuerden lieber entlang von richtungen laufen die konjugiert sind  so jetzt ueberlegen wir uns ein verfahren  das ist das verfahren der konjugierten gradienten  wir laufen also entlang von richtungen ri und wir haetten gerne ri so gewaehlt dass ri  transponiert a rj gleich null ist fuer alle j kleiner i ja das ist die idee  und wie machen wir das", "start": 2966.7200000000003, "end": 3032.44}, {"text": " wir werden ri warum habe ich es gemacht ri  so dass wir sagen naja das soll vielleicht so ungefaehr der gradient sein  aber nicht ganz ja denn wir wollen ja nicht die gradienten richtung waehlen sondern wir wollen  eine waehlen die orthogonal die die konjugiert ist zu all diesen anderen richtungen jetzt machen", "start": 3032.44, "end": 3063.44}, {"text": " wir das genaues gleich wie bei kram schmidt wir projizieren all die teile raus die die hier  drin stecken jetzt entfernen wir all das was in dieser richtung gradient von f drin steht wenn  wir hier das das produkt bilden ja und das das sieht dann einfach so aus wir machen hier  rj warum habe ich es gemacht rj transponiert a mal diesen gradienten von f von xi geteilt durch", "start": 3063.44, "end": 3105.04}, {"text": " rj transponiert rj rj ja das ist einfach die laenge und das muessen wir machen fuer alle j kleiner i  also das ist einfach raus projizieren von den anteilen die wir nicht haben wollen und in  gewisser Weise ist das verfahren der konjugierten gradienten damit fertig ja ausser dass es irgendwie  so ueberhaupt nichts bringt in der praxis ja und es bringt nichts aus aus verschiedenen gruenden", "start": 3105.04, "end": 3143.56}, {"text": " der haupgrund ist dass wir in der praxis diese matrix a nicht kennen ja wir haben irgendeine  funktion die meisten funktionen sind nicht quadratisch und wir haben sie nicht hingeschrieben  und ansonsten hatte ich schon gesagt diese matrix a hat die funktion der hesse matrix und  ich sag mal so wenn ich die hesse matrix gut ausrechnen kann kann ich gleich Newton machen", "start": 3143.56, "end": 3165.04}, {"text": " ja dann brauche ich das ganze ding hier nicht meistens ist die hesse matrix viel zu gross und  viel zu muehsam auszurechnen mal abgesehen davon was an diesem verfahren auch ziemlich erschrocken ist  ist dass ich mir alle alten richtungen merken muss ja ich muss mir also die komplette historie  merken und diese diese summe wird auch mit jedem schritt natuerlich groesser also das ist so", "start": 3165.04, "end": 3192.1600000000003}, {"text": " so ist es ist es irgendwie eine beobachtung aber fuer die praxis voellig nutzlos ja und  jetzt kommt der teil den ich den ich hier nicht auch noch durch akkern will sondern den wir uns  sozusagen jetzt hier zum schluss einfach nur hin pinn ja im buch wuerde stehen elementare  transformation bringen uns zu folgenden es sind in der tatelementare transformation aber leider", "start": 3192.1600000000003, "end": 3226.96}, {"text": " ziemlich viele und dazu habe ich keine lust  ich sage gleich was dazu  diese elementaren transformationen fuehren auch folgendes  statt das zu tun reicht es das hier zu tun ja und was man hier sieht ist es innenprodukt  von dem gradienten im schritt i mit sich selbst geteilt durch das innenprodukt von den gradienten", "start": 3244.2, "end": 3291.6400000000003}, {"text": " im schritt zuvor mal die richtung aus dem schritt zuvor und das ist der komplette korrektur term fuer  diesen gradienten ja das heisst das verfahren der konjugierten gradienten besteht darin dass man  im i den schritt den gradienten ausrechnet und dazu addiert ein vielfaches von der richtung aus  dem vergangenen schritt aus dem vorangegangenen schritt und dieser faktor fuer fuer den fuer diesen", "start": 3291.64, "end": 3323.6400000000003}, {"text": " term fuer den fuer die richtung aus dem vorangangenen schritt ist dieser bruch naemlich die laenge des  gradienten quadriert geteilt durch die laenge des gradienten aus dem vorangegangenen schritt  quadriert und und diese transformation von diesem ausdruck zu diesem ausdruck ist muehsam deswegen  schreibe ich die hier nicht hin sondern sage so ist das es steht seit ewigkeiten so in buechern", "start": 3323.6400000000003, "end": 3360.6800000000003}, {"text": " drin von daher glauben wir das und jetzt ja wir brauchen jetzt auch nicht mehr alle richtungen  aus den vorangegangenen schritten sondern wir merken uns immer nur die richtung aus dem vorangegangenen  schritt und den gradienten aus dem vorangegangenen schritt mehr nicht aber noch viel wichtiger ist  wir brauchen die matrix a nicht die wir im allgemeinen nicht kennen so das ist das verfahren der", "start": 3360.6800000000003, "end": 3389.8}, {"text": " konjugierten gradienten und das konvergiert im allgemeinen schneller als gradienten abstieg das  heisst was ich jetzt eben mache wieder ist ich sage x e plus eins ist x e plus delta mal r  e habe ich mit plus habe ich mit minus ok machen wir hier auch minus so der nachteil an den konjugierten  gradienten also vorteil wir waehlen kluge richtungen das heisst bei so einer elliptischen funktion sind", "start": 3389.8, "end": 3427.48}, {"text": " wir mit den konjugierten gradienten nach zwei stritten fertig wir wuerden hierhin laufen irgendwie  zack bumm schritt eins und dann wuerden wir eine richtung waehlen die jetzt konjugiert ist und nicht  orthogonal und das waere diese richtung und wir waeren fertig der nachteil an diesem an diesem  verfahren der konjugierten gradienten ist dass diese transformation hier da steckt eine annahme", "start": 3427.48, "end": 3457.8799999999997}, {"text": " drin und die annahme ist ist dass wir delta stets optimal bestimmen sonst funktioniert es nicht  das heisst wir koennen uns jetzt nicht mehr erlauben wie vorher beim gradienten abstieg  irgendwas zu raten hauptsache der funktionswert wird kleiner wenn wir das machen dann wird dieses  verfahren typischerweise ziemlich schlecht jetzt muessen wir wirklich delta bestimmen mit so was", "start": 3457.88, "end": 3483.04}, {"text": " wie dieser goldene schnitz suche oder so und dann bringt es auch was das heisst dann hat man  sozusagen jetzt so ein richtig so ein halbwegs kompliziertes verfahren zusammengebaut man  man rechnet sich diese richtungen aus auf diese art und Weise und dann bestimmt man in jedem  schritt delta mit diesem goldene schnitz suche verfahren also wer moechte kann es auch in der", "start": 3483.04, "end": 3507.2799999999997}, {"text": " hausaufgabe mal ausprobieren statt irgendwie gradienten abstieg zu machen diese sachen auch  noch zu machen ja und dann sieht man man braucht auf jeden fall weniger schritte als beim gradienten  abstieg zumindest war das bei mir so als ich es ausprobiert habe ja das sind so eine letzte  sache noch dann haben wir eine letzte sache noch machen wir machen wir", "start": 3507.2799999999997, "end": 3555.1200000000003}, {"text": " hier weiter manchmal haben wir solche probleme wir wollen gerne eine funktion minimieren  aber wir haben eine nebenbedingung und die nebenbedingung sieht vielleicht so aus ja  wenn ich sage manchmal dann dann heisst das eigentlich ist es furchtbar oft so ja dass wir  nicht irgendwas optimieren sondern wir haben oft eben randbedingungen und diese randbedingungen", "start": 3555.12, "end": 3593.76}, {"text": " sehen im allgemeinen so aus so und jetzt ueberlegen wir uns mal was das bedeutet machen wir wieder  bilder sie naemlich gluecklicherweise die gleichen bilder wie hier alles das gleiche so sagen wir  mal diese funktion g ist eine funktion von dem r 2 nach r ja also g geht vom r 2 nach r und g von  x gleich c ist dann einfach so eine kurve also wir haben irgendeine funktion ueber dem r 2 sagen", "start": 3593.76, "end": 3642.32}, {"text": " wir mal ein paraboloid oder so kann man sich immer vorstellen wie ein berg und die bedingung g von x  soll bitte schoen c sein heisst dann ich interessiere mich fuer die orte fuer die der berg eine gewisse  hoehe hat ja ich schneide also zum beispiel den berg gegen die kugel die hundert meter ueber der  meeres spiegel liegt und dann kriege ich so eine hoehenlinie genau wie ich sie von der landkarte", "start": 3642.32, "end": 3667.88}, {"text": " kenne das heisst das hier koennten zum beispiel die orte sein fuer die gilt g von x gleich c  ok was gilt wenn ich mich entlang dieser kurve bewege ich moechte ja irgendeine andere funktion  sagen wir mal auch f auch von r 2 nach r die moechte ich ja minimieren ja und ich koennte ja einfach  sagen na ja ich lauf entlang dieser kurve und guck mir immer die funktionswerte von f an und", "start": 3667.88, "end": 3705.4}, {"text": " versuche eben dass die funktionswerte dann moeglichst klein werden oder so wenn ich so entlang  der kurve spaziere was kann ich ueber meine richtung sagen bezueglich der funktion g ja ich kann sagen  wenn ich da lang laufe dann laufe ich die ganze zeit orthogonal zum gradienten von der funktion  g der gradient zeigt in richtung des steilsten anstiegs und ich laufe aber auf der funktion", "start": 3705.4, "end": 3741.44}, {"text": " g so dass die werte von g weder groesser werden noch kleiner das heisst ich laufe orthogonal  zur richtung des steilsten anstiegs ich halte meine hoehe auf dem berg so das gilt also fuer  die gesamte bewegung auf dieser kurve die laufe die ganze zeit orthogonal zum gradienten von g von  x und jetzt schauen wir uns an was die funktionswerte von f machen die werden eben groesser oder kleiner", "start": 3741.44, "end": 3778.96}, {"text": " aber was gilt an der stelle an der das minimum von f ist ja ich laufe dann also an der stelle in  dem moment so dass die funktion f weder groesser wird noch kleiner wird sondern fuer einen moment  konstant bleibt das heisst die richtung in die ich laufe die richtungs ableitung an der stelle  von f in die richtung in die ich laufe ist null das heisst ich laufe an der stelle orthogonal zum", "start": 3778.96, "end": 3812.24}, {"text": " gradienten von f dass die extremstellen von f auf diesem ring sind dadurch gekennzeichnet dass  ich in dem moment auch orthogonal laufe zum gradienten von f das heisst die notwendige  bedingung dafuer dass das gilt ist dass ich sowohl entlang der kurve orthogonal zum gradienten von  f laufe als auch orthogonal zum gradienten von g oder mit anderen worten die gradienten von f", "start": 3812.24, "end": 3850.88}, {"text": " und g sind parallel gleich sind sie wahrscheinlich nicht weil sie ja nicht gleich lang sein muessen  das heisst die notwendige bedingung ist sowas und diese notwendige bedingung ob ich das nun  in der ebene hin male oder mir in hoehere dimension ueberlege ist voellig egal solange ich mich auf der  menge bewege die durch so eine gleichung spezifiziert ist veraendern sich die werte nicht das heisst ich", "start": 3850.88, "end": 3877.6800000000003}, {"text": " laufe die ganze zeit orthogonal zum gradienten von g und wenn f nicht mehr kleiner wird und so dann  ist es genau das gleiche ich habe eine stelle gefunden an der ich auch orthogonal laufe zum  gradienten von f das ist also die notwendige bedingung so und daraus basteln wir uns jetzt ein  lustiges verfahren und zwar machen wir folgendes wir definieren uns eine neue funktion die heisst", "start": 3877.6800000000003, "end": 3920.76}, {"text": " typischerweise 11 die haengt jetzt ab von x und von diesen parameter lambda und die heisst f von x minus  lambda mal g von x minus c und jetzt machen wir folgende beobachtung wenn wir das ding ableiten  nach x was bleibt dann stehen ja also wenn wir jetzt den gradienten von l bezueglich x bilden  dann ist das der gradient von f von x minus lambda mal den gradienten von g von x c haengt nicht von x", "start": 3920.76, "end": 3970.36}, {"text": " ab also lambda mal c faellt einfach raus und wenn ich das gleich null setze dann ist das genau die  bedingung die ich erfuellen muss und die ableitung von l nach lambda ist einfach g von x minus c und  wenn ich das gleich null setze bekomme ich diese bedingung die idee ist also wir haben eine neue  funktion gebaut die hat einen parameter mehr und bei der ist es jetzt so dass der kritische punkt", "start": 3970.36, "end": 4007.56}, {"text": " genau die bedingungen erfuellt die wir erfuellen muessen ja der kritische punkt das minimum dieser  funktion hat die eigenschaft dass es diese funktion minimiert mit dieser nebenbedingung und damit  haben wir dieses erst mal etwas haessliche problem man minimiere f von x unter der nebenbedingung dass  g von x gleich c ist dieses problem haben wir uebersetzt in ein problem mit dem wir die ganze", "start": 4007.56, "end": 4031.1600000000003}, {"text": " zeit schon umgehen naemlich ein problem ohne nebenbedingungen ja das ist jetzt eine funktion von  x und lambda und fuer die muss ich einfach nur noch das minimum suchen ja und wenn ich das gefunden habe  bin ich fertig und die nebenbedingung ist automatisch erfuellt ja und dieses verfahren wird l'agrange  zugeschrieben und heisst deswegen das verfahren der l'agrange multiplica toren", "start": 4031.1600000000003, "end": 4069.6000000000004}, {"text": " multiplica toren und multiplica toren im plural weil ich hier viele nebenbedingungen haben kann ja also  hier koennte stehen g i von x gleich c dann habe ich viele solche parallelitaetsbedingungen dann  habe ich hier ganz viele lambdas und meine funktion hier lautet dann einfach so ja summe i lambda i  mal g i und dann habe ich eben immer noch das hier diese orthogonalitaetsbedingung ist erfuellt", "start": 4069.6, "end": 4103.360000000001}, {"text": " ja und fuer die lambda i's kriege ich hier die ganzen nebenbedingungen  so das ist das verfahren der l'agrange multiplica toren ja  wie bitte  ah na klar das ist ja genau die bedingung die wir erfuellen wollen ja die idee ist dass wir uns  ueberlegt haben dass diese das diese gradienten parallel sein muessen im kritischen punkt und", "start": 4103.36, "end": 4153.4800000000005}, {"text": " die idee ist dass genau das hier drin steht genau das ist es und mit diesem verfahren kann man  eine ganze menge probleme mit nebenbedingungen angehen und dann auf dem weh geloesen das war es  ja mir haben wir nicht naechste woche wie gesagt noch mal eine vorbereitungsrunde auf die clausur  ja viel glueck weiterhin alles gute wir sehen uns vielleicht noch mal wenn sie computergrafik", "start": 4153.4800000000005, "end": 4191.04}, {"text": " machen wollen wenn nicht dann auch gut ja dann insgesamt viel glueck noch viel spass viel erfolgt", "start": null, "end": 4191.04}]}]