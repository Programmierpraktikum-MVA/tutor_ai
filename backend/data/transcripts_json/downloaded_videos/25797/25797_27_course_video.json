[{"lecture": "25797_27_course_video", "Timestamps": [{"text": "  In diesem Video wollen wir weitermachen mit der Wiederholung der Linie an Algebra.  Das hatte ich in dem Video davor schon erwaehnt, dass wir diesen Coat hier benutzen wollen, im  Wedelichen die Plottfunktion.  Die wird uns helfen, garfische Veranschaulichungen von Vektoren darzustellen und uns dann beim  Verstaendnis von dem Stoff helfen.", "start": 0.0, "end": 22.0}, {"text": "  Also wird im Wedelichen die Funktion Mapplotlib verwendet, die Bibliothek Mapplotlib.  Ihr muesst den Coat hier aber nicht im Detail verstehen.  Das Thema, was uns in diesem Video eigentlich durchwaert beschaeftigen wird, sind linaere  Abbildungen.  Linaere Abbildungen sind halt Abbildungen, Operationen oder Funktionen, die einen Vektor nach einem  anderen Vektor transformieren.", "start": 22.0, "end": 42.0}, {"text": "  Zum Beispiel halt von einem zweidimensionalen Vektor zu einem dreidimensionalen Vektor.  Also die Dimensionen koennen sich dann unterscheiden von dem Unitionsbereich zum Bildbereich.  Es kann aber halt auch sein, dass einer der klassischen Faelle, die wir auch mit betrachten  werden, besser die Dimensionen gleich bleiben.", "start": 42.0, "end": 61.06}, {"text": "  Also dass wir von einem zweidimensionalen Vektor wieder auf einen zweidimensionalen Vektor  abbilden.  Bei jede linaere Abbildung zwischen endlich-dimensionalen Vektoraeumen kann dann durch eine Matrix A  ausgedrueckt werden, die dann eben die Dimension M by N hat, wenn wir von einem n-dimensionalen  Raum auf einen m-dimensionalen Raum abbilden.", "start": 61.06, "end": 86.54}, {"text": "  Als Matrix-Vektorprodukt A by X ist gleich Y, wird dann eben den Vektor X mit enddimensionen  auf ein Vektor Y mit enddimensionen ab.  Und jetzt wollen wir uns das nochmal im Detail anschauen.  Also erstmal ist wichtig, dass die Dimensionen passen muessen.", "start": 86.54, "end": 104.94}, {"text": " Wenn wir eine Matrix A haben, Dimension M by N, dann brauchen wir ein Vektor V, der auch  die Dimension N hat, damit wir den auf diese Matrix drauf verdienen koennen.  Wenn wir es so vorstellen, wenn wir A mal V schreiben, dann steht da eine M by M Matrix  neben einem n-dimensionalen Vektor, dass die beiden inneren Dimensionen muessen passen.", "start": 105.46, "end": 121.62}, {"text": "  Also dieses N, die Anzahl der Schwalten der Matrix muessen, M muss eben zu den Dimensionen  des Vektors passen.  Und was dann rauskommt, ist eben ein m-dimensionaler Vektor.  Eine lineare Abbildung A kann ein Vektor im selben Vektorraum transformieren oder eben  in ein anderen Vektorraum abbilden, wenn sich m und n von dieser Matrix eben unterscheiden  und die Matrix nicht quadratisch ist.", "start": 121.62, "end": 145.14}, {"text": "  Jetzt kann man jetzt so zwei verschiedene Varianten interpretieren.  Eine Variante ist die Rheinweiseinterpretation, dass quasi die typische Interpretation hier  wahrscheinlich auch aus der linearen Algebra dann kennt.  Das bedeutet im wedentlichen, wir multiplizieren, Zeile mal spalten, weil man die Fausten  hat.", "start": 145.14, "end": 163.26000000000002}, {"text": "  Als wir nehmen die erste Zeile, zwei, drei, und bilden das Skalaprodukt mit den ersten  Spalten mit dem Vektor ein, zwei.  Also wir haben das Skalaprodukt zwischen der ersten Zeile und dem Vektor.  Und der zweite Eintrack in dem Ergebnis Vektor ist dann die zweite Zeile mal Vektor, also  das Skalaprodukt davon.", "start": 163.26000000000002, "end": 184.29999999999998}, {"text": "  Dann koennen wir wieder in zwei Dimensionalen Vektor raus, weil wir eben in der zweimal  Zweimartikel.  Eine andere Interpretation waere eine Linearkombination aus den beiden Spalten in der Matrix.  Also wenn wir uns das hier mal angucken, im wedentlichen multiplizieren wir ja die zwei  mit eins und addieren dann zweimal drei.", "start": 184.29999999999998, "end": 202.74}, {"text": "  Auch die vier multiplizieren wir mit eins und addieren dann zweimal fuenf.  D.h. zwei und vier wird eben mit eins multipliziert und rein fuenf mit zwei multipliziert.  Und das kann man halt auch so schreiben, dass wir quasi diesen ersten Spalten Vektor  mit eins skalieren und den zweiten Spalten Vektor mit zwei skalieren.", "start": 202.74, "end": 220.01999999999998}, {"text": "  Dann haben wir eben diese Linearkombination aus den beiden Spalten Vektoren mit dem Gewicht  des Vektors, mit dem wir eben multiplizieren.  Bei lineare Abbildungen haben wir bestimmte Eigenschaften.  Die haben wir vorhin schon mal bei den Skalaprodukt gesehen.  Die erste Eigenschaft ist die Homogenitaet.", "start": 220.01999999999998, "end": 240.42}, {"text": " Heisst wenn ich einen Vektor abhuelle und dann skaliere, dann ist das genau das gleiche,  als wenn ich den skalierten Vektor abhuelle.  Ausserdem haben wir die Additivitaet.  Das heisst, wenn ich die Summe von zwei Vektoren abhuelle, dann kann ich genauso gut einfach  die Summe von den beiden abgebildeten Vektoren nehmen.", "start": 241.22, "end": 258.21999999999997}, {"text": "  So, wenn ich dann zusammenfassen, heisst der Vektor V skaliert plus der Vektor B skaliert  abgebildet, ist genau dasselbe, wie der Vektor U abgebildet und dann skaliert plus der Vektor  V abgebildet und dann skaliert.  Bei die Linearitaet von Abbildungen, also genau diese beiden Eigenschaften hier, sind fuer  die numerische Berechnungen dann sehr, sehr wichtig.", "start": 258.22, "end": 279.46000000000004}, {"text": "  Das heisst, wir werden die auch noch in diesem Video noch mal verwenden, diese beiden Eigenschaften.  Und auch im Laufe des Semesters werden wir halt immer wieder implizit diese Rechenregeln verwenden.  Jetzt habe ich wieder eine Frage an euch.", "start": 280.06, "end": 298.42}, {"text": " Das heisst, die Bitte haltet das Video dann kurz an, ueberlegt, denkt ueber diese Frage  nach und wenn ihr vor euch eine Antwort gefunden habt, dann koennt ihr weiter schauen.  Die Frage ist, welche Transformationen durch lineare Abbildungen umgesetzt werden koennen.", "start": 298.42, "end": 313.54}, {"text": " Stellt euch vor, ihr habt einen zweidimensionalen Vektorraum, also einfach nur quasi eine Flaeche  und da habt ihr jetzt ein Vektor, der vom Ursprung ausgeht, also ein bestimmtes, ein geometrisches  Objekt, was an eine bestimmte Richtung anzeigt, in eine bestimmte Richtungzeit, ausgehend  vom koordinaten Ursprung.", "start": 313.54, "end": 326.42}, {"text": "  Die Frage ist jetzt, was kann geometrisch alles durch eine lineare Abbildung umgesetzt werden?  Genau, eine lineare Abbildung kann skalieren, das heisst, der Vektor kann gestreckt, gestaucht  oder invertiert werden.  Eine lineare Abbildung kann rotieren, das heisst, der Vektor kann gedreht werden, also  wenn der Ursprung sitzt, kann der sich halt drehen.  Und was nicht moeglich ist, ist Translation.", "start": 330.94, "end": 355.3}, {"text": "  Also wir interpretieren den Vektor immer eben als eine Richtung ausgehend vom koordinaten Ursprung.  Also der Vektor lebt immer, geht immer vom koordinaten Ursprung aus und nicht von einem beliebigen  anderen Punkt.  Wenn wir Vektoren verschieben wollen, das heisst, interpretieren von einem anderen Punkt,  dann brauchen wir zum Beispiel einen Affidemraum.", "start": 356.58, "end": 373.5}, {"text": "  Das ist also nicht in den normalen Vektoren mit den linearen Abbildungen, so wie wir sie  jetzt betrachten, abgedeckt.  Dann habe ich jetzt hier ein erstes Beispiel.  Und zwar wollen wir jetzt hier eine Matrix und NumPy definieren.  Das heisst, ich sage A ist wieder ein NumPyArray.", "start": 373.5, "end": 396.34000000000003}, {"text": " Ich benutze diese Funktion NumPyArray und diesmal uebergebe ich halt keine einfache Liste fuer  ein Vektor, sondern eine verschachtete Liste, also quasi eine zweidimensionale Liste.  Eine Liste mit mehreren Listen und diese zweidimensionale Liste kann ich dann eben  als Matrix interpretieren.", "start": 397.09999999999997, "end": 408.9}, {"text": " Also ich habe die erste Zeile, die zweite Zeile und dann gibt es eben halt noch die  Spalten von der Matrix.  So, wenn ich diese Matrix definiert habe, dann kann ich mich hier einfach ausgeben und auf  die Dimension von dieser Matrix kann ich mit A.Shape zugreifen.  Und dann sehe ich eben eine zweimal drei Matrix, zwei Zeilen, drei Spalten.", "start": 409.85999999999996, "end": 429.74}, {"text": "  Wenn ich mir jetzt noch ein Vektor definiere, also sage X ist gleich MPArray, 123, dann  kann ich jetzt diesen Vektor auf die Matrix rauf multiplizieren mit A.X, also genau dieses  Matrix-Vektor-Produkt, dann bekomme ich Y heraus.", "start": 429.74, "end": 447.82}, {"text": " Da wir jetzt eine zweimal drei Matrix haben und ein Vektor mit drei Dimensionen rauf  multiplizieren, kreben wir ein Vektor mit zwei Dimensionen wieder raus, das gebe ich mir  hier aus mit Y.Shape.  Dann gibt es noch weitere nuetzliche Methoden und Funktionen, mit denen ich mir nuetzliche  Matrizen ausstellen kann in Numphile, zum Beispiel mp.i.", "start": 447.82, "end": 467.22}, {"text": "  Es ist in dem Fall hier mit dem Parameter drei, das heisst ich kriege die dreimal drei Identitaet  wieder.  Und dann moechte ich mir eine Matrix mit konstanten Eintraegen erstellen.  Dafuer kann ich erst mal mp1s verwenden.  Haben wir in dem Video der Forschung gesehen, diesmal eben mit dem Shape dreimal drei, fuer  keine dreimal drei Matrix, dann habe ich eine dreimal drei Matrix mit einzem.", "start": 467.22, "end": 491.3}, {"text": "  Wenn ich diese Matrix multipliziere mit Numphile p, dann kriege ich halt eine Matrix gefuehlt  mit ganz wem p raus.  In der Stelle moechte ich einmal ganz kurz darauf eingehen, wie man auf die einzelnen Elemente  zugreifen kann von dieser Matrix.  Da ist ich erstell mir hier eine zuverlebene Matrix, dann gebe die jetzt einmal aus.", "start": 491.3, "end": 511.3}, {"text": "  Und jetzt kann ich halt zugreifen oder moechte zugreifen auf die Elemente.  Das kann ich tun, indem ich halt den aeckigen Klammern in den Index schreibe.  Mit r0 kriege ich einfach den nullten Eintrag von den aeusseren A-Rail und das waere damit  die erste Zeile, kriege ich ja ausgegeben.  So.  Es kann es sein, dass ich zum Beispiel auf ein bestimmtes Element in diese ersten Zeile  zugreifen moechte.", "start": 512.02, "end": 537.2199999999999}, {"text": "  Dann kann ich schreiben 0, und dann den Spaltenindex 1.  Aus C kennt ihr das vielleicht so, dass man das dann so hier schreibt, also gute Zeile  und dann davon die erste Spaltung.  Numphile erlaubt es eben, die Indizies mit einem Komma zu trennen.  Heisst ich habe nullte Zeile, erste Spaltung.  So.  Jetzt kann es sein, hier habe ich eben auf die erste Zeile zugegriffen.", "start": 537.8199999999999, "end": 559.5}, {"text": "  Jetzt stellt sich so ein bisschen die Frage, was moechte ich, wie kann ich es denn erreichen,  wenn ich hier die erste Spalte haben moechte.  Dafuer muss sagen, okay, ich moechte ja alle Zeilen haben.  Kann ich es tun, indem ich den Doppelpunkt angebe und dann den Spaltenindex, und dann  kriege ich eben alle Zeilen, also alle Eintraege in diese ersten Spalte.", "start": 559.5, "end": 583.4599999999999}, {"text": "  Das sind eben die Werte 0,98, 2,4, 3,9, was 0,39.  Genau.  Also und das hier ist quasi ein Slicing-Operator.  Das heisst vor den Doppelpunkten kann ich angeben, wann ich Elemente haben wird, zum  Beispiel bei null, dann muss ich die nullen nicht schreiben.  Ich kann sagen, ab den ersten Element bis zum zweiten Element.", "start": 584.38, "end": 606.6600000000001}, {"text": "  Und ich moechte bei den Spalten, ab den nullten Element, bis zum Ausschliess, die  zweiten Element alles haben.  Wenn ich das eingebe, dann kriege ich hier ein Ausschnitt von dieser Matrix.  Ich habe mir jetzt hier noch drei hin.  Dann kriege ich wirklich diese zweimal zwei Teile Matrix, quasi von der ersten Zeile.", "start": 606.6600000000001, "end": 625.9}, {"text": "  Die Zeile moechte ich auch noch haben bis ausschliesslich drei, also eins und zwei sind  inhalten.  Und ich moechte alle Spalten haben bis auf die letzte.  Und dann kriege ich eben genau diesen Ausschnitt 0,4, 0,96, 0,51, 0,76.  Genau.  Es ist relativ praktisch, damit kann man halt dann immer aus der Matrix etwas  herausschneiden, auch bestimmte Elemente zugreifen und so weiter.", "start": 625.9, "end": 655.26}, {"text": "  Also ich lasse das mal so stehen, dass wir jetzt hier quasi alle Zeilen haben  wollen, aber halt die erste Spaltung.  Jetzt habe ich hier noch mal ein Beispiel, wo wir eben diese Plotting Funktionen vom  Anfang verwenden.  Zwar definiere ich hier einfach nochmal ein, eine Matrix mit zwei Spaltenvektoren,  naemlich 0 und minus 1 und 1 und 0.", "start": 655.26, "end": 678.94}, {"text": "  Offensichtlich sind die beiden Spaltenvektoren orthogonal zueinander.  Das heisst, das Skalaprodukt von 0 und minus 1, wie dem Vector 1 0 ist genau 0.  Also stehen die orthogonal aufeinander.  Dann bei der Einheitslaenge, also sogar eine autonormale Basis, darauf kommen wir  spaeter nochmal zurueck.  Und dann definieren wir uns den Vector V 0,5 und 1.", "start": 678.94, "end": 700.46}, {"text": "  Im Vector V haben wir hier einen Gelb dargestellt und den multiplizieren wir  jetzt auf diese Matrix M rauf.  Dann kriegen wir den Vector R hier unten in Lila dargestellt.  Was wir jetzt sehen, ist, dass wir im Wedeligen hier eine Rotation haben.  Also diese Matrix setzt die lineare Abbildung, setzt eine Rotation um minus 90 Grad.", "start": 700.46, "end": 722.38}, {"text": "  Da wir hier gegen den Uhrzeigersinn drehen, ist es, ne mit dem Uhrzeigersinn,  ist es ein negativer Winkel.  Und wenn wir gegen den Uhrzeigersinn drehen wuerden, waere es ein positiver Winkel.  Also hier sind es eben gerade minus 90 Grad.  So, jetzt wollen wir uns genauer anschauen, wie eine lineare Abbildung die  Koordinaten Darstellung von einem Vector veraendert.", "start": 722.38, "end": 741.5799999999999}, {"text": "  Dafuer nehmen wir uns ein einfaches Beispiel fuer eine lineare Abbildung,  naemlich eine isotope Skalierung.  Isotope bedeutet, dass auf der diagonalen Ueberall derselbe Wert steht.  Das heisst, wir eskalieren alle Dimensionen bei dem selben Wert Alpha.  Heisst, ein Vector V, den wir jetzt hier rauf multiplizieren wuerden, der wuerde  einfach in Eisen Dimensionen um Alpha skaliert werden.", "start": 741.5799999999999, "end": 761.58}, {"text": "  So, dann werden wir jetzt hier mal machen.  Wir schreiben SV.  Wir wollen den Vector V auf S rauf multiplizieren und koennen jetzt hier  erst mal diese geometrische Darstellung verwenden.  Heisst, V ist ja eben V 1, der erste Eintrag bei den ersten Einheitsvektor  plus V 2, den zweiten Eintrag bei den zweiten Einheitsvektor.  Die Linearkombination mit der Standardbasis.", "start": 761.58, "end": 783.22}, {"text": "  Da haben wir eben hier vorstehenden, diese lineare Abbildung.  Jetzt koennen wir genau die Additivitaet und die Homogenitaet von dieser  linearen Abbildung ausloesen.  Der ist die Additivitaet.  Heisst, das koennen wir einfach trennen.  Wir koennen erst V 1 mal ersten Einheitsvektor abbilden plus  V 2 mal zweiten Einheitsvektor abgebildet.", "start": 783.7, "end": 805.18}, {"text": " Jetzt koennen wir noch die Homogenitaet ausnutzen und dieses V 1,  dieses Skalader vorziehen.  Was Sie jetzt sehen, ist, dass wir im Wedentlichen einfach nur die  Standardbasisvektoren, die Einheitsvektoren abbilden und das Gewicht  V 1 und V 2 mit denen wir hier lineare kombinieren,  und zwar diese abgebildeten Vektoren lineare kombinieren.  Diese Gewichte bleiben gleich.", "start": 805.18, "end": 826.46}, {"text": "  Das Ganze koennte man jetzt noch mal schreiben,  also die geometrische Darstellung von einem Vektor.  Allerdings wuerden sich hier die Gewichte U dann deutlich  von den Gewichten von V 1 unterscheiden,  wenn wir noch mal lineare kombinieren mit den Standardbasisvektoren.  Wir behalten das ja mal im Kopf und gucken uns jetzt die naechste  Vorjahre hier an, habe ich einmal das geziert nochmal.", "start": 826.46, "end": 849.46}, {"text": "  Betrachten wir einmal erst Skalierung, die Alpha Glyde 2,  das heisst wir skalieren ein Vector bei dem Faktor 2.  Wir haben jetzt den Vector 2 1, den Gruenen dargestellt,  also auf diese Matrixraup multiplizieren, dann skalieren wir den  um 2, kommen also genau 4, 2 raus, also die den Blauen Vektor.", "start": 849.46, "end": 871.34}, {"text": " Jetzt koennen wir das eben so schreiben, wie ich es eben getan habe  und koennen einmal S eben auf diese geometrische Darstellung anwenden.  Dann wird 1 0 der erste Einheitsvektor hier in Gruen um 2 skaliert  und wird dann zu dem Blauen Vektor.  Der andere Basisfektor hier auch wieder Gruen wird auch um 2 skaliert  und wieder zu dem anderen Basisfektor.", "start": 871.34, "end": 888.66}, {"text": "  Wenn was jetzt sehe, ist das diese geometrische Darstellung  bzw. diese Linearkombination am Ende wieder bei genau den selben Vektor  fuer 2 rauskommen.  Hier kann ich das verstehen.  Also wenn ich mir 2 1 nehme und um diesen Basisfektor und 2 in diese Richtung gehe  und um 1 in diese Richtung, das andere Basisfektor,  komme ich zusammen, wenn ich bei der Bewegung kombiniere, genau hier raus.", "start": 888.66, "end": 915.66}, {"text": "  Wenn ich jetzt aber erst die Basisfektoren skaliere, also den Blauen Vektor  und dann den anderen Blauen Vektor und jetzt 2 in diese Richtung gehe  und 1 in diese Richtung gehe, dann komme ich eben genau bei den skalierten Vektor raus.", "start": 915.66, "end": 934.66}, {"text": " Als jetzt endlich kann man das einfach so verstehen, dass wir uns einmal angucken,  wie sich halt die Basis veraendert durch die Abbildung  und dadurch auch verstehen koennen, was genau die Abbildung dann linear macht.", "start": 934.66, "end": 946.66}, {"text": " Also wir muessen nur verstehen, wie die Basisfektoren veraendert werden  und wie man das umgangssprachlich vielleicht formulieren kann,  ist, dass die lineare Abbildung quasi den Vektorraum veraendern,  in denen diese Vektoren leben.  Also, Moment.  Hier wieder zu dieser Darstellung.", "start": 946.66, "end": 964.66}, {"text": " Wir veraendern, wir koennen halt uns vorstellen, dass Sie direkt diese Abbildung  aus den Vektor abbilden, koennen uns aber auch einfach vorstellen,  dass Sie halt ein gemeindem Vektorraum skalieren.  Man skaliert sich dieser Vektor, der eben in diesem Vektorraum lebt,  genauso mit, wie sich eben diese Standardbasis mit veraendert.", "start": 964.66, "end": 984.66}, {"text": " Das ist quasi eine erste Form des Basisfeksel  und auf den Basisfeksel kommen wir am Ende nochmal zurueck.  So, jetzt wollen wir uns anschauen, wie wir Matrizen kombinieren koennen.  Also die hintereinander ausgehoeren von linearen Abpuelern.  Wenn mehrere Matrizen haben, zum Beispiel eben eine Rotation und eine Skalierung,  dann koennen wir die durch Matrizenmultiplikation zusammenfassen.", "start": 984.66, "end": 1002.66}, {"text": "  Uns also vorstellen, dass Sie den Vektor V zum Beispiel erst skalieren wollen  durch B und dann rotieren wollen.  Dann koennen wir auch genauso gut erst diese Skalierungsmatrix  mit der Rotationsmatrix multiplizieren,  den einen Matrix und den Vektor durch diese eine Matrix direkt abbilden.  Genau das Gleiche ist, wenn wir sie hintereinander abbilden.", "start": 1002.66, "end": 1024.6599999999999}, {"text": "  Genau, jetzt habe ich wieder die Frage an Euch.  Welches der folgenden drei Gesetze gilt fuer die Matrix-Multiplikation nicht?  Das Kompotativgesetz, das Assoziativgesetz oder das Distributivgesetz.  Die Matrix-Multiplikation ist im Allgemeinen nicht komotativ.  Das heisst, wenn ich zwei Matrizen habe und A mit B multipliziere,  dann ist das im Allgemeinen nicht das Gleiche wie B mal A.", "start": 1024.6599999999999, "end": 1049.6599999999999}, {"text": "  Ein einfaches Gegenbeispiel habe ich hier einmal dargestellt,  und zwar ist bei der Matrix-Multiplikation auch wieder wichtig,  dass die Dimensionen zueinander passen muessen.  Also die 3 mal 2 Matrix A mit der 2 mal 4 Matrix B multiplizieren will,  also A mal B.  Und ich mir vorstellen, dass das da steht, 3 mal 2 mal 2 mal 4.", "start": 1049.66, "end": 1070.66}, {"text": " Diese innen liegenden Dimensionen, also die beiden 2,  muessen eben wieder zueinander passen.  Das funktioniert, wenn ich A mal B schreibe,  dann moechte ich quasi, wenn ich hier dieses Produkt mir angucke,  also zahle mal Spalter.  Erste Zeile, mal Spalter, habe ich jeweils genau zwei Elemente  in dieser Zeile und zwei Elemente in Liter-Schweite,  dann funktioniert es.", "start": 1070.66, "end": 1086.66}, {"text": "  Die beiden Matrizen aber vertauschen,  weil ich in dieser Zeile 4 Eintraege und in dieser Spalter nur 3 Eintraege.  Kann ich nicht das Skalarprodukt zwischen dieser Zeile und dieser Spalter nehmen.  In der Stelle funktioniert die Matrix-Multiplikation also nicht.", "start": 1086.66, "end": 1107.66}, {"text": " Das ist ein gutes Beispiel dafuer, dass nicht komotativ sein kann,  weil das Produkt B mal A gar nicht definiert ist,  wenn das Produkt A mal B sehr wohl definiert ist.  Das Assoziativgesetz gilt.  Auf das Distributivgesetz, weil wir im Folgenden nicht eingehen,  aber das Assoziativgesetz ist nochmal interessant und das gilt auch.", "start": 1111.66, "end": 1128.66}, {"text": " Das heisst, bei mehrfach Multiplikationen,  kann einfach die Reihenfolge der Multiplikation vertauscht werden.  Es macht keinen Unterschied, ob ich erst B mal C multipliziere  und dann das Ergebnis mit A multipliziere  oder ob ich erst A mal B multipliziere  und dann das Ergebnis mit C multipliziere.", "start": 1129.66, "end": 1146.66}, {"text": " Diese Reihenfolge der Multiplikationen auszutauschen  ist sehr sinnvoll fuer die Performance und fuer die Numerik.  Dafuer muessen wir kurz die Laufzeit  von der naive Matrix-Multiplikation angucken.  Die ist M mal M mal K,  das heisst, wenn ich eine 400 und eine 100 mal 4  Matrix miteinander multipliziere,  dann ist quasi N gleich 4, M gleich 100  und K gleich wieder 4.", "start": 1146.66, "end": 1170.66}, {"text": "  Das heisst, die beiden indienenden Dimensionen hier sind beide gleich M.  Wir muessen auch beide gleich sein.  Dann habe ich halt eine Laufzeit von 4 mal 100 mal 4.  Mittlerweile gibt es auch Algorithmen,  die die Matrix-Multiplikation noch effizienter umsetzen.  Wir gehen aber einfach mal als obere Schanke  von der naiven Implementation aus.", "start": 1170.66, "end": 1190.66}, {"text": " Wenn ich mir jetzt angucke, wie viele Operationen  ich dann fuer diese Reihenfolge brauche,  dann muss ich mir erst mal anschauen, wie viele Operationen  ich fuer B mal C brauche.  B mal C sind 100 mal 4 mal 100 Operationen.  100 mal 4 mal 100.", "start": 1190.66, "end": 1212.66}, {"text": " Und dann habe ich am Ende eine 100 mal 100 Matrix  und diese Moment, diese 100 mal 100 Matrix,  moechte ich jetzt wieder multiplizieren  und dann 4 mal 100 Matrix.  Das heisst, ich habe dann 4 mal 100 mal 100 Operationen, die doch.  Dann habe ich insgesamt auf gigantische 80.000 Operationen.  Wenn ich dagegen aber erst A mal B multipliziere,  dann habe ich 4 mal 100 mal 4 Operationen.", "start": 1212.66, "end": 1233.66}, {"text": "  Plus dann 4 mal 4 Matrix mal 4 mal 100 Matrix,  also 4 mal 4 mal 100 Operationen.  Und ich habe jetzt insgesamt nur auf 3.200 Operationen,  also wesentlich weniger, als wenn ich die Matrizen  in der anderen Reihenfolge multipliziere.  In dem Fall waere es also performantstechnisch  und wahrscheinlich auch numerisch, wederlich sinnvoller,  erst A und B miteinander zu multiplizieren.", "start": 1233.66, "end": 1258.66}, {"text": "  Hier habe ich noch ein allgemeines Beispiel  zu der Hintereinanderausfuehrung von Matrizen.  Einmal definieren wir uns hier oben eine Rotationsmatrix  mit 45 Grad.  45 geteilt durch 180 mal Pi,  ergibt dann genau den Winkel im Bogenmass.  Und den geben wir dann ein in eine klassische Rotationsmatrix  mit den Cosinos auf der diagonalen  und Sinos bzw. Minus Sinos auf den anderen beiden Elementen.", "start": 1258.66, "end": 1288.66}, {"text": "  Zusaetzlich haben wir hier noch eine Asitroposkalierung,  also die Fiskalieren, die Dimension mit unterschiedlichen Werten,  die erste Dimension mit 2 skaliert  und die zweite Dimension mit 1 skaliert.  Das heisst, der Vektor wird am Ende nicht nur gesteckt und gestaut,  sondern wir haben noch eine Schere mit da drinnen.", "start": 1288.66, "end": 1312.66}, {"text": " So, dann definieren wir uns jetzt hier  einen einfachen Vektor, 1, 0, das ist quasi der Einheitsvektor,  der erste Basisvektor, Standardbasisvektor.  Und den multiplizieren wir jetzt einmal zuerst mit S,  also wir skalieren erst und dann rotieren wir  und beim zweiten Mal rotieren wir erst und dann skalieren wir.  Jetzt kommen wir uns mit dem Plot an.", "start": 1312.66, "end": 1332.66}, {"text": " Erstmal haben wir die beiden Speitenvektoren  von den beiden Matrizen nochmal dargestellt.  Die Skalierungsmatrix ist die gelbe,  die erste Speitenvektor ist 2,0, also dieser lange gelbe Vektor.  Weitere Speitenvektor ist 0,1, also dieser gelbe Vektor.  Und die Rotationsmatrix ist hier in gruen dargestellt,  das sind dann diese beiden Speitenvektoren.", "start": 1332.66, "end": 1351.66}, {"text": "  Und den originalen Winkel, den originalen Vektor,  haben wir quasi dann direkt hier,  wird es aus dem gelben Vektor liegen.  Und wenn wir jetzt sie erst skalieren,  dann bedeutet das, dass dieser Standardbasisvektor  zuerst gestreckt wird,  quasi genau diesen gelben Speitenvektor hierhin spricht.  Dann rotieren wir den, das heisst,  wir rotieren den zusammen mit den Vektoren.", "start": 1351.66, "end": 1374.66}, {"text": "  Also ihr seht ja, dass das quasi im Uhrzeigersinn  um 45 Grad gedreht ist, diese beiden gruenen Vektoren.  Das Gleiche machen wir jetzt mit den gelben,  um 45 Grad rotieren, gegen den Uhrzeigersinn,  dann landen wir genau bei X.  X ist eben erst skaliert und dann rotiert.  Genauso koennen wir das andersrum machen.", "start": 1374.66, "end": 1396.66}, {"text": " Wenn wir jetzt erst rotieren,  dann rotieren wir den Standardbasisvektor,  quasi genau auf den gruenen Speitenvektor.  Wenn wir jetzt skalieren,  dann ist es eben so,  dass beide Dimensionen unterschiedlich skaliert werden.  Heisst, dann kommt am Ende hier dieser blaue Vektor raus.  Wenn wir jetzt sie erst skalieren, den Vektor 1,0,  dann ist dieses zweite Dimensionen ja sowieso 0.", "start": 1396.66, "end": 1417.66}, {"text": "  Das heisst, es skalieren eigentlich nur die erste Dimensionen auf 2,0.  Und rotieren dann, dann ist kein Problem.  Dadurch, dass sie hier erst rotieren,  haben wir hier quasi 0,72 und 0,72 stehen.  Da ist beide Dimensionen sympathisiert.  Und wenn wir die jetzt unterschiedlich skalieren,  dann haben wir eben hier mit diese Scherungen drin.  Moechte ich nochmal zwei Begriffe einfuehren.", "start": 1417.66, "end": 1438.66}, {"text": "  Einmal den Spannen.  Der Spannen ist die Menge aller Linearkombinationen  aus einer Liste von Vektoren in einem Vektoraum.  Heisst, angenommen, ich habe in diesem Spannen  drei Vektoren mit vier Dimensionen.  Dann ist der Spannen die Menge aller Vektoren,  die ich durch Linearkombinationen  von den drei Vektoren erzeugen kann.", "start": 1438.66, "end": 1465.66}, {"text": " Da ich aber jetzt in einem vierdimensionalen Vektoraum lebe,  aber nur drei Vektoren habe,  kann ich damit maximal in drei-dimensionalen Unterraum-Linearkombinieren.  Und der zweite Begriff, den ich einfuehren moechte,  ist die Linear-Rottenabhaengigkeit.  Linear-Rottenabhaengigkeit bedeutet,  dass ein Vektor nicht linear aus anderen Vektoren kombiniert werden kann.", "start": 1465.66, "end": 1487.66}, {"text": " Also, wenn ich ein Vektor V habe,  wenn nicht linear aus un- und-we kombinieren kann,  dann ist V Linear-Rottenabhaengigkeit von un- und-we.  Dann ist das genau mit dieser Definition  von den Spannenzusammenbringern.  Der Spannende ist genau die Menge aller Vektoren  nicht aus un- und-we linear kombinieren kann.", "start": 1487.66, "end": 1505.66}, {"text": " Wenn V jetzt nicht in den Spannen liegt,  dann muss V Linear-Rottenabhaengigkeit von un- und-we sein.  Jetzt wieder eine Aufgabe an euch.  Und zwar gibt mir mal bitte ein geometrisches Beispiel  fuer drei Vektoren an,  die im R3 leben, also drei Dimensionen haben,  und einen Unterraum aufsparen, also einen zwei-dimensionen Unterraum.  Kurze Pause und dann eine Loesung anschauen.", "start": 1505.66, "end": 1528.66}, {"text": "  Eine moegliche Loesung waere das hier.  Ich habe Z gleich 0 gesetzt,  das heisst, ich betrachte diese XY-Ebenen.  Das kann nicht mehr einfach drei Vektoren suchen,  die genau in diese XY-Ebenen liegen, zum Beispiel diese drei hier.  Die drei Vektoren untereinander sind jetzt nicht linear unabhaengig,  aber sie spannen eben diesen zwei-dimensionalen Unterraum  von drei-dimensionalen Raum auf.", "start": 1528.66, "end": 1550.66}, {"text": "  So als anderes Beispiel, das sind nicht die drei Blauen Vektoren  hier in der Zeichnung, kann ich halt sagen 1 0 0 0 1 0 und 1 1 0.  Dann habe ich drei dreidimensionale Vektoren  in den zwei-dimensionalen Unterraum aufsparen,  aber die sind eben nicht linear unabhaengig.  Das geht auch gar nicht, weil sie eben in einen zwei-dimensionalen Raum liegen.", "start": 1550.66, "end": 1570.66}, {"text": "  Als 1 1 0 kann ich ohne Probleme aus 1 0 0 und 0 1 0 linear kombinieren,  indem ich einfach beide Vektoren zusammen mit hierin,  dann komme ich genau bei den dritten Vektoren raus.  Anerwortend, dadurch dass sie in den zwei-dimensionalen Unterraum leben,  kann ich keinen Vektor linear kombinieren aus den Vektoren,  weil quasi aus dieser Ebene heraus zeigt.", "start": 1570.66, "end": 1594.66}, {"text": " Ja, dann ebenfalls sehr sehr wichtig,  was wir vor allem im Kontext von der Hauptkomponentenanalyse sehen werden,  sind Eigenwert und Eigenvektor.  Die Spiegel im Wichtige Eigenschaften von den nernen Abhildern wieder,  die charakteristische Gleichunterhinter war quasi a mal v ist gleich lambda mal v.", "start": 1594.66, "end": 1613.66}, {"text": " Als der Vektor v wird durch a abgebildet  und am Ende kommt dieser Vektor v skaliert bei den Eigenwertlammen daraus.", "start": 1614.66, "end": 1631.66}, {"text": " Der 0-Vektor ist kein Albenvektor, das werden die dreidiale Loesungen,  wenn ich hier mal 0 einsetze, dann geht die Gleichung offensichtlich,  unabhaengig von den Eigenwert, unabhaengig von der Matrix,  das ist komplett unspezifisch diese Loesung,  und deswegen haben wir halt der 0-Vektor, soll keine Eigenvektor sein.", "start": 1633.66, "end": 1641.66}, {"text": " Jetzt wieder eine Frage an euch, was bedeutet diese Gleichung geometrisch?  Was passiert mit dem Vektor v, wenn er ein Eigenvektor ist geometrisch?  Der Eigenvektor ist genau ein Vektor, der von der linaren Abhildung eben nur skaliert wird.  Ich buendige den Vektor v durch a ab,  und am Ende kommt genau der gleiche Vektor v raus,  nur eben skaliert bei dem Wert lambda.", "start": 1643.66, "end": 1662.66}, {"text": "  Also durch diese Abhildung a wird dieser Vektor v entweder nur gestreckt, gestaucht,  oder sogar noch invertiert, abhaengig von dem Wert lambda.  Also lambda 2, dann skalieren wir v um den Faktor 2,  bis lambda minus 3, dann strecken wir halt um den Faktor 3  und invertieren noch zusaetzlich.  Wer wie stark der Vektor skaliert wird, haengt eben von den Eigenvektoren ab.", "start": 1662.66, "end": 1686.66}, {"text": "  Der Eigenwert gibt uns halt an, wie stark dieser Eigenvektor  skaliert wird von dieser Abhildung.", "start": 1686.66, "end": 1711.66}, {"text": " Spaeter werden wir sehen, dass Eigenvektoren von Bildern mit,  also wir werden Eigenvektoren von Bildern mit Gesichtern berechnen,  wir werden sehen, dass diese Eigenvektoren dann eine Basis bilden,  und er wird die Bilder, die ordinate Vektoren von den Bildern  auf eine besonders interessante Art und Weise interpretieren koennen.  Das habe ich hier nochmal zwei Beispiele.", "start": 1714.66, "end": 1721.66}, {"text": "  Einmal definiere ich dafuer in dem ersten Beispiel die Matrix hier  mit den Spreitenvektoren 1, 2 und 2 minus 1.  Sie stehen autokonal aufeinander in diese Spreitenvektoren wieder.  Hier unten sind die dargestellt durch die Liedernendenvektoren.  Und dann nicht von dieser Matrix,  sondern von dieser Matrix mit die Eigenwerte berechnen.", "start": 1721.66, "end": 1741.66}, {"text": "  Das kann ich tun mit dieser Operation NAMP, die naehere Algebra.eigen.  Wirklich genau diese Eigenwerte, die Eigenvektoren.  In BAL, das ist jetzt ein Vektor mit den Eigenwerten.  Und da stehen halt die Eigenwerte drin,  und in IV stehen dann in den Spreiten die Eigenvektoren drinnen.  Das ist quasi eine 2x2 Matrix.", "start": 1741.66, "end": 1766.66}, {"text": " Bei einer 2x2 Matrix A gibt es genau zwei Eigenvektoren,  die wirklich genau diese Eigenvektoren in den Spreiten drinnen.  Und wie wir vorhin gesehen haben,  koennen wir mit den doppelpunkten Index auf die Spreiten zugreifen.  Das heisst, hier picken wir uns dann quasi die ersten und den zweiten Eigenvektoren.  Das wollen wir das Matrix-Vektor-Produkt anwenden.", "start": 1766.66, "end": 1783.66}, {"text": "  Das heisst, wir multiplizieren die Eigenvektoren auf die Matrix drauf  und kriegen genau V0 raus.  Und V0 muesste ja jetzt eigentlich nur dieser Eigenvektor sein,  eben skaliert bei diesem Eigenwert, der im Wald drinnen steht.  the same fuer den anderen Eigenvektor,  wenn ich den jetzt auch auf A multipliziere,  dann kriege ich wieder den selben Vektor raus, nur eben skaliert.", "start": 1783.66, "end": 1804.66}, {"text": "  So, da sehen wir hier unten relativ deutlich.  Hier pfinte ich einmal die beiden Vektoren,  nachdem sie abgewedet wurden.  Und ich nehme die beiden Eigenvektoren  und multipliziere sie einmal mit ihren Eigenwert.  Und wie wir sehen, kommen halt bei beiden genau das gleiche raus.  Das heisst, wir fuellen genau diese Gleichung, die wir eben gesehen haben.", "start": 1804.66, "end": 1826.66}, {"text": "  So, dann haben wir hier nochmal diese grafische Veranschau-Loechung.  Also lila waren wie gesagt die Spaltenvektoren von dem Matrix A.  Und in gelb haben wir die beiden Eigenvektoren dargestellt.  Und wenn wir jetzt diesen Eigenvektor hier auf die Matrix A abbilden,  dann kreben wir genau den Vektor nur skaliert,  das heisst, der Vektor wird dann einfach skaliert.", "start": 1826.66, "end": 1848.66}, {"text": "  Und mit dem Fall von dem anderen Eigenvektor wird dieser Vektor auskaliert,  also gestreckt, aber dann noch invertiert.  Jetzt habe ich hier nochmal eine grafische Veranschau-Loechung davon.  Und zwar, das erklaere ich einmal ganz kurz,  hier haben wir wieder die beiden Eigenvektoren, das sind genau die gleichen Eigenvektoren wie davor.  Und zusaetzlich haben wir den roten Vektor.", "start": 1848.66, "end": 1871.66}, {"text": "  Der roten Vektor ist immer ein Einheitsvektor.  Und zwar rotiert er hier so ein bisschen, das heisst, wir betrachten alle moeglichen Einheitsvektoren  fuer alle Winkel von 0 bis 360 Grad und den Ursprung herum.  Und der gruene Vektor ist dann quasi dieser rote Vektor abgebildet durch diese Matrix A.", "start": 1871.66, "end": 1892.66}, {"text": " Also haben wir den roten Vektor auf A raufmultiplizieren,  dann koennen wir den gruenen Vektor hier raus.  Der rote Vektor ist ein Eigenvektor.  Und wenn wir jetzt diese Animationen betrachten,  und dieser rote Vektor sich quasi herum bewegt, dann gibt es vier interessante Punkte.  Der erste Punkt ist diese hier, wo wir genau den Eigenvektor treffen,  und die Abbildung quasi genau skaliert.", "start": 1892.66, "end": 1913.66}, {"text": "  Und zwar ist es eine Streikung und dann eben die ...  ... Inverse, also der invertierte Vektor.  Der zweite Punkt ist genau der Punkt, wo wir quasi genau diesen Vektor hier wieder treffen,  den anderen Eigenvektor, der wird einfach nur gestreckt.", "start": 1913.66, "end": 1940.66}, {"text": " Und jetzt gibt es noch zwei weitere interessante Punkte, und da wieder die Frage an euch, welche Punkte sind das?  Das sind genau die Punkte, wo der rote Vektor im Gegengesetz von der Richtung von einem Eigenvektor zeigt.", "start": 1940.66, "end": 1951.66}, {"text": " Und zwar, seit der rote Vektor hier genau in die andere Richtung von dem Eigenvektor,  und der rote Vektor wird trotzdem wieder nur invertiert und skaliert, also invertiert und gestreckt.  Das heisst, der rote Vektor hier ist wieder ein Eigenvektor, und das ergibt auch Sinn,  weil alle Vielfachen von einem Eigenvektor auch Eigenvektoren sind.", "start": 1951.66, "end": 1965.66}, {"text": "  Heisst, ich kann diesen Eigenvektor hier beliebig strecken, stauchen, invertieren,  und er bleibt ein Eigenvektor, das heisst, wenn ich ihn wieder auf die Matrix anwende,  wird er nur weiter gestreckt, gestaucht oder invertiert.  Genau das gleiche wieder fuer den anderen.", "start": 1965.66, "end": 1984.66}, {"text": " Wenn ich hier genau im Gegengesetz von dem anderen Eigenvektor zeige,  dann habe ich wieder einen Eigenvektor, weil auch egal, wie ich den Eigenvektor strecke oder stauche,  wenn ich ihn abbilde durch die Matrix, kriege ich wieder nur den gestreckten oder gestauchten Vektor raus.  Genau, nochmal zu den Code, den muesst ihr an der Stelle wieder nicht verstehen.", "start": 1984.66, "end": 1999.66}, {"text": "  Der verstehe quasi nur diese Animationen.  Aber inhaltlich ist es quasi genau das Gleiche, wie wir es eben davor hatten.  Lieben Beispiel.  Eine wichtige Operation, die aber sicher noch gut in Erinnerung hat, dass die Transponierter,  das Transponieren von einem Vektor oder einer Matrix, vertauscht nur Zeilen und Spalten.", "start": 1999.66, "end": 2026.66}, {"text": " Heisst aus den Zeilen, in dem Fall 1v3, wird die erste Spalte,  und aus der zweiten Zeile 4,5,6 wird die zweite Spaltung.  Heisst aus einer MIM-Matrix, wird eine MIM-Matrix.  Es gelten zwei wichtige Gesetze, und zwar hebt sich die Transposition von den anderen Transponiertern auf.  Also A transponiert, mal und wieder transponiert, ist wieder einfach A.", "start": 2026.66, "end": 2045.66}, {"text": "  Also wenn ich einmal die Zeilen zu den Spalten mache, und die Spalten wieder zu den Zeilen,  dann kriege ich wieder die urspruengliche Matrix.  Weite wichtige Regel, die euch unbedingt merken solltet, ist, dass ich dieses Transponieren nach innen sehen kann,  aber dann das Matrix-Matrix-Bedot vertauschen muss.", "start": 2045.66, "end": 2065.66}, {"text": "  Also A mal B transponiert, ist genau das Gleiche, wie B transponiert, mal A transponiert.  Fuer autogonale Matrizen geht ausserdem die Inverse auch hoch-1 der transponierten,  autogonale Matrizen und besondere Matrizen, die unter anderem diese Eigenschaft haben.  Darauf gehe ich am Ende auch noch mal.", "start": 2066.66, "end": 2088.66}, {"text": " So, dann haben wir jetzt eben schon die Inverse angesprochen,  die Inverse ist nur definiert fuer quadratische Matrizen.  Also die Matrix muss quadratisch sein, also M M M, und das weitere muss die Matrix einen vollen Rang haben.", "start": 2088.66, "end": 2107.66}, {"text": " Das bedeutet, also Ragen von A ist genau M bei einer M M M Matrizen,  und geht damit einher, dass die Determinante von 0 verschieden sein muss,  und dass alle Eigenwerte ungleich 0 sein muss.  Genau dann haben wir vollen Rangen, und genau dann koennen wir halt eine Inverse berechnen.", "start": 2107.66, "end": 2123.66}, {"text": " Die Inverse ist dann definiert, dass genau die Matrix A invertiert,  die multipliziert mit der urspruenglichen Matrix A genau die Identitaet ergibt.  Die Inverse kann dann zum Beispiel genutzt werden, um ein generisches Gleichungssystem zu loesen,  heisst, wir suchen ein Vektor X, der abgebildet auf A genau den Vektor B ergibt.", "start": 2125.66, "end": 2138.66}, {"text": "  Dann koennen wir jetzt hier von der linken Seite die Inverse multiplizieren,  dann haben wir A inverse mal A, das ergibt genau die Identitaet nach Definition,  und dann bleibt hier einfach auf der linken Seite X stehen.", "start": 2138.66, "end": 2159.66}, {"text": " Jetzt kann ich hier auf der rechten Seite, habe ich dann quasi auch  von der linken Seite A ran multipliziert, als was uebrig bleibt,  das ist genau X, ist gleich die Inverse von A mal B.  Wir werden aber sehen, dass es noch effizientere und nemerisch stabilere Methoden gibt,  um diese Inverse zu berechnen.  Nicht um die Inverse zu berechnen, sondern um unser Gleichungssystem zu loesen.", "start": 2159.66, "end": 2175.66}, {"text": "  Dann haben wir eben schon angesprochen, die Determinante,  die muss von 0 verschieden sein, damit wir vollen Rang haben.  Und die Determinante ist auch wieder nur definiert fuer quadratische Matrizen,  und bildet die Matrix dann auf eine reale Zahl ab.  Fuer 2 mal 2 und 3 mal 3 Matrizen gibt es feste Regen,  hat hier die Determinanten berechnet werden.", "start": 2175.66, "end": 2194.66}, {"text": "  Ich habe hier einmal die Regel fuer eine 2 mal 2 Matrizen geschrieben,  da ist die Determinante definiert nach A mal D minus B mal C,  das ist genau die Determinante.", "start": 2194.66, "end": 2215.66}, {"text": " Fuer 3 mal 3 moechte ich da jetzt nicht drauf eingehen,  und fuer N groesser 3 muesste man zum Beispiel den Laplace Entwicklungssatz verwenden,  dort gehen wir nicht ein, weil die Berechnung von Determinanten  ist immer ein allgemein relativ aufwendig,  und wir werden das wahrscheinlich fuer wissenschaftliches Rechnen  nicht sehr intensiv brauchen.", "start": 2215.66, "end": 2225.66}, {"text": "  Genau, den Laplace Entwicklungssatz sollte die aber auch schon kennen  aus der Linan-Ei-Gebra.  Jetzt stellen wir uns natuerlich auch wieder die Frage,  was bedeutet die Determinante geometrisch,  und zwar entspricht die Determinante genau dem Faktor der Volumenaenderung  der Transformation von einer Linan-Appelung.", "start": 2225.66, "end": 2247.66}, {"text": " Ganz weit runtergebrochen habe ich hier ein 2-dimensionales Beispiel,  und ein 2-dimensionales ist eben ein Volumen,  einfach nur eine Flaeche,  und genau, jetzt habe ich hier mal 2 Vektoren aufgeschrieben,  oder gezeichnet, V1 und V2,  und beiden Vektoren spannen jetzt ein Parallelogramm auf,  und deshalb hat dieses Parallelogramm einen bestimmten Flaechenden.", "start": 2247.66, "end": 2265.66}, {"text": "  Dann habe ich diese beiden Vektoren eben durch diese Abbildung A abbilden,  und dann kriege ich ein anderes Parallelogramm,  aufgeschwannen von V1-Stich und V2-Stich.  Das ist das andere Parallelogramm kann,  oder hat jetzt wahrscheinlich einen anderen Flaecheninhalt.  In der Faktor, mit der sich dieses Volumen bzw.", "start": 2265.66, "end": 2284.66}, {"text": "hier ein 2-dimensal  in dieser Flaecheninhalt veraendert durch Abbildung mit dieser Matrix A,  dieser Faktor ist genau gegeben durch den Betrag der Determinante.", "start": 2284.66, "end": 2302.66}, {"text": " So, jetzt gibt es zwei interessante Faelle,  und zwar kann es sein, dass die Determinante gleich 0 ist,  jetzt haben wir eben schon gelernt, wenn die Determinante gleich 0 ist,  dann haben wir keinen vollen Rang,  und wir koennen die Matrix nicht invertieren.", "start": 2302.66, "end": 2312.66}, {"text": " Geometrisch in diesem Fall bedeutet das, dass es ein Vektor V gibt,  der im Kern von dieser Matrix A liegt,  das heisst, auf den 0-Vektor abgebildet wird.  Und wenn das passiert, und ein Vektor auf den 0-Vektor abgebildet wird,  dann verschwindet dieses Volumen oder diese Flaeche ja komplett,  und am Ende kommt eine Flaeche mit 0 Flaecheninhalt raus, also quasi keine Flaeche.", "start": 2312.66, "end": 2329.66}, {"text": "  Das heisst, wenn die Determinante gleich 0 ist,  dann bedeutet das, dass so ein Volumen einfach kollabiert,  und der Bildraum, das heisst, die Menge am Vektor auf die A abbilden kann,  hat eine geringere Dimension.  Jetzt kommen wir zum letzten Thema,  aber mit eines sehr wichtigsten Themen,  und zwar der Basen bzw.", "start": 2329.66, "end": 2354.66}, {"text": "der Basiswechsel,  und in dem Kontext,  wenn wir noch mal auf autokonale Matrizen zu sprechen kommen.  Eine Liste linear unabhaengiger Vektoren heisst Basis von einem Vektorraum,  wenn dieser durch die Vektoren aufgespannt wird.  Heisst, in einem 2-dimensionen Fall brauche ich mindestens 2 Vektoren,  und diese 2 Vektoren spannen dann,  wenn sie linear unabhaengig voneinander sind, den Vektorraum auf.", "start": 2354.66, "end": 2376.66}, {"text": "  Einfachste Beispiel ist einfach immer die Standardbasis 1, 0 und 0, 1.  Die sind beide linear unabhaengig  und spannen halt den 2-dimensionen Vektorraum auf.  Eine andere Basis habe ich hier mal hingeschrieben,  2, 1 und minus 2 und 4.  Die beiden Vektoren stehen auch wieder autokonal aufeinander,  aber sie sind nicht autonomal zueinander.", "start": 2376.66, "end": 2400.66}, {"text": " Das heisst,  autonomal waere diese Basis genau dann,  wenn die beiden Basis-Vektoren noch einheitslaenger haben.  Die Standardbasis ist autonomal,  weil beide Vektoren stehen autokonal aufeinander und haben laenger 1.  Hier stehen die beiden Vektoren nur autokonal aufeinander,  haben eine Laenge verschieden von 1,  das heisst, wir haben nur eine autokonale Basis,  aber keine autonormale Basis.", "start": 2400.66, "end": 2420.66}, {"text": "  Jetzt habe ich mir hier ein Vektor ausgedacht, 4, 7,  und den kann ich jetzt wieder schreiben  in dieser geometrischen Linearkombination aus den Basis-Vektoren.  Also 4 mal 1, 0 plus 7 mal 0, 1 ist genau dieser Vektor 4, 7.  Heisst, ich habe eine Linearkombination  mit den Basis-Vektoren von der Standardbasis.", "start": 2420.66, "end": 2442.66}, {"text": " Da gibt es genau einen Vektor,  und was ich auch machen kann, ist diese Basis-Vektoren  in die Spalten einer Matrix zu schreiben,  dann ich den Vektor 4, 7 hier rauf multiplizieren  und kriege halt mein Vektor raus.  Das habe ich ja noch diese andere Basis.", "start": 2442.66, "end": 2459.66}, {"text": " Im Vektor 4, 7 kann ich auch Linearkombination aus dieser Basis,  sonst waere es keine Basis, weil ich eben nicht jeden Vektor  aus den 2-dimensionalen Vektorraeumen linearkombinieren kann.  Das eine Basis ist, geht das,  und mit 3 mal 2, 1 plus 1 mal minus 2 und 4  komme ich ja wieder genau auf den Vektor 4, 7.", "start": 2459.66, "end": 2474.66}, {"text": " Genauso wie eben auch, kann ich das jetzt  aus Matrix-Vektorprodukt schreiben,  heisst, ich schreibe die Basis-Vektoren in die Spalten  dieser Matrix, multipliziert mit dem Vektor 3, 1,  komme ich dann eben wieder genau auf den Vektor 4, 7.", "start": 2474.66, "end": 2492.66}, {"text": " Und das, was ich jetzt hier gemacht habe,  ist genau ein Basiswechsel,  heisst der Vektor 3, 1,  das ist ja erstmal grundsaetzlich ein Vektor,  der verschieden von 4, 7 ist, also ein anderer Vektor.  Aber wenn ich den Vektor 3, 1,  also nur dann, wenn ich den Vektor 3, 1  bezueglich der Standardbasis verstehe,  ist es ein Vektor, der von 4, 7 verschieden ist.", "start": 2492.66, "end": 2505.66}, {"text": "  Wenn ich den Vektor 3, 1, aber bezueglich der Basis 2  hier auffasse, und auf den Vektor,  auf die Matrix hier rauf multipliziere,  die in den Spalten die Basis-Vektoren hat,  dann komme ich wieder genau bei dem Vektor 4, 7 raus.  Heisst,  dieser Vektor 3, 1 lebt in einer anderen Basis,  in dieser Basis,  beschreibt aber quasi genau den selben Vektor,  wie 4, 7 bezueglich der Standardbasis.", "start": 2505.66, "end": 2532.66}, {"text": "  Eine Veranwendung wird so oft interessant sein,  mit verschiedenen Basen zu arbeiten,  zum Beispiel eben bei den Bildern von den Gesichtern,  koennen wir die Bilder einmal in der Standardbasis angeben,  das heisst in den ueblichen Format, in dem wir Bilder vorliegen haben,  wir koennen die aber auch in die Eigenbasis transformieren,  also einen Wechsel in die Eigenbasis vornehmen.", "start": 2532.66, "end": 2552.66}, {"text": "  Der Vorteil ist, dass wir dann eben diese Koordinaten,  die Rezentation von dem Vektor, also die einzelnen Werte in dem Vektor,  in andere Art und Weise interpretieren koennen.", "start": 2552.66, "end": 2576.66}, {"text": " Also die Koordinaten in Eigenbasis koennen umwie verstaendlicher sein,  das heisst, das kann in diesem simplen Beispiel hier sein,  dass sich die 3, 1 in einer anderen Basis  besser interpretieren kann,  dass die 4, 7,  auf wenn die 3, 1 bezueglich der Basis quasi genau den selben Datenpunkt beschreibt,  wie diese 4, 7.", "start": 2576.66, "end": 2586.66}, {"text": " Da ist es wichtig, zwischen verschiedenen Koordinaten transformieren zu koennen,  das heisst, wir wollen einen Wechsel von der Darstellung in einer Basis  zu der Darstellung in einer anderen Basis  und das dann eben genau dieser Basiswechsel,  geometrisch veraendern sich damit eben die Koordinatenachsen des Vektoraums.", "start": 2586.66, "end": 2602.66}, {"text": "  Das ist die Koordinatenachsen, hier bei der Standardbasis sind eben die Standardachsen  und hier kann ich eben den Vektor 2, 1  und dem Vektor minus 2, 4 als die Achsen dann von den Koordinatensystemen auffassen.  Damit transformiere ich oder veraender ich quasi den kompletten,  dieses komplette Koordinatensystem.", "start": 2602.66, "end": 2625.66}, {"text": "  Ja, Spaltenvektoren von Matrizen mit vollen Rang koennen dann als Basis interpretiert werden.  Das ist quasi genau die andere Berichtung,  wenn ich mir das anschaue, die Matrizen haben vollen Rang,  das heisst, die Spaltenvektoren sind linier unabhaengig voneinander,  das heisst, wir haben genau eine Basis, die in Spalten von dieser Matrik stehen.", "start": 2625.66, "end": 2647.66}, {"text": " Und diese Matrix, die Matrix mit vollen Rang,  transformiert dann zwischen zwei Baden, genau das, was Sie hier gesehen haben.  Wenn ich 3, 1 auf diese Matrix raubt, multipliziere,  dann wird dieser Vektor 3, 1 transformiert im Vektor 4, 7 bezueglich der Standardbasis.  Besondere Basiswechsel-Matrizen, dann eben die autogonalen Matrizen,  da hatte ich versprochen, dass ich das nochmal anspreche.", "start": 2647.66, "end": 2667.66}, {"text": "  Autogonale Matrizen sind dadurch definiert,  dass die Spaltenvektoren einmal Einheitslaenge haben  und die Spaltenvektoren autogonal zueinander stehen.  Als die in beiden Eigenschaften folgt dann,  dass die Inverse genau die Transponierte ist.  Das macht zum Beispiel die Ruecktransformation  zur verheerenden Basis besonders einfach.", "start": 2667.66, "end": 2688.66}, {"text": " Ich kann ein Basiswechsel vornehmen,  indem ich ein Vektor auf diese autogonale Matrix multipliziere.  Wenn ich dann wieder zurueckwechsel moechte in die andere Basis,  muss ja diese Transformation, diese Abbildung genau rueckgaengig machen.  Heisst, ich moechte mit den Inversen arbeiten,  also genau diese Abbildung invertieren.", "start": 2688.66, "end": 2705.66}, {"text": " Weil die Inverse aber genau die Transponierte ist,  kann ich einfach den abgebildeten Vektor nochmal  mit der Transponierten multiplizieren  und dann lande ich wieder in meiner urspruenglichen Darstellung.  Geometrisch sind autogonale Matrizen Rotation und Reflektion.  Das haengt wieder mit der Determinante zusammen.  Also autogonale Matrizen sind Winkel und Laengentreu.", "start": 2705.66, "end": 2728.66}, {"text": "  Und genau dann eine Rotation, wenn die Determinante gleich eins ist  und genau dann, wenn die Determinante minus eins ist,  dann habe ich dann noch eine Reflexerumme drinnen.  Wurzdem bleiben sie Laengen und Winkeltreuen.  Darauf kommen wir aber im Semester wahrscheinlich nochmal genauer zu sprechen.  Autogonale Matrizen haben die wunderbare Eigenschaft,  dass sie nomaerisch besonders stabil sind.", "start": 2728.66, "end": 2748.66}, {"text": "  Wir koennen nomaerisch besonders gut damit arbeiten.  Zum Beispiel, weil wir keine komplexen Operationen umsetzen muessen,  um die Inverse zu berechnen, sondern einfach nur transponieren koennen.  Das ist einfach schweiten und Zeilen vertauschen.  Es geht ganz einfach.  Die nomaerische Genauigkeit von den Werten bleibt einfach erhalten.", "start": 2748.66, "end": 2767.66}, {"text": " Der Zukunft hat weitere interessante Eigenschaften,  auf die ich jetzt hier nicht eingeben moechte.  Jetzt habe ich nochmal ein Beispiel.  Ich glaube, es ist das letzte Beispiel,  was aber das mit dem Basiswechsel nochmal ganz cool zeigt.  Einmal nehme ich mir die Standardbasis wieder.  MPi erzeugt mir die Identitaet.  In den Spaltenvektoren stehen einfach die Standardbasis-Fektoren.", "start": 2768.66, "end": 2789.66}, {"text": "  Dann definiere ich mir ausserdem die Matrix B1.  B1 hat in den Spalten stehen minus eins eins und minus eins und minus eins.  Bei den Vektoren stehen grundsaetzlich erstmal autokonal aufeinander.  Aber die haben keine Einheitslaenge.  Das heisst, es ist keine autokonale Matrix,  aber jeglich eine Matrix mit autokonalen Spaltenvektoren.", "start": 2789.66, "end": 2816.66}, {"text": " Dann die Matrix B1, aber ganz einfach autokonal machen,  indem ich einmal durch die Wurzel 2 teile.  Dadurch bringe ich dann die beiden Spaltenvektoren auf Einheitslaenge,  also die Vektornormen von den Spaltenvektoren  waere quasi minus eins um Quadrat plus eins um Quadrat,  das ist genau Wurzel 2.  Also minus eins Quadrat plus eins Quadrat ist erstmal einfach 2.", "start": 2816.66, "end": 2835.66}, {"text": "  Und die Normen davon werden die Wurzel,  das heisst, ich komme auf Wurzel 2,  teile ich dadurch, habe ich dann die Standard,  eine autokonale Matrix B1.  Dann schnappe ich mir den Vektor V.  Ich habe den Vektor V, das 1,5, jetzt bezueglich der Standardbasis aufgefasst.  Und den moechte ich auch einmal normieren,  einfach damit der Einheitslaenge hat.  Jetzt habe ich den Vektor V1.", "start": 2835.66, "end": 2860.66}, {"text": "  Und jetzt gucken wir uns mal ganz kurz schon mal die Plot an.  Hier habe ich erstmal die beiden Spaltenvektoren  von diesen Basen dargestellt.  Heisst, B0 ist die Standardbasis,  diese lielernen Vektoren,  erster Basisvektor, zweiter Basisvektor.  Und B1 ist hier die zweite Basis,  erster Basisvektor, zweiter Basisvektor.", "start": 2860.66, "end": 2882.66}, {"text": " Wesentlichen sehe ich, dass diese zweite Basis  einfach in der Rotation von dieser ersten Basis ist.  Und was ich jetzt mache, genau den Vektor V,  haben wir hier in so einen Tuerkees dargestellt,  oder im dunklen Gruen, was auch immer.  Und das erste, was ich jetzt mache,  ist den Vektor V auf die Matrix B1 multiplizieren,  also auf diese andere Basis.  Dann kriege ich V1 raus.", "start": 2882.66, "end": 2901.66}, {"text": "  Und was jetzt hier passiert,  ist einfach der Basisvektor.  Ich rotiere quasi den Vektor in die neue Basis.  Ich mir das vorstellen kann,  ich nehme dieses gesamte Koordinatensystem  mit den Basisvektoren und rotiere  das gesamte Koordinatensystem in diese neue Basis.", "start": 2901.66, "end": 2924.66}, {"text": " Heisst, was ihr erkennen ist,  dass dieser gruene Vektor genau dasselbe Verhaeltnis  zu deinen Basisvektoren hat,  wie der tuerkisfarbene Vektor zu seiner Standardbasis.  Wir rotieren also quasi dieses gesamte System.  Und das ist genau dieser Basisvektor.  Das heisst, bezueglich der Basis sind die beiden Vektoren,  tuerkis und gruen quasi genau gleich.", "start": 2924.66, "end": 2939.66}, {"text": "  Wenn wir beide Vektoren aber bezueglich der Standardbasis aufpassen,  dann zeigt der tuerkisfarbene Vektor natuerlich  in eine voellig andere Richtung als der gruene Vektor.  Dann koennen wir den ganzen Spiess noch einmal umdrehen.", "start": 2939.66, "end": 2962.66}, {"text": " Wenn wir uns jetzt vorstellen,  dass wir diesen tuerkisfarbenen Vektor,  dass der jetzt lebt in dieser anderen Basis,  in der dunkelblauen Basis hier,  und wir den jetzt wieder zuruecktransformieren wollen  in die Standardbasis,  dann muessen wir den Basiswechsel ja rueckgaengig machen.  Eben haben wir den Basiswechsel vorzogen,  indem wir einfach mit der Matrix multipliziert haben.", "start": 2962.66, "end": 2973.66}, {"text": "  Jetzt wollen wir diesen Matrixwechsel genau rueckgaengig machen.  Also ich kann einfach direkt B1,  ja genau rueckgaengig machen,  kann ich das durch die Inverse.  Also ich kann B1 das invertieren,  das durch die Funktionen,  Nampeln, die naeherer eingebraucht, invertieren.", "start": 2973.66, "end": 2996.66}, {"text": " Dann kriege ich die Inverse von B1,  und wenn ich jetzt den Vektor V wieder mit der Inverse multipliziere,  kriege ich V2,  V2 ist dieser gelbe Vektor.  Kann ich mir quasi vorstellen,  dass diese Standardbasis mit den gruenen Vektor,  der tuerkisfarbene Vektor,  wieder komplett zurueckdrehen.", "start": 2996.66, "end": 3010.66}, {"text": " Und wenn ich diese Basis hier wieder raufdrehe  und diesen gruenen Vektor,  tuerkisfarbene Vektor, sorry, mitdrehen,  dann komme ich eben wieder am Ende bei den gelben Vektor raus.  Dann habe ich den gelben Vektor,  bezueglich der Standardbasis.  In der bezueglich der Standardbasis,  dieselbe Position wie der tuerkisfarbene Vektor,  bezueglich der anderen Basis.", "start": 3010.66, "end": 3029.66}, {"text": " Der Basisfaktor, jetzt haben wir gesagt,  B1 ist eine autogonale Matrix.  Anstatt von der Inverse,  koennte ich auch hier einfach transponieren,  das heisst ich koennte es einfach sagen, B1,  und von Spose,  kann ich auch Kot schreiben, einfach als B.T,  das ist quasi die transponierte,  Punkt Dott,  dann erhalte ich genau das gleiche Ergebnis.", "start": 3029.66, "end": 3051.66}, {"text": " Dadurch, dass B1 im autogonalen ist,  ist die Inverse eben genau die transponierte.  Das mache ich aber jetzt hier an der Stelle nochmal rueber.  So weit sind wir aber mit dem Video durch,  das war der Basisfaxel.  Naechste Woche gibt es dann nochmal Live-Tutorials  und Sprechstunden zu diesem Thema.", "start": 3051.66, "end": 3069.66}, {"text": " Und danach in der Woche  werden wir dann starten mit den ersten  inhaltenden Themen mit den Gleitkommunzahlen.  Bis dahin, viel Erfolg  und dann bis zum naechsten Tutorialsvideo.  Oder bis zum naechsten Sprechstunden.", "start": 3069.66, "end": 3076.66}]}]