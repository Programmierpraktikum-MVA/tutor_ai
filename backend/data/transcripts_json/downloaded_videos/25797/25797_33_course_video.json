[{"lecture": "25797_33_course_video", "Timestamps": [{"text": "  In diesem Video soll es um das gradienten Verfahren gehen, auch genannt gradient descent.  Wir befinden uns im Thema der Optimierung und wir moechten eine Skalarefunktion f minimieren.  Skalarefunktionen sind Funktionen, die aus dem R hoch n gehen, also auch von mehreren Variablen abhaengen,  aber deren Funktionswerk ist eine reale Zahl, deshalb Skalarefunktion.", "start": 0.0, "end": 30.0}, {"text": "  In den letzten Videos haben wir ein Algorithmus kennengelernt, wie wir das machen koennen.  Wir berechnen zunaechst den gradienten, das ist der Vektor mit allen ersten Patienten Ableitungen,  und den setzen wir mit dem Nullvektor gleich und loesen dann einen Gleichungssystem.", "start": 30.0, "end": 52.0}, {"text": " Aus diesem Gleichungssystem kriegen wir dann eine Loesungsmenge, die enthaelt alle kritischen Punkte,  und diese Punkte muessen nicht unbedingt ein Extremum sein.  Aber die Hessematrix an diesen Stellen kann uns dann Auskunft darueber geben, ob das jetzt nun ein Extremum ist,  also ob das ein Hochpunkt oder Tiefpunkt ist, oder ob das ein Sattelpunkt ist, meistens.", "start": 52.0, "end": 73.0}, {"text": "  Das Problem ist, dieses Verfahren funktioniert nur auf dem Papier fuer einfache Funktion ganz gut.  In der Praxis eher nicht. Zunaechst liegt das daran, dass das Gleichungssystem gradientf von x gleich Nullvektor  selten analytisch loesbar ist.", "start": 73.0, "end": 93.0}, {"text": "Zudem ist die Hessematrix nicht mit angemessenem Aufwand zu bestimmen,  und das Gleiche geht auch fuer den gradienten. Die Funktion ist einfach zu komplex, und der gradient selbst wird auch zu komplex.  Das koennen wir nicht bestimmen. Wir wissen aber, der gradient zeigt in die Richtung des staerksten Anstiegs.", "start": 93.0, "end": 118.0}, {"text": " Das heisst, wenn wir unsere Funktion minimieren moechten, dann ist es hilfreich,  dass wir in die entgegengesetzte Richtung gehen, von der in die der gradient zeigt. Das heisst, wir minimieren entgegen dem gradienten.  Und wir nutzen dazu ein iteratives Verfahren, in dem wir in jedem Schritt unseren aktuellen Wert  in die entgegengesetzte Richtung, in die der gradient zeigt, verschieben.", "start": 118.0, "end": 136.0}, {"text": "  Und dieser Algorithmus heisst gradient descent, und der funktioniert wie folgt.  Wir starten zunaechst mit einem zufaelligen Vektor x Null.  Und solange mindestens ein Abbruchkriterium noch nicht erfuellt ist,  machen wir dann folgenden Update Schritt. Unser neues x ist unser altes x minus der gradient von f an der Stelle x,  also von unserem alten x, mal eine kleine positive konstante Alpha.", "start": 136.0, "end": 167.0}, {"text": "  Zunaechst, diese kleine positive konstante Alpha ist die schrittweite im Machine Learning Jagon,  denn man hat das auch Lernrate. Und dies ist wichtig, um die geht es noch spaeter.  Aber zunaechst, wie kann man sich das vorstellen? Dazu habe ich ein kleines, eindimensionales Beispiel mitgebracht.  Wir haben hier unsere schoene Parabel. Und die haengt nur von einem x Wert ab.", "start": 167.0, "end": 190.0}, {"text": "  Und weil sie nur von einem x Wert abhaengt, kann der gradient hier entweder nur nach links oder nach rechts zeigen.  Und wir starten hier bei dem Punkt oben. Hier zeigt der gradient offensichtlich ungefaehr so nach rechts,  weil die Funktion hier ansteigt. Und jetzt gehen wir in die entgegengesetzte Richtung, also nach links.", "start": 190.0, "end": 209.0}, {"text": "  Und das Ganze wiederholen wir ein paar Mal, bis wir dann eventuell irgendwann in diesem Minimum hier unten landen.  Haeufig ist das aber nicht so, wie in diesem eindimensionalen Fall, dass wir dann direkt auf unser Minimum zulaufen.  Unsere Funktionen sind auch selten convex wie diese Parabel. Aber generell ist das eher so, dass das aussieht wie hier.", "start": 209.0, "end": 235.0}, {"text": "  Hier sehen wir die Funktionen nicht, also wir sehen den Funktionskraft nicht.  Stattdessen haben wir erstmal diese roten Linien gegeben und diese rote Linien sind Niveau-Linien.  Das heisst entlang dieser Linie hier veraendert sich der Funktionswert nicht.  Das koennte man zum Beispiel so schreiben, als die Menge aller x Werte fuer die gilt, dass f von x eine gewisse Konstante c ist.", "start": 235.0, "end": 261.0}, {"text": "  Also der Funktionswert entlang dieser Linie veraendert sich nicht.  Und hier dieses x ist dann unser Minimum und unser Verfahren, wenn es von hier aus kommt, geht nicht direkt auf unser Minimum zu, sondern macht eher so ein Zickzack.  Das liegt daran, dass der Gradient immer orthogonal ist zu unseren Niveau-Linien.  Das heisst der Gradient hier waere ungefaehr so.", "start": 261.0, "end": 286.0}, {"text": "  Und daran, das liegt, das ist halt der Grund dafuer, dass wir uns eher in so einem Zickzack naehern.  Und jetzt bleiben wir noch ein paar offene Fragen uebrig.  Zuerst, mit was fuer einem x Wert starten wir, gibt es da irgendwie eine intelligente Art und Weise, wie wir einen guten Startwert kriegen.", "start": 286.0, "end": 316.0}, {"text": " Dann, warum genau gibt es nun diese schrittweite Alpha und wie sie zu waehlen?  Ausserdem, was sind denn Beispiele fuer ein Abbruchkriterium und wenn der Gradient so schwierig in der Praxis zu berechnen ist, wie tut man es denn?  Und zunaechst fangen wir an mit dem Initialwert, also mit unserem Startwert fuer x.", "start": 316.0, "end": 335.0}, {"text": "  Und dieser muss auf jeden Fall gut gewaehlt werden, denn wie wir hier sehen, haben Funktionen selten nur ein lokales Minimum oder nur ein globales Minimum, sondern viele lokale Minima.  Hier sehen, hier haben wir eine Funktion, die vier davon hat, hier, hier, hier und hier.  Und abhaengig davon, wo wir starten, koennen wir natuerlich in einem anderen lokalen Minimum landen.", "start": 335.0, "end": 352.0}, {"text": "  Also wenn wir hier starten, koennen wir hier landen, natuerlich abhaengig von der Lernrate, und wenn wir hier starten, koennten wir hier landen.  Und wir sehen, dass dieses lokale Minimum viel besser ist als dieses weiter rechts.  Das heisst, es ist schon wichtig, dass wir einen guten Startwert haben.  In der Praxis bedient man sich haeufig dem Zufall.", "start": 352.0, "end": 369.0}, {"text": "  Das heisst, man waehlt einfach irgendein zufaelligen Punkt, aber man macht das Verfahren einfach mehrmals.  Also man laesst den Algorithmus mehrmals laufen und dann sucht man sich das beste Ergebnis davon raus.", "start": 369.0, "end": 391.0}, {"text": " Ja, und wenn unsere Funktion halt mehrere lokale Minima hat, also wie es haeufig der Fall ist,  ist selten convex, dann heisst ein schlechter Startwert, dass wir in einem schlechten lokalen Minimum konvegieren.  Und das ist nicht das, was wir wollen.  Als naechstes moechten wir uns die Schrittweite anschauen.  Dieses wie gesagt eine kleine positive Zahl, zum Beispiel 0,01.", "start": 391.0, "end": 409.0}, {"text": "  Und diese brauchen wir, damit das Verfahren ueberhaupt konvegiert.  Wir koennen bei einer viel zu grossen Schrittweite oder bei gar keiner Schrittweite,  kann unser Gradient in der Naehe unseres Minimums einfach zu gross werden.  Der Betrag wird zu gross und dann schiessen wir komplett ueber unseren Tiefpunkt hinaus.  Wir moechten in unserem Minimum landen und dazu brauchen wir halt diese kleine Zahl.", "start": 409.0, "end": 434.0}, {"text": "  Und diese kleine Zahl kann entweder fest oder adaptiv sein.  Fest ist sie in den seltensten Faellen, sie ist meistens eher adaptiv.  Das heisst, sie veraendert sich nach jeder Iteration.  Und wenn sie adaptiv ist, dann muss sie sich nicht in jeder Iteration veraendern.  Sie kann gleich bleiben, aber sie kann auch vergroessert oder verkleinert werden.", "start": 434.0, "end": 463.0}, {"text": "  Damit das Verfahren aber konvegiert, ist es haeufig notwendig, dass die Schrittweite insgesamt verkleinert wird.  Das heisst, sie muss sich nicht in jedem Schritt verkleinern,  aber wenn wir die Schrittweite am Anfang und am Ende vergleichen,  sollte sie am Ende schon viel kleiner sein.  Und auch die Initialeschrittweite ist wichtig.", "start": 463.0, "end": 482.0}, {"text": "  Wenn wir sie zu klein waehlen, kann es sein, dass unser Verfahren viel zu langsam konvegiert.  Wenn wir sie aber zu gross waehlen, kann es sein, dass wir ueber unser gutes Lokal das Minimum hinaufschiessen  und dann in einem schlechten landen.  Haeufig ist es so, dass man schaut, wie der neue Funktionswert ist bei unserer aktuellen Schrittweite.", "start": 482.0, "end": 506.0}, {"text": "  Das heisst, man setzt einmal den neuen Wert ein, bevor man diesen Update macht und schaut, verbessern wir uns.  Das heisst, ist der neue Funktionswert kleiner als der aktuelle?  Und falls ja, dann macht man den Update Schritt und falls nein, dann macht man den nicht  und testet solange eine neue Lernrate raus, eine neue Schrittweite aus, bis sich der Funktionswert tatsaechlich verbessert.", "start": 506.0, "end": 529.0}, {"text": "  Und das ist das, was man bei den Minimalflaechen macht, also in der Hausaufgabe.  Da sollt ihr naemlich schauen, ob sich der Funktionswert verbessert.  Das heisst, ob die Flaeche kleiner wird.  Und nur falls die Minimalflaeche kleiner wird, sollt ihr auch diesen Gradient-Decent-Schritt gehen.  Und sonst sollt ihr solange eine neue Schrittweite ausprobieren, bis sich die Flaeche verkleinert.", "start": 529.0, "end": 554.0}, {"text": "  Ein anderes Beispiel fuer eine additive Schrittweite sieht man hier.  Hier haengt das Ganze davon ab, ob sich der Funktionswert im letzten Schritt verbessert hat.  Das heisst, man schaut nicht, ob das jetzt besser oder schlechter wird und abhaengig davon macht man das.", "start": 554.0, "end": 581.0}, {"text": "  Sondern hier geht man den Schritt sowieso, egal was passiert, aber die neue Schrittweite ist abhaengig davon, wie sich der letzte Schritt verhalten hat.  Das heisst, wenn man sich im letzten Schritt verbessert hat, dann multipliziert man das mit einer Konstante etwas groesser als 1, hier 1,1.  Aber falls man sich verschlechtert hat, dann laesst man die Schrittweite haeufig viel viel kleiner werden.", "start": 581.0, "end": 601.0}, {"text": "  Solche Werte sind in der Praxis nicht unnueblich.  Und weil das ja ein bisschen dem Zufall basiert, ist das so, dass man den Algorithmus mit unterschiedlichen Parametern, also sowohl fuer Startwert als auch fuer Schrittweite mehrmals laufen laesst.  Und dann schaut, welches Ergebnis am besten ist.", "start": 601.0, "end": 629.0}, {"text": "  Und jetzt kommen wir zu den Abbruchkriterien. Diese sind eigentlich relativ standard. Man kann auch andere nehmen, aber das hier sind so typische Abbruchkriterien.  Einmal heisst die Veraenderung des Funktionswertes zu klein wird.  Das heisst, wenn der Nutzen durch diesen Update-Schritt nicht gerade gross ist, dann kann man abbrechen.", "start": 629.0, "end": 649.0}, {"text": "  Und das ist haeufig ein Indiz dafuer, dass man konvergiert. Also das ist ein Indiz fuer Convergence.  Und falls man eben nicht konvergiert, weil es irgendwelche Probleme da gibt, da kann man auch einfach die Anzahl der Iterationen begrenzen.  Und dann bricht unser Verfahren ab, weil wir zu viel iteriert haben und zu keinem guten Ergebnis kommen.", "start": 649.0, "end": 673.0}, {"text": "  Als naechstes schauen wir jetzt an, wie man den Gradienten berechnet, denn die Funktion ist ja haeufig unbekannt oder einfach zu komplex.  Und da muessen wir uns halt eine Apoximation bedienen, zum Beispiel durch finiete Differenzen.  Und auch hier ist es wichtig, dass die Apoximation gut ist, damit unser Verfahren auch wirklich erfolgreich ist.", "start": 674.0, "end": 697.0}, {"text": "  Und eine Moeglichkeit, wie man die Laufzeit des Gradienten reduzieren kann, bietet das Verfahren Stochastic Gradient Ascent.  Da kommt der Name Stochastic vor, weil man sich dem Zufall bedient.  Und im Machine Learning wird das gerne benutzt. Da haben wir haeufig Fehlerfunktionen oder Kostenfunktionen dieser Form.", "start": 697.0, "end": 717.0}, {"text": "  Also Q ist dann der Fehler und Qi ist dann der Fehler in Abhaengigkeit von einem Punkt.  Wir haben zum Beispiel, als wir die lineare Ression besprochen haben, gesagt, dass es dort eine Fehlerfunktion gibt,  naemlich die quadratische Fehlerfunktion, das war die Summe von K gleich 1 bis N von Yk minus F von Xk und das Ganze zum Quadrat.", "start": 717.0, "end": 746.0}, {"text": "  Und hier packt man noch eine Konstante 1 durch N, so eine Normierungskonstante vor, und dann ist das praktisch ein Beispiel dafuer.  Also lineare Regression hat zum Beispiel so eine Funktion, die wir minimieren moechten.  Und wenn wir halt den Gradienten davon berechnen, dann haengt der insgesamt von allen Datenpunkten ab.  Das heisst, wir berechnen dann einen Gradienten fuer jeden Datenpunkt i.", "start": 746.0, "end": 771.0}, {"text": "  Und das nimmt dann haeufig eine sehr hohe Laufzeit ein.  Und die Idee ist dann einfach, wir berechnen nicht mehr den tatsaechlichen Gradienten,  sondern nur eine Approximation fuer diesen Gradienten, indem wir nur den Gradienten in Abhaengigkeit von einem einzigen Datenpunkt berechnen.  Und zwar, dieser Punkt wird haeufig zuwaehlig gewaehlt.", "start": 771.0, "end": 795.0}, {"text": "  Und dann sieht das so aus, anstatt dass wir jetzt diese Summe ueber alle Datenpunkte haben und davon halt den Gradienten berechnen und dann am Ende noch normieren,  haben wir am Ende einfach nur den Gradienten in Abhaengigkeit von einem einzigen Datenpunkt.  Und jetzt kommt noch ein kleiner Ausblick.", "start": 795.0, "end": 813.0}, {"text": "  Der Algorithmus gradient descent ist eine Basis fuer viele andere Algorithmen, zum Beispiel konjugierte Gradienten.  Das findet man dann ins Skript im Abschnitt 9.5.  Dann gibt es auch noch das Newton-Verfahren, das auch behandelt wird.  Und dann gibt es auch noch so Hybrid-Verfahren, zum Beispiel cgnuten, das ist so eine Mischung aus den beiden.", "start": 813.0, "end": 832.0}, {"text": "  Und wir schauen uns gleich mal an, welche Motivation dahinter steckt.  Und haeufig moechte man auch optimieren und man moechte nicht irgendwelche Werte haben, sondern die sollen noch eine Nehmbedigung erfuellen.  Und da schreiben wir uns noch das Lagrange-Verfahren an.", "start": 832.0, "end": 859.0}, {"text": " Beim Newton-Verfahren, das sieht tatsaechlich sehr aehnlich aus zu unserem gradient descent,  nur mit dem Unterschied, dass wir anstatt unserer schrittweite Alpha jetzt die Inverse der Hesse Matrix haben.  Und das Newton-Verfahren wird im anderen Video behandelt, aber ich moechte nur mal zeigen, was fuer einen grossen Vorteil das Newton-Verfahren hat.", "start": 859.0, "end": 875.0}, {"text": "  Naemlich haben wir gesagt, dass unser Annaehern an das Minimum in so einem Zickzack stattfindet.  Und beim Newton-Verfahren haben wir den Vorteil, dass der Raum, also diese Scheibe hier, entgruemmt wird, entzerrt wird.  Ja, das heisst, letztendlich wirkt sich das darauf hinaus, dass wir direkt auf unseren Minimum zu steuern.  Hier ist es eher so ein Zickzack und hier gehen wir direkt zu.", "start": 875.0, "end": 900.0}, {"text": "  Das heisst, das Newton-Verfahren konvergiert in weniger Schritten.  Auf der anderen Seite koennen wir diese Inverse Hesse Matrix selten bestimmen.  Das hat dann auch eine zu hohe Laufzeit oder wir koennen es generell nicht tun.  Und das ist halt auch ein Nachteil des Newton-Verfahrens.  Und dann habe ich ja gerade die konjugierte Gradienten besprochen.  Hier ist das sehr aehnlich.", "start": 900.0, "end": 923.0}, {"text": "  Das Verfahren basiert auch ein bisschen auf dem Gradientenverfahren.  Und hier bedient man sich noch dem, dass man im Update Schritt noch was dazu addiert,  in Abhaengigkeit von den Schritten, die man bisher gegangen ist.  Und das Ergebnis davon ist, dass es dazu fuehrt, dass nach dem ersten Schritt,  der erste Schritt ist fuer beide gleich, dass sich dann das Verfahren direkt unsere Minimum naehert.", "start": 923.0, "end": 950.0}, {"text": "  Und zwar in dieser Linie.  Also die gruene Linie sind dann unsere konjugierten Gradienten.  Und das hat den Vorteil, dass das schneller konvegiert.", "start": 950.0, "end": 957.0}]}]