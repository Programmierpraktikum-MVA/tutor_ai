[{"lecture": "25797_65_course_video", "Timestamps": [{"text": "  Herzlich willkommen zum Tutorialsvideo zur Principal Components Analysis, PCA oder auch  Hauptkomponentenanalyse.  In diesem Video gebe ich euch erst mal einen Ueberblick ueber die PCA, was sie ist und dann  wird der Hauptteil dieser Tutorialsvideos daraus bestehen, dass ich die Herleitung der  Hauptkomponentenanalyse durchfuehren werde.", "start": 0.0, "end": 30.96}, {"text": "  Am Ende zeige ich euch nochmal, wie ein Ergebnis aussieht dieser Principal Components Analysis  fuer eine Menge von Gesichtsbildern.  Diese PCA ist erst mal eine Methode, die man benutzen, der Datenanalyse, um seine Daten  einfacher zu strukturieren, indem man eben eine Basis findet fuer die Daten, die sie am  besten repraesentiert.  Das werden auch wir machen.", "start": 30.96, "end": 58.28}, {"text": "  Wir werden PCA benutzen, um den besten linearen Unterraum fuer Bilddaten zu finden.  Das machen wir, indem wir die optimale Orthogonalbasis finden, mit der wir die Bilddaten repraesentieren  koennen.  Das ist ja sinnvoll, weil wir hatten gesehen, dass Bilder als Vektoren dargestellt werden  koennen.", "start": 58.28, "end": 81.6}, {"text": " Und diese Bilder-Vektoren haben oft eine Ruherdimension, also entsprechende Anzahl der Pixel  in einem Bild.  Nicht jeder Vektor aus diesem Vektoraum ist jetzt ein sinnvolles Bild, wenn wir zum Beispiel  Gesichtsbilder betrachten.  Deswegen bietet sich es an, wenn man eine Menge an Gesichtsbildern hat, eine bessere  Basis finden zu wollen.", "start": 81.6, "end": 107.84}, {"text": "  Und wir nehmen jetzt an fuer unsere Daten, dass die zentriert sind, also dass sie befreit  sind vom Mittelwert.  Das heisst, wir haben den Mittelwert der Daten, der ja gegeben ist durch die Summe aller  Datenpunkte.  Dann haben wir von allen Vektoren, die diesen Daten zugehoeren, den Mittelwert abgezogen.", "start": 107.84, "end": 173.84}, {"text": " Hier haben wir zum Beispiel eine zentrierte Datenmenge gegeben, das heisst, sie ist eben  um den Ursprung zentriert, weil wir den Mittelwert abgezogen haben.  Und jetzt sehen wir, wir koennten diese Daten mit der Standardbasis darstellen.  Aber wenn wir zum Beispiel nur eine Dimension nehmen wollen, wuerden wir diese Daten darstellen,  zum Beispiel die x-Achse.", "start": 173.84, "end": 197.35999999999999}, {"text": "  Das waere dann nur, koennten wir unsere Daten, das heisst nur auf diesem Strich hier betrachten,  dann wuerden wir immer den x-Wert von den Daten nehmen.  Immer diese Punkte.  Und wir koennen dann nicht mehr unterscheiden jetzt zwischen zum Beispiel diesem Punkt, der  ja einen y-Wert hat, der negativ ist und diesem Punkt, der auf die x-Achse der gleichen Position  waeren.", "start": 197.36, "end": 228.32}, {"text": "  Aber deswegen ist die Frage, gibt es nicht vielleicht eine Basis, mit der wir eben besser  diese Daten eindimensional darstellen koennten.  Und das werden wir sehen.  Die PCA liefert genau das, eben eine Basis fuer unseren Raum, in dem die Daten legen,  der angepasst ist an wie die Daten aussehen.", "start": 228.32, "end": 259.44}, {"text": " Hier habe ich schon einmal das Ergebnis von so einer PCA praesentiert und eingezeichnet  sind ein Rot der erste Hauptkomponente, die first principle component und in gruen die  zweite Hauptkomponente.  Und wir sehen, in dieser Richtung gibt uns eben die beste lineare Approximierung fuer  unsere Daten.  Ich moechte euch jetzt zeigen, wie man auf die Hauptkomponentenanalyse kommt.", "start": 259.44, "end": 284.2}, {"text": "  Und dafuer betrachten wir das Problem, dass wir eine Datemenge gegeben haben und wir suchen  eben den linearen Unterraum, der unsere Daten am besten repraesentiert.  Und die Herleitung Skizze, also die Idee ist hier dargestellt.  Und die Idee ist naemlich, wir haben eine Menge an Daten, das sind hier die X Punkte,  das sind also Punkte R2 in unserem Fall.", "start": 284.2, "end": 313.79999999999995}, {"text": "  Und wir suchen jetzt die Richtung, mit der wir unsere Daten am besten lineare darstellen  koennten.  Das heisst, wir suchen eine erstmal unbekannte Richtung, also einen Vektor in unseren Daten,  ein Vektor in dem Raum, in dem sich unsere Daten befinden.", "start": 313.79999999999995, "end": 339.84}, {"text": " Und wir finden eben diese Richtung, indem wir sagen, dass der ordentlichen Abstand,  also hier der eingezeichnete Abstandssektor RI, fuer einen bestimmten Datenpunkt BI,  das seinen Betrag minimiert werden soll.  Und wir haben jetzt eine Datenmenge gegeben und jetzt betrachten, das heisst, zum Beispiel  eben Daten im R2.", "start": 339.84, "end": 368.96000000000004}, {"text": "  Und wir betrachten jetzt einen allgemeinen Ausdruck fuer eine allgemeine Richtung,  denommiert es in diesen Daten.  Und dann koennen wir, wie in der vorherigen Skizze gezeigt, eben den Abstandsvektor fuer  jeden Datenpunkt berechnen, indem wir eben den Datenpunkt-Vektor nehmen und davon eben  die autogonale Projektion abziehen.", "start": 369.91999999999996, "end": 403.44}, {"text": "  Und entsprechend koennen wir dann auch den Fehler bilden, der dem Abstand,  entsprechend koennen wir den Fehler bilden und der entspricht nichts anderem als der Vektor-Norm  dieses Abstand-Vektors.  Diese Richtung in den Daten werden wir jetzt finden, indem wir uns eine Fehlerfunktion definieren.", "start": 403.44, "end": 433.44}, {"text": " Das ist, die wir hier als Summe der quadrierten Fehler haben und wir wollen diese Fehlerfunktion  minimieren.  Das heisst, die Richtung fuer dieser Term hier, minimal wird, ist die Richtung, die unsere  Daten am besten linear approximiert.  Und das koennen wir jetzt einsetzen.  Das heisst, wir moechten diese Fehlerfunktion e von a minimieren unter der Bedingung, dass  a normiert ist.", "start": 433.44, "end": 467.88}, {"text": "  Und dann koennen wir eben unseren Ausdruck fuer den Fehler-Vektor hier einsetzen.  Also fuer die Summe von diesen Fehler-Quadraten und unseren Vektor-RI, wie vorher definiert  einsetzen.  Und dann kommen wir auf den folgenden Ausdruck.  Das heisst, wir suchen die Richtung, die diesen Ausdruck hier minimiert.", "start": 467.88, "end": 497.47999999999996}, {"text": " Und wenn wir uns diesen Ausdruck angucken, diese Summe, dann sehen wir, dass dieser  Term hier nicht mehr abhaengig ist von a.  Also der ist nicht abhaengig von a.  Und deswegen koennen wir sehen, dass eine Abhaengigkeit von a wird dieser Term minimal,  wenn schon dieser Term hier minimal wird.  Weil das hier aendert ja nicht an unserem Term.  Da ist kein a drinne.", "start": 497.47999999999996, "end": 526.64}, {"text": "  Das heisst, wir koennen diesen Term vernachlaessigen und koennen dementsprechend unsere Fehlerfunktion  umdefinieren.  Und zwar nur noch den Anteil, der von a abhaengig ist betrachten.  Und in der urspruenglichen Fehlerfunktion geht dieser Term negativ rein.  Und wir wollten sie minimieren.  Und ein negativer Term, dieser Term wird ja genau maximal, wenn, minimal, wenn dieser  Term maximal wird.", "start": 526.64, "end": 572.36}, {"text": "  Und so koennen wir das Problem auch als Maximierungsproblem aufpassen mit dieser folgenden  Fehlerfunktion.  Und jetzt wollen wir diese Fehler hier maximieren.  Also diesen Ausdruck maximieren.", "start": 573.36, "end": 598.4}, {"text": " Und das ist ja nichts als das Skalarprodukt, wenn wir das einsetzen in die Gleichung fuer  das Skalarprodukt und das dann quadrieren, koennen wir auf den Abschnitt dann quadrieren,  koennen wir auf diesen Ausdruck.  Und diesen Ausdruck koennen wir jetzt weiter vereinfachen.  Hier steht ja auch wieder nur das Skalarprodukt von dem Produkt von bi und a.  Das koennen wir also wie folgt aufloesen.", "start": 599.4, "end": 623.12}, {"text": "  Und dann steht hier einmal bi, ta transponiert, mal bi transponiert a.  Und diese Transposition koennen wir auch wieder ausfuehren.  Also dann vertauscht sich die Reihenfolge von a und bi und sie werden beide transponiert,  sodass wir insgesamt dann auf den folgenden Ausdruck kommen.  Die Summe von i gleich 1 bis m von a, t, bi, bi transponiert mal a.", "start": 623.12, "end": 657.6400000000001}, {"text": "  Und hier haben wir schon eine Matrix stehen, weil das hier ist ja ein Produkt von einem  Spalten und einem Zeilenvektor, das ergibt dann also eine Matrix.  Und das heisst, wir haben hier immer wieder eine Summe von einer Matrix, einmal von links  ranmultipliziert mit a transponiert und von rechts ranmultipliziert mit a.", "start": 657.64, "end": 681.88}, {"text": "  Und diese Summe koennen wir eben dann direkt schreiben als gross b und wobei dann dieses  Produkt eine Matrix ist.  Und diese Matrix nennen wir im folgenden jetzt c.  Das heisst, unsere neue Fehlerfunktion koennen wir umschreiben als a transponiert mal die  Matrix c mal a.  Und diese Matrix c entspricht gerade fuer die Leute, die schon Statistik gehoert haben, der  Covariant Matrix, der da.", "start": 681.88, "end": 717.88}, {"text": "  Aber es ist jetzt auch nicht schlimm, wenn ihr das noch nicht gehoert habt.  Und nach Konstruktion ist diese Matrix symmetrische und positiv definiert, weil wir ja hier das  Produkt von dem Vektor mal sich selbst transponiert haben und deswegen hat c auch nur positive  Eigenwerte.", "start": 717.88, "end": 744.16}, {"text": " Aus dem Video zur Eigenzerlegung wissen wir, dass, wenn wir eine symmetrische, positiv  definierte Matrix haben, dass es dann eine Eigenzerlegung gibt zu dieser Matrix, die  den Produkt der Matrix der Eigenvektoren u, der Diagonalmatrix der Eigenwerte lambda und  der Transponierten der Matrix der Eigenvektoren ut entspricht.", "start": 744.16, "end": 761.48}, {"text": "  Und vor allem ist u eine Orthogonalmatrix, weil die Eigenvektoren orthogonal zueinander  sind.  Wir koennen jetzt ohne Beschraenkungen der allgemeinheit fest, aus dem Video zur Eigenzerlegung wissen  wir, dass wir fuer eine symmetrische Matrix eine Eigenzerlegung finden koennen.", "start": 761.48, "end": 790.9200000000001}, {"text": " Das heisst, fuer c koennen wir auch eine Eigenzerlegung finden, die dem Produkt der Matrix der Eigenvektoren,  der Diagonalmatrix der Eigenwerte und der Transponierten der Matrix der Eigenvektoren  entspricht.  Wichtig ist auch, dass u eine Orthogonalmatrix ist, weil die Eigenvektoren orthogonal zueinander  sind und wir koennen sie auch als normiert waehlen.", "start": 790.9200000000001, "end": 809.1999999999999}, {"text": "  Weiterhin koennen wir auch waehlen, dass diese Eigenwerte, die fuer eine positiv definierte  Matrix alle groesser 0 sind, sortiert sind und wir nehmen an, dass lambda 1 der groesste  Eigenwert ist und dass die weiteren Eigenwerte dann absteigen sortiert sind.  Diese Fehlerfunktion koennen wir jetzt einsetzen in unsere Fehlerfunktion.  Das sieht dann aus wie folgt.", "start": 809.1999999999999, "end": 837.88}, {"text": "  Wir setzen einen fuer c unsere Eigenzerlegung ein und hier sehen wir, hier haben wir das  Produkt von einer Matrix, einer orthogonalmatrix unter einem Vektor.  Da muss auch wieder ein Vektor rauskommen.", "start": 837.88, "end": 869.48}, {"text": " Wir wissen ja, dass wir maximieren unter der Bedingung, dass a normiert ist und da u  eine Orthogonalmatrix ist, ut eben auch, wissen wir, dass der Vektor, der rauskommt, den  wir jetzt einfach mal y nennen, auch normiert sein muss, weil orthogonale Matrizen sind  Winkel erhalten und Laengen erhalten.", "start": 869.48, "end": 896.04}, {"text": " Wenn wir jetzt den transponierten Vektor davon bilden, sehen wir auch gleich, dass das dem  Vektor entspricht, der auf der linken Seite der diagonalen Matrix steht.  a transponiert mal u.  Und des a und y muessen beide normiert sein.  Wenn wir das einsetzen, dann kommen wir auf diesen folgenden Ausdruck fuer die Fehlerfunktion.  Das heisst, hier muesste das e stehen, e' von a.", "start": 896.04, "end": 924.48}, {"text": "  Und dieses Produkt koennen wir jetzt ausfuehren.  Wir nehmen jetzt an, dass y erstmal generell aussieht wie folgt.  Also die Eintraege sind y1, y2 und so weiter.  Und das Produkt von der Matrix, der diagonalmatrix, Eigenwerte, die wie folgt aussieht.  Lambda 1.  Das ist ja da nichts als ein Vektor.  Wir skalieren die Eintraege von diesem Vektor mit den Eigenwerten.", "start": 925.12, "end": 978.12}, {"text": "  Das heisst, dieser Vektor sieht dann aus Lambda 1, y1, Lambda 2, y2.  Und so weiter.  Und das Skalarprodukt von diesem Vektor mit y ergibt dann den folgenden Ausdruck.  Wir multiplizieren die gleichen Indizes miteinander und summieren die Therme auf, so dass wir dann  eben aufkommen, dass dieser Ausdruck, den wir dann maximieren wollen, wie folgt aussieht.", "start": 978.12, "end": 1021.68}, {"text": "  Und dieser Ausdruck wird eben maximal, wenn Lambda, wenn y diesem Vektor anspricht.  Weil wir wissen ja, dass y auch normiert sein muss.  Das heisst, es kann maximal Betrag 1 haben.  Und der maximale Wert, den dann dieser Ausdruck kriegen kann, ist halt Lambda 1.  Das heisst, die erste Komponente muss 1 sein und alle anderen muessen 0 sein, weil der Vektor normiert ist.", "start": 1021.68, "end": 1050.68}, {"text": "  Und wenn wir uns jetzt angucken, okay, wir wissen, wie y aussehen muss und wir wissen im Allgemeinen,  wie u aussieht, mit den Eigenvektoren u1 bis un.  Das heisst, dass unsere gesuchte Richtung gerade dem ersten Eigenvektor entspricht, dieser Matrix, u.  Dem ersten Vektor entspricht dieser Matrix u und das ist ja gerade der erste Eigenvektor von unserer Matrix C.", "start": 1050.68, "end": 1084.28}, {"text": "  Das heisst, den ersten Eigenvektor mit dem groessten Eigenwert von dieser Matrix.  Diesen Eigenvektor nennt man dann auch erste Hauptkomponente oder first principle component.  Und man kommt darauf, dass dieser Eigenvektor auch der erste linke Singulaervektor von B ist.  Das heisst, der Singulaervektor mit dem groessten Singulaerwert.", "start": 1085.28, "end": 1110.28}, {"text": "  Und das ist der Zusammenhang zur Singulaerwaertserlegung, die wir dann benutzen werden,  um diese im Allgemeinen diese first principle component analysis durchzufuehren.  Die weiteren Hauptkomponenten sind dann gegeben durch die weiteren Eigenvektoren von C.  Und je groesser der Eigenwert ist, desto wichtiger ist die Hauptkomponente.", "start": 1110.28, "end": 1135.28}, {"text": "  Und insgesamt bilden dann diese Hauptkomponenten eben die Basis, die wir gesucht haben,  also die Basis, mit dem wir am besten unsere Daten repraesentieren koennen.  Das heisst, hier nochmal Zusammenfassung, wir hatten ja die Herleitung angefangen damit,  dass wir den optimalen, linearen Unterraum fuer eine beliebige Datenmenge gesucht haben.", "start": 1135.28, "end": 1160.28}, {"text": "  Und wir haben angesetzt, dass wir den Richtungsvektor zu einer beliebigen Geraden,  die einer beliebigen Richtung in den Daten entspricht, ansetzen und dann darueber die Fehlerfunktion bilden.  Und diese Fehlerfunktion dann minimieren unter der Nebenbedingung,  dass die Richtung, die wir betrachten, eben, dass der Richtungsvektor normiert ist.", "start": 1160.28, "end": 1185.28}, {"text": "  Und als Ergebnis haben wir dann herausgekommen und als Ergebnis sind wir dann darauf gekommen,  dass der beste lineare Unterraum eben durch den Eigenvektor zum maximalen Eigenwert dieser Matrix C entspricht.  Und da diese Eigenvektoren dieser Matrix, die ja den Hauptkomponenten entsprechen,  Eigenvektoren von einer symmetrischen Matrix sind, sind sie auch orthogonal zueinander.", "start": 1185.28, "end": 1225.28}, {"text": "  Etere Methode, wie man auf die Principle Components kommt, moechte ich euch hier erlaeutern, also einmal ansprechen.  Und das ist naemlich, dass wir die Richtung in den Daten suchen, bei der die Varianz maximiert wird.", "start": 1225.28, "end": 1251.28}, {"text": " Und ich will jetzt nicht zu weit in Varianz und in Wahrscheinlichkeitstheorie eingehen,  aber diese Varianz bedeutet fuer uns jetzt erstmal quasi der Spread, also wie weit verteilt sind die Daten in eine beliebige Richtung.  Und da sehen wir auch wieder, dass diese erste Hauptkomponente genau auch die Richtung angibt, in der die Daten am meisten gespreadet sind.", "start": 1251.28, "end": 1270.28}, {"text": "  Das heisst, man koennte auch die Herleitung fuer die PCA durchfuehren, indem wir die Richtung suchen, in der die Varianz maximiert wird.  Wir hatten das ja jetzt gemacht ueber die autogenalen Abstaende.  Zum Schluss praesentiere ich einmal das Ergebnis einer PCA fuer eine Menge von Gesichtsbildern.  Und wir wissen aus vorigen Videos, dass wir Bilder als Vektoren darstellen koennen.", "start": 1270.28, "end": 1294.28}, {"text": "  Also koennen wir auch diese Matrix BB transponiert, die wir aus der Herleitung kennen, fuer Bilder bilden.  Und es sind ja genau die Eigenvektoren von dieser Matrix, die den optimalen Unterraum bilden, in dem unsere Daten liegen,  bzw. mit denen wir unsere Daten darstellen koennen.  Und fuer Bildervektoren sind diese Eigenvektoren eben auch wieder Bilder, und wir nennen sie dann Eigenfaces.", "start": 1294.28, "end": 1322.28}, {"text": "  Und diese Eigenfaces koennen wir als Basis benutzen, um Bilder effiziente Datenschutzstellen.  Weil wir hatten gesehen, dass Bilder mit 98 mal 116 Pixeln, also insgesamt 11386 Pixeln,  als Vektoren im R hoch 11386 aufgefasst werden koennen.  Und das ist, wenn wir jetzt die Standardbasis waehlen, um ein Bild darzustellen, dann brauchen wir 11368 verschiedene Vektoren.", "start": 1324.28, "end": 1367.28}, {"text": "  Aber diese Eigenfaces bilden eben eine Basis mit vier weniger Vektoren, mit der wir unsere Bilder oder unsere Daten dann rekonstruieren koennen.  Das Ergebnis einer PCA praesentiere ich hier.  Also wir haben wieder eine Menge von Bildern, ungefaehr 150 solcher Bilder.  Und das sind Vektoren, weil wir hier 98 mal 116 Pixeln haben im R hoch 11368.", "start": 1368.28, "end": 1411.28}, {"text": "  Aber ueber die Eigenzerlegung, darueber, dass wir diese Eigenvektoren der Matrix BAB transponiert finden,  haben wir eine Basis, mit der wir unsere Bilder effizienter darstellen koennen.  Hier habe ich einmal ein Bild geplottet, so wie es aussieht, und dann hier das Bild rekonstruiert mit einer variablen Anzahl von Eigenfaces.", "start": 1411.28, "end": 1440.28}, {"text": "  Also in diesem Fall ist es gerade so, dass wir eine Basis von 10 Eigenfaces benutzen.  Wenn wir alle Eigenvektoren benutzen, sehen wir, dass wir das Bild sehr gut rekonstruieren koennen.  Und wir benutzen nur 151 Bilder, also eigentlich haben wir 151 Bilder.", "start": 1440.28, "end": 1470.28}, {"text": " Wir benutzen nur 151 Eigenfaces und wir koennen schon das Bild rekonstruieren,  anstatt dass wir 11368 verschiedene Vektoren brauchen, um dieses Bild fuer Pixel zu rekonstruieren.  Das Weiteren sehen wir auch hier in diesem Graf ist der entsprechende Eigenwert zu dem Index dargestellt, also das Spektrum der Eigenwerte.", "start": 1471.28, "end": 1494.28}, {"text": "  Und wenn wir nur die, wir koennen schon mit nur den ersten 40 Eigenfaces das Gesicht relativ gut rekonstruieren.  Und da sehen wir auch der Vorteil von dieser PCA, wir brauchen nicht mehr einen Vektoraum, also wir brauchen nicht mehr eine Basis mit 11368 Vektoren oder 150 Vektoren.", "start": 1494.28, "end": 1518.28}, {"text": "  Wir koennen je nach Anwendung auch schon eine geringere Anzahl von Principle Components benutzen, um unsere Daten zu rekonstruieren.  Genau, das ist was ich euch in diesem Video zeigen wollte, ich wollte euch vor allem die Herleitung erlaeutern, wie man auf die PCA kommt.", "start": 1518.28, "end": 1547.28}, {"text": " Das ist jetzt ein Beispiel, was rauskommt, wenn man das angewendet hat und wie man ein Algorithmus implementiert fuer die PCA,  das werdet ihr im naechsten Video erfahren.", "start": null, "end": 1547.28}]}]