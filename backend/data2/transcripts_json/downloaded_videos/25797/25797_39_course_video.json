[{"lecture": "25797_39_course_video", "Timestamps": [{"text": "  In diesem Video geht es um zwei Probleme, die wir bereits in vergangenen Themenbloecken  behandelt haben, aber dieses mal aus der Sicht der Optimierung.  Die beiden Probleme, von denen hier die Rede ist, sind einmal die lineare Regression, auch  genannt ordinary Least Squares oder einfach nur Least Squares und dann noch die Hauptkomponentanalyse  kurz PCA.", "start": 0.0, "end": 28.64}, {"text": "  Dieses Bild kommt manchen von euch wahrscheinlich schon bekannt vor.  Wir sehen hier einmal vier Punkte und eine blaue Funktion, die diese vier Punkte approximiert.  Ausserdem sehen wir in Rot eingezeichnet den Fehler, der durch das jeweilige Verfahren minimiert wird.", "start": 29.64, "end": 55.28}, {"text": " Bei der linearen Regression war das der Fehler entlang der Y-Richtung und um genau zu sein,  war das nicht nur dieser Fehler, sondern auch noch das Quadrat des Fehlers.  Das heisst, wir muessten theoretisch hier so Quadrate einzeichnen.  Bei der Hauptkomponentanalyse allerdings war das der tatsaechlich kuerzeste Fehler, der minimiert wurde.  Das ist dann der orthogonalste Funktion.", "start": 55.28, "end": 71.0}, {"text": "  Deshalb wird es auch haeufig orthogonale Regression genannt.  Und auch hier muss man sagen, es ist eigentlich der Fehler zum Quadrat.  Die Hauptkomponentanalyse haben wir aber damals nicht eingefuehrt, als die Minimierung dieses  orthogonalen Fehlers, sondern als die Maximierung der Varianz entlang der Richtung der Hauptkomponenten.", "start": 71.0, "end": 96.68}, {"text": "  Das heisst, wir wollten ein Vektor und entlang der Richtung dieses Vektors sollen sich die Daten am meisten unterscheiden.  Das Problem der linearen Regression haben wir damals mit der normalen Gleichung geloest  und die haben wir mit Hilfe von Orthogonalprojektion hergeleitet.  Jetzt moechten wir das Ganze mit Ableitungen und dem Gradienten herleiten.", "start": 98.68, "end": 121.44}, {"text": "  Die Hauptkomponentanalyse haben wir damals als geloest, indem wir eine Eigenzerlegung oder eine Singulaerwaertszerlegung gemacht haben.  Und in dem Video wollen wir uns das Ganze noch mal als Eigenwertproblem mit Hilfe der langgrange Multiplikatoren herleiten.  Fangen wir zuerst an mit der linearen Regression.  Dort haben wir eine Fehlerfunktion.", "start": 121.44, "end": 141.44}, {"text": "  Diese Fehlerfunktion ist die norm, die standard euklide norm, die wir immer nutzen zum Quadrat,  und zwar von einem Matrix-Vektorprodukt AX.  Und dieses Matrix-Vektorprodukt ist dann nichts anderes als die Schaetzwerte fuer unsere Y-Werte.  Das heisst, A mal X ist unsere Schaetzfunktion und das sind dann unsere geschaetzten Y-Werte.  Und der Vektor B enthaelt dann die tatsaechlichen Y-Werte.", "start": 141.44, "end": 164.44}, {"text": "  Die haben wir damals anders genannt, hier heissen sie jetzt AX und B.  Und das ist wie gesagt eine sogenannte Fehlerfunktion und diese moechten wir optimieren, das heisst minimieren.  Und diese Fehlerfunktion entspricht dem sogenannten quadratischen Fehler.  Das ist nichts anderes als die Summe ueber alle Punkte von i gleiche eins bis n, wir haben n Datenpunkte.", "start": 165.44, "end": 188.44}, {"text": "  Und wir moechten den Schaetzwert minus den tatsaechlichen Wert zum Quadrat minimieren.  Und das Ganze kann man wie gesagt als der Quadrat der zwei Norm aufschreiben.  Die zwei Norm ist die standard euklide Norm, also das bedeutet die zwei hier unten.  Und zwar von einer von A mal X, das ist dann wie gesagt unser Vektor mit den Schaetzwerten  und B dem Vektor mit den tatsaechlichen Y-Werten.", "start": 189.44, "end": 213.44}, {"text": "  Und wir haben auch damals gesagt, wir haetten theoretisch auch den Betragsfehler nehmen koennen  und der Betragsfehler entspricht dann der eins Norm oder der L eins Norm  und die eins Norm eines Vektors uebrigens ist nichts anderes als einfach nur die Summe der Betraege.  Also die eins Norm von X waere dann X1 zum Betrag plus X2 zum Betrag und so weiter.", "start": 213.44, "end": 240.44}, {"text": "  Da haben wir gesagt, dass wir die quadratische Fehlerfunktion bevorzugen.  Warum? Denn die hat eine wunderschoene Eigenschaft, die ist naemlich convex.  Ungefaehr so wie diese Normalparabel kann man sich das Ganze vorstellen, natuerlich in mehreren Dimensionen.", "start": 240.44, "end": 265.44}, {"text": " Hier unten, das ist der Punkt, der uns interessiert, das ist naemlich das globale Minimum  und das ist auch der einzige kritische Punkt der Funktion.", "start": 266.44, "end": 284.44}, {"text": " Das kann man hier nochmal in drei Dimensionen sehen, eine Parabel, die von X und Y abhaengt  und der Punkt hier unten, das ist dann der Punkt, den wir suchen und weil die Funktion convex ist,  koennen wir einfach den Gradienten unserer Fehlerfunktion berechnen, den mit dem Null-Vektor gleichstellen,  dieses Gleichungssystem loesen und so kriegen wir unser Ergebnis raus.", "start": 284.44, "end": 291.44}, {"text": "  Und jetzt moechten wir damit die Normalengleichung herleiten.  Damit wir das koennen, vereinfachen wir die Funktion erstmal.  Wir wissen, die Norm zum Quadrat ist ja nichts anderes als das transponiert mal das nochmal.  Das heisst, wir kriegen dann das hier raus.  Jetzt wenden wir das Diskriminativgesetz an, jeder mit jedem.", "start": 292.44, "end": 314.44}, {"text": "  Das heisst, A mal X transponiert, das ist ja X transponiert mal A transponiert mal AX,  da kriegen wir den ersten Summanden hier raus.  Dann AX transponiert mal minus B, da kriegen wir den zweiten raus.  Dann minus B transponiert mal AX, kriegen wir das raus.  Und hier muesste eigentlich ein Plus stehen.  Und minus B transponiert, minus B ergibt B transponiert B.", "start": 314.44, "end": 343.44}, {"text": "  Das heisst, das hier ist jetzt unsere neue Funktion, einmal anders aufgeschrieben.  Und hier haben wir einmal X transponiert mal A transponiert mal B.  Und das ist dann nichts anderes als das Skalaprodukt von A mal X und B.  Und hier haben wir das Ganze anders herum, das ist das Skalaprodukt von B und AX.  Und wir wissen, das Skalaprodukt ist komutativ.", "start": 343.44, "end": 370.44}, {"text": "  Das heisst, wir koennen das als 2 mal B transponiert mal A mal X zusammenfassen.  Als naechstes machen wir ein paar kleine Nebenrechnungen.  Zuerst schauen wir uns an, was X transponiert mal eine Matrix C mal X eigentlich ist.  Das Ganze nennt man eine quadratische Form einer Matrix.  Und das ist ja letztendlich nichts anderes als das Skalaprodukt von X mit C mal X.", "start": 374.44, "end": 400.44}, {"text": "  Wir wissen, wie man Matrix-Vectormultiplikation macht.  Das ist ja immer Zeile der Matrix.  Spalte, also der Vector hat nur eine Spalte, deshalb immer das hier.  Das heisst, da kommt am Ende raus C11 mal X1 plus C12 mal X2 plus und so weiter.  Und dann im zweiten Eintrag natuerlich C22 mal X1 plus C21 plus C22 mal X2 plus und so weiter.", "start": 400.44, "end": 445.44}, {"text": "  Und wenn wir jetzt noch das hier ranmultiplizieren, also hier ueberall ranmultiplizieren,  da kommt hier X1 noch mal ueberall vor, hier noch mal X2.  Und das Ganze kann man dann im Allgemeinen als diese doppelte Summe aufschreiben.  Das ist dann die Summe ueber i von 1 bis n, j von 1 bis n, Cij mal Xi mal Xj.  Und jetzt schauen wir uns an, was passiert, wenn wir das nach Xk-parzell ableiten.", "start": 445.44, "end": 474.44}, {"text": "  Zuerst, wir wissen, ableiten ist linear, auch die parzellableitung ist linear.  Das heisst, Ableitung der Summe ist Summe der Ableitungen, das heisst, es reicht, wenn wir diesen Termich in der Mitte betrachten.  Und was uns auffaellt, wenn i ungleich k und j ungleich k, dann haengt dieser Term gar nicht von k ab.", "start": 474.44, "end": 501.44}, {"text": "  Das heisst, dieser Term ist entweder muss i gleich k oder j gleich k oder beide gleich k gelten, damit k ab hier vorkommt.  Und wenn jetzt koennen wir sozusagen so eine Fallunterscheidung machen, wir teilen diese Summe dann in zwei Summe auf.  In der einen wehen wir i fest, gleich k und gehen ueber j, und in der anderen wehen wir j fest und gehen ueber i.", "start": 502.44, "end": 527.44}, {"text": "  Und bei der Ableitung hier muss man dann nichts anderes tun, als wenn sozusagen einer von den beiden Xk ist,  dann haengt es der Linear ab von Xk und dann streisst sich das weg, wenn wir das Ganze ableiten.  Das heisst, die parzellableitung ist dann einmal die Summe ueber alle j's, von ckj mal xj, und dann genau das Gleiche nur mit i.", "start": 527.44, "end": 552.44}, {"text": "  Und wenn wir uns jetzt davon den Gradienten betrachten, dann wehen wir immer k gleich 1, k gleich 2, und so weiter bis k gleich n.  Und dann setzen wir das sozusagen einfach mal ein und dann ist das hier der Gradient.  Und dann machen wir daraus erstmal eine Summe von zwei anderen Vektoren, indem wir einfach hier den linken Teil, den einen Vektor packen, den rechten Teil, den anderen.", "start": 552.44, "end": 576.44}, {"text": "  Und jetzt sehen wir hier die erste dieser beiden Vektoren im ersten Eintrag.  Hier haben wir sozusagen die Summe ueber die erste Zeile der Matrix C mal unseren Vektor.  Das heisst, das hier ist nichts anderes als das Skalaprodukt von c1, Sternchen, mache ich mal fuer die erste Zeile von C und unseren Vektor x.  Und fuer die zweite genau das Gleiche und so weiter.", "start": 576.44, "end": 608.44}, {"text": "  Und nach der Definition der Matrix-Multiplikation ist es ja nichts anderes als C mal x.  Das Rechte hier ist sehr aehnlich, nur dass das Ganze jetzt in der Spalte ist.  Also hier ist die Spalte fest und wir machen das dann ueber alle Zeilen.  Das heisst, das ist nichts anderes als c transponiert mal x.", "start": 608.44, "end": 635.44}, {"text": "  Und unser Gradient von unserer quadratischen Form ist dann letztendlich c plus c transponiert mal x.  Und hier noch eine kleine Anmerkung, falls unsere Matrix C symmetrisches ist, dann geht ja c transponiert gleich c.  Und dann ist der Gradient davon einfach 2 mal c mal x. Das werden wir spaeter brauchen.  Jetzt machen wir das Ganze nochmal mit einer aehnlichen Funktion, ein bisschen schneller.", "start": 635.44, "end": 659.44}, {"text": "  Hier kann nur noch, wenn wir das hier nach, also es geht erstmal um b transponiert mal c mal x.  Das kann man wieder so eine doppelte Summe aufschreiben, aber hier gibt es nur 1x.  Und hier kann auch nur dieses x von, hier kann auch nur dieses x k von, hier kann nur j gleich k sein und dann ist es ungleich 0.", "start": 659.44, "end": 688.44}, {"text": "  Das heisst, was wir tun muessen ist, um das Partial nach x k abzuleiten, muessen wir einfach nur ueber alle i's iterieren.  Und das j muessen wir dann fest als k waehlen, damit das ungleich 0 ist.  Und wenn das so dann x k ist und wir leiten das nach x k ab, dann ist es ja linear.  Das heisst, das verschwindet und dann die Konstante, die hier uebrig bleibt, ist die Ableitung.", "start": 688.44, "end": 703.44}, {"text": "  Und das ist dann die Partielle Ableitung.  Und wenn wir das Ganze jetzt als gradient aufschreiben, dann ist das dann c transponiert mal b.  Jetzt kehren wir zu unserem eigentlichen Problem zurueck, und zwar zu least squares.  Wir haben gesagt, wir muessen einmal den gradient in unserer Fehlerfunktion berechnen und den mit dem Nullvektor gleichsetzen.", "start": 703.44, "end": 738.44}, {"text": "  Und wir haben ja gesehen, dass unsere Fehlerfunktion, also die Norm von ax \u2013 b\u00b2, gleich dieser lange Term hier ist.  Und wir wissen, der gradient ist ein lineare Operator.  Das heisst, der gradient der Summe ist die Summe der gradienten.  Das heisst, wir koennen einfach jeden Term einzeln betrachten und ableiten.  Und wir stellen zuallererst fest, dass b keine Funktion von x ist.", "start": 738.44, "end": 762.44}, {"text": "  Das heisst, b transponiert mal b haengt gar nicht von x ab.  Das heisst, der gradient hier von null, der verschwindet.  Und jetzt moechten wir die beiden Nebenrechnungen, die wir gerade gemacht haben, verwenden.  Wir haben ja gesehen, dass fuer eine symmetrische Matrix c gilt, dass x transponiert mal c mal x abgeleitet.  Also der gradient davon, falls c Symmetrisches ist, ist 2 c x.", "start": 762.44, "end": 790.44}, {"text": "  Und jetzt nutzen wir die Matrix a transponiert mal a fuer c.  Und wir wissen, a transponiert a ist immer symmetrisch.  Das heisst, davon der gradient ist dann 2 a transponiert a x.  Wir haben auch gesehen, dass der gradient von b transponiert mal c mal x, gleich c transponiert b ist.  Und das nutzen wir.  Das heisst, wir setzen hier a fuer c ein und dann kriegen wir genau das als Gradient.", "start": 790.44, "end": 827.44}, {"text": "  Das haben wir wie immer mit dem Null-Vektor gleichgesetzt.  Und jetzt koennen wir das hier durch Addition auf beiden Seiten auf die andere Seite bringen.  Und dann haben wir 2 a transponiert a x gleich 2 a transponiert b.  Und wenn wir beide Seiten durch 2 dividieren, dann kriegen wir genau unsere normalen Gleichung raus.  a transponiert mal a mal x gleich a transponiert b.", "start": 827.44, "end": 849.44}, {"text": "  Und wir haben damals gesagt, dass unsere Matrix a ueberbestimmt ist.  Und jedes Mal, wenn unser a gs a mal x gleich b keine Loesung hat, loesen wir stattdessen a transponiert a x gleich a transponiert b.  Und schlussendlich betrachten wir einmal das Problem der Hauptkomponentenanalyse.  Wir haben das ja wie gesagt als Maximierung der Varianz entlang der Richtung der Hauptkomponenten definiert.", "start": 849.44, "end": 879.44}, {"text": "  Das heisst, wir haben diesmal eine Funktion, die wir maximieren wollen.  Und dies dann definiert als die quadratische Form einer Matrix a, also x transponiert mal a mal x.  Und die Matrix a ist eine ganz besondere Matrix, das ist naemlich die sogenannte Covarianzmatrix.  Und was wir da gemacht haben ist, wir hatten ja unsere Datenmatrix x.", "start": 880.44, "end": 902.44}, {"text": "  Davon haben wir den Durchschnitt abgezogen, wir haben sie mittelwert befreit, ich nenne immer my.  Und dann mussten wir x transponiert mal x rechnen.  Und das ist dann bis auf ein Vielfaches die Covarianzmatrix oder eine Schaetzung fuer die Covarianzmatrix.  Und mit dieser Matrix kriegen wir dann raus, in welche Richtung sich die Daten am meisten unterscheiden.", "start": 902.44, "end": 925.44}, {"text": "  Und das Optimierungsproblem sieht halt noch eine Nebenbedingung vor.  Und zwar zum einen interessiert uns nur die Richtung des Vektors.  Das heisst, wir moechten, uns ist egal wie lang der Vektor wird, uns ist wirklich nur die Richtung wichtig.  Und das Problem kann unter gewissen Umstaenden keine Loesung haben.", "start": 925.44, "end": 952.44}, {"text": "  Wenn wir unseren Vektor x4 zu gross waehlen, dann koennte die quadratische Form dieser Matrix gigantisch werden.  Deshalb ist es auch sinnvoll, die Laenge des Vektors zu beschraenken.  Und hier haben wir gesagt, wir moechten einen Einheitsvektor haben.  Das heisst, er soll die Laenge 1 haben.  Damit das aber einfacher zu rechnen ist, sagen wir, das Quadrat der Norm soll 1 sein.  Und 1 Quadrat ist auch 1.", "start": 952.44, "end": 972.44}, {"text": "  Das heisst, es laeuft auf genau das Gleiche hinaus.  Aber es macht uns das Rechnen einfacher.  Ich hoffe, die Methode von Lagrange kennt ihr bereits.  Wir definieren uns eine neue Funktion, die sogenannte Lagrange Funktionen.  Schreiben einmal die Maximierungsfunktion ab.  Minus, dann kommt unser neuer Parameter Lambda noch dazu.  Das ist jetzt ein Skalar.", "start": 972.44, "end": 994.44}, {"text": "  Und dann einmal unsere Nebenbedingungen umgeschrieben.  Wir wissen, dass die Norm die Wurzel des Skalarproduktes ist.  Das heisst, die Norm zum Quadrat ist dann nichts anderes.  X transponiert mal x.  Das ist dann genau die eukidische Norm davon.  Und dann minus unsere Konstante 1.  Das Ganze koennen wir dann ableiten.", "start": 994.44, "end": 1019.44}, {"text": "  Wir wissen bereits, wie wir die quadratische Form einer symmetrischen Matrix ableiten.  Das ist dann 2ax.  Und hier ist es natuerlich wichtig, wie wir Lambda mal x transponiert mal x ableiten.  Das Lambda, das ist eine Konstante, das behalten wir.", "start": 1019.44, "end": 1043.44}, {"text": " Aber die Frage ist natuerlich, wie leiten wir x transponiert x ab?  Und das ist ja nichts anderes als x1 zum Quadrat plus x2 zum Quadrat plus und so weiter.  Und wenn wir jetzt davon den Gradienten nehmen, zuerst die Parzelle Ableitung nach x1.  Die anderen Terme dahinter haengen ja gar nicht von x1 ab.  Das heisst, hier steht zuerst 2x1 drin.  Im zweiten Eintrag steht da 2x2 drin und so weiter.", "start": 1043.44, "end": 1061.44}, {"text": "  Und das ist dann, wenn wir das so weiterfuehren, genau 2x.  Das heisst, der Gradient davon ist dann minus 2 Lambda x.  Und das hier hinten einmal nach Lambda abgeleitet, es haengt linear von Lambda ab.  Das heisst, 1 minus x transponiert mal x.  Das untere ist ja wie gesagt unsere Nebenbedingungen.  Die koennen wir erstmal so stehen lassen.", "start": 1061.44, "end": 1086.44}, {"text": "  Und jetzt ist es wichtig, dass wir das hier oben dem Gradienten gleichsetzen,  der Quadrat der 0-Vektor gleichsetzen.  Das heisst, wir haben 2ax minus 2 Lambda x gleich 0.  Das formen wir dann um, indem wir einfach die 2 Lambda x auf die andere Seite bringen durch Addition auf beiden Seiten.", "start": 1087.44, "end": 1106.44}, {"text": "  Das heisst, wir haben dann 2ax gleich 2 Lambda x, dividieren wir durch 2 und dann haben wir A mal x gleich Lambda mal x.  Und das bedeutet, wenn wir unsere Vektor x an der Matrix ranmultiplizieren,  dann kriegen wir ein Vielfaches von diesem Vektor raus.  Und das ist ein Eigenwertproblem.  Und jetzt schauen wir uns an, wie wir das Ganze loesen.", "start": 1106.44, "end": 1128.44}, {"text": "  Wir wissen aus dem Lagrange-Verfahren, dass unter den gefundenen Punkten,  unter den Vektoren, die hier infrage kommen, also den Eigenvektoren von A,  muss es einen geben, der die Funktion maximiert und eine, der die Funktion minimiert.  Uns interessiert natuerlich das Maximum.  Und jetzt setzen wir mal einen Eigenvektor x von A ein in unsere quadratische Form.", "start": 1128.44, "end": 1148.44}, {"text": "  Und wir wissen, dass x ein Eigenvektor ist.  Das heisst, A mal x ist gleich Lambda mal x.  Das heisst, wir kommen hier an, dann koennen wir unser Skalar Lambda an den Anfang multiplizieren und dann haben wir x transponiert mal x.  Und jetzt wissen wir, dass die anderen Nebenbedingungen gelten muss.  Das ist ja schliesslich die Norm zum Quadrat.", "start": 1148.44, "end": 1170.44}, {"text": "  Und die muss gleich eins sein, weil wir das in unserer anderen Nebenbedingungen so aufgeschrieben haben.  Und dann kommt genau Lambda raus.  Das heisst, wir moechten unsere Funktionen fuer ein Eigenvektor, das am Ende der Eigenwert, der dazugehoert, und den moechten wir maximieren.  Das heisst, was wir am Ende suchen, ist der Eigenvektor x zum groessten Eigenwert von A.", "start": 1170.44, "end": 1199.44}, {"text": "  Und ich habe ja diese paar Nebenrechnungen gemacht und das Ganze ist ein bisschen kompliziert fuer etwas schwere Faelle.  Und diese vektorieren Funktionen abzuleiten ist generell immer sehr, sehr unschoen.  Und man macht das Ganze natuerlich nicht immer, sondern Leute haben es schon fuer euch gemacht.  Und das Ganze koennt ihr dann auf Wikipedia zum Beispiel in Tabellen nachschlagen.", "start": 1199.44, "end": 1218.44}, {"text": "  Oder es gibt ganze Webseiten, wo man das Ganze eingeben kann.  Und die Webseite leitet das dann fuer euch ab.", "start": 1218.44, "end": 1220.44}]}]